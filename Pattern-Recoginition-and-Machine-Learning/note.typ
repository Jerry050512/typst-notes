#import "../template/conf.typ": conf
#import "../template/components.typ": *

#show: conf.with(
  title: [
    模式识别与机器学习
  ],
  authors: (
    (
      name: [Gang.],
      affiliation: [Hangzhou Dianzi University],
      email: "jerry050512@outlook.com",
    ),
  )
)

#show "!待确认": align(
  center, 
  rect(
    text(
      "待确认", 
      size: 15pt, 
      stroke: red,
      weight: "thin"
    )
  )
)

= 考试重点

== 题型

- 名词解释: 10 (30)
- 问答题: 7 (50) (最后一题计算)
- 分析论述: 1 (20)

考试时间地点: 6.19 18:00-20:00 6教北102

== 基本概念

#align(
  center, 
  grid(
    align: left, 
    columns: (1fr, 1fr), 
    [
      - 非监督模式识别
      - 参数估计 (概率密度函数)
      - 最小错误率贝叶斯决策
      - 分类器
      - 支持向量机
      - 核函数
      - 特征选择
    ], 
    [
      - 流形学习
      - k-means 算法
      - 深度学习
      - 激活函数
      - 梯度消失与梯度爆炸
      - 交叉验证
    ]
  )
)

==  算法原理

#align(
  center, 
  grid(
    align: left, 
    columns: (1fr, 1fr), 
    [
      - 模式识别系统典型构成
      - 贝叶斯估计
        - 算法原理及步骤
        - 朴素贝叶斯估计的计算应用 (计算题)
      - 特征提取/降维
        - 主成分分析
        - 多维尺度法
        - 上述两种线性降维方法的区别
      - 类别可分性判据
        - 在刻画特征对分类的贡献时需要希望满足那些条件
        - 常见的四种类别可分性判据
    ], 
    [
      - 非监督模式识别/聚类算法
        - k-means, 模糊C均值, ISODATA, DBSCAN, 密度峰值聚类等多种聚类算法的重点及区别
      - 循环神经网络
      - 模式识别系统的评价方法
        - 监督模式识别: 训练错误率, 测试错误率, 交叉验证, 自举法与.632法
        - 非监督模式识别: 紧致性, 连接性质与分离度. 
      - 模式识别系统知识的综合运用
        - 根据实际问题灵活运用所学模式识别系统相关知识
    ]
  )
)

#pagebreak()

= 基本概念

== 非监督模式识别

/ 非监督模式识别 (Unsupervised Pattern Recognition): 在没有已知类别标签的情况下，通过分析数据的内在结构或相似性，将数据划分为若干类别或聚类的机器学习方法

#align(
  center, 
  grid(
    columns: (1fr, 1fr, 1fr), 
    inset: 5pt, 
    align: horizon,
    card(
      title: [基于模型的聚类]
    )[基于模型的聚类方法假设样本在特征空间中的概率分布，并寻找样本分布密度的单峰区域进行聚类。], 
    card(
      title: [基于距离的聚类]
    )[
      基于距离的聚类方法直接利用样本之间的距离或相似性来划分聚类。例如，K-均值（K-means）算法通过计算样本到聚类中心的距离，将样本分配到最近的聚类中。ISODATA 方法则是一种动态聚类算法，可以根据数据密度自动调整聚类的数量。
    ], 
    card(
      title: [模糊聚类]
    )[
      模糊聚类方法允许样本同时属于多个聚类，每个样本对各个聚类的隶属度由一个模糊度表示。模糊C-均值（FCM）算法是模糊聚类的典型代表，它通过优化目标函数来调整样本的隶属度。
    ],
    card(
      title: [自组织映射（SOM）]
    )[
      自组织映射是一种无监督学习方法，通过神经网络的自组织特性，将高维数据映射到低维网格上，从而发现数据的内在结构。SOM 网络通过竞争学习机制，使网络中的神经元逐渐适应输入数据的分布。
    ], 
    card(
      title: [基于密度的聚类]
    )[
      基于密度的聚类方法（如DBSCAN）通过计算样本的局部密度来划分聚类，能够有效识别噪声点和任意形状的聚类。
    ]
  )
)

#table(
  columns: (auto, 1fr, 1fr), 
  fill: (x, y) => if y == 0 {luma(80%)}, 
  align: center+horizon, 
  [*特征*], [*监督模式识别*], [*非监督模式识别*], 
  [是否有类别标签], [是], [否], 
  [学习目标], [分类], [聚类], 
  [评估方法], [准确率、错误率等], [聚类质量指标、稳定性评估], 
  [数据要求], [需要已知类别样本], [无需已知类别标签], 
  [模型设计], [分类器（如SVM、神经网络）], [聚类算法（如K-均值、DBSCAN）], 
  [数据分布假设], [有明确假设], [更加灵活], 
)

== 最小错误率贝叶斯决策

最小错误率贝叶斯决策的目标是使分类错误的概率最小。具体来说，它要求在所有可能的决策中，选择使错误率最小的决策规则。错误率的定义为：
$ P(c) = integral P(c | x) p(x) d x $
其中，$P(c | x)$是在特征$x$下属于类别$c$的后验概率，$p(x)$是特征$x$的先验概率密度函数。通过最小化$P(c)$，可以得到最小错误率贝叶斯决策的最优分类规则。

/ 最小错误率贝叶斯决策: 对于二分类问题 $ "If" P(omega_1 | x) >= P(omega_2 | x), "Then" x in omega_1 $

其中，后验概率用贝叶斯公式求得
$ P(omega_i | x) = (p(x | x_i)P(omega_i)) / p(x) = (p(x | omega_1) P(omega_2)) / (p(x | omega_1) P(omega_1) + p(x | omega_2) P(omega_2)) $

由于先验概率$P(omega_i)$是确定的, 人们经常把决策规则整理成如下形式: 
$ "If" I(x) = P(x | omega_1) / P(x | omega_2) >= lambda = P(omega_2) / P(omega_1), "Then" x in cases(omega_1, omega_2) $
对于每一个样本$x$，计算其似然比$I(x)$，并与阈值$lambda$比较，若$I(x) >= lambda$，则决策为第一类，否则决策为第二类。

概率比$I(x)$反映了在$omega_1$类中观察到特征值$x$的相对可能性，也称为*似然度*，而$I(x)$本身被称为*似然比（likelihood ratio）*
。

为了简化计算，人们通常使用*对数似然比*$h(x) = - ln [I(x)] = ln P(x | omega_1) + ln P(x | omega_2)$ 。此时，决策规则可以表示为：
$ 1 / 2 h(x) >= ln P(omega_1) / P(omega_2) $
若满足该条件，则$x$被决策为第一类，否则为第二类。

== 参数估计 (概率密度函数)

/ 参数估计（probability density function estimation）: 目标是根据已知的训练样本，估计出描述这些样本分布的概率密度函数$p(x)$

=== 参数估计（Parametric Estimation）

参数估计假设概率密度函数的形式是已知的，但其中的部分或全部参数未知。

1. 最大似然估计（Maximum Likelihood Estimation, MLE）

其基本思想是：

假设有若干样本，分为不同类别，在同一类内部的若干数据满足同样的概率密度函数。例如，有5+5两类共10个数据，前5个满足正态分布，后5个满足伯努利分布。

基于这种假设，对同一类内部的数据建立联合概率密度函数$L(theta)$。

$ L(theta) = Pi_(i = 1)^N p(x_i | theta) $

在$L(theta)$中只有$theta$是未知量，解方程$(d L(theta))/(d theta)=0$可以得到结果。

正态分布下最大似然参数估计公式为: 
- 均值: $hat(mu) = 1 / N sum_(i = 1)^N x_i$
- 方差: $hat(sigma)^2 = 1 / N sum_(i = 1)^N (x_i - hat(mu))^2$

2. 贝叶斯估计（Bayesian Estimation）

贝叶斯估计与最大似然估计不同，它将参数视为随机变量，并结合先验分布$p(theta)$和似然函数$p(x | theta)$，通过贝叶斯定理估计后验分布 $p(theta | x)$。

计算步骤如下：

(1).根据对问题的认识或猜测确定$p(theta)$

(2).求出样本集的联合分布

$ p(X|theta)=Pi_(i=1)^N p(x_i|theta) $

(3).求出$theta$的后验概率

$ p(theta|X) = (p(X|theta)p(theta))/(integral_Theta p(X|theta)p(theta)d theta) $

(4).根据上式，$theta$估计量：

$ theta^(star) =integral_(Theta) theta p(theta|X)d theta $

考虑到我们的最终目的不是求$theta$，而是做分类，即求样本出现在样本集的概率$p(x|X)$(贝叶斯决策)，可以直接通过以下表达式计算。

$ p(x|X) = integral_Theta p(x|theta)p(theta|X)d theta $

话题回归对参数的讨论。引入样本数量$N$，即样本集$X → X^N$。可以得到

$ p(theta|X^N) = (p(x_N|theta)p(theta|X^(N-1)))/(integral p(x_N|theta)p(theta|X^(N-1))d theta) $

随着$N$的增加，$p(theta|X^N)$会收敛于在参数真实值上的一个脉冲函数，这样的过程称为贝叶斯学习。此外样本概率密度函数也可以类似的逼近真实的密度函数
$ p(x|X^(N→ infinity)) = p(x) $

综合两种参数估计方法，参数估计都是得到$theta$的值，最终目的可以是计算$p(x|X)$用以分类(贝叶斯决策)。

=== 非参数估计（Nonparametric Estimation）

非参数估计不假设概率密度函数的具体形式，而是直接根据训练样本估计其分布。

#grid(
  columns: (1fr, 1fr), 
  inset: 5pt, 
  card(title: [直方图法 \ （Histogram Method）])[直方图法是最简单的非参数估计方法。它将样本空间划分为若干个小格（bin），统计每个小格内的样本数，然后将每个小格的样本数除以样本总数和小格体积，得到概率密度估计。], 
  grid.cell(
    rowspan: 2, 
    card(title: [Parzen 窗法 \ （Parzen Window Method）])[Parzen 窗法是一种基于核函数的非参数估计方法。它通过在每个样本点周围定义一个核函数（如高斯核），并计算所有样本点对该点的贡献，从而估计概率密度。其公式为：$ hat(p) (x) = 1 /N sum_(i = 1)^N K((x - x_i) / h) $其中，$K$是核函数，$h$是平滑参数（窗口宽度），$N$ 是样本总数。Parzen 窗法能够提供平滑的概率密度估计，并且在样本数足够大时，估计结果会收敛到真实密度函数。]
  ),
  card(title: [k-近邻估计 \ （k-Nearest Neighbor Estimation）])[k-近邻估计是一种基于局部密度的非参数估计方法。它通过计算每个样本点的k个最近邻的平均距离，来估计该点的概率密度。这种方法在处理高密度区域和低密度区域时表现良好。], 
)

=== 参数估计与非参数估计比较

#table(
  columns: (auto, 1fr, 1fr), 
  fill: (x, y) => if y == 0 { luma(80%) }, 
  align: center+horizon, 
  [*特征*], [*参数估计*], [*非参数估计*], 
  [假设], [概率密度函数形式已知], [不假设概率密度函数形式], 
  [估计目标], [估计参数, 如$mu, sigma^2$], [估计概率密度函数$p(x)$], 
  [优点], [估计结果更精确，适合已知分布形式], [无需假设分布形式，适用于未知分布], 
  [缺点], [需要先验知识，对分布形式敏感], [对样本数量要求较高，计算量较大], 
  [常用方法], [最大似然估计、贝叶斯估计], [直方图法、k-近邻估计、Parzen 窗法], 
  [应用场景], [已知分布形式], [未知分布形式]
)



== 分类器

/ 分类器: 一种函数或模型，它能够将输入数据映射到特定的类别

=== 分类器的分类

+ *线性分类器*：这类分类器通过线性函数来实现分类，例如线性判别函数$g(x) = omega^T x + omega_0$ ，其中$omega$是权向量，$x$是输入特征向量，$omega_0$是偏置项。线性分类器在处理简单数据分布时效果较好，但在处理复杂数据时可能需要通过特征变换或非线性方法来提高性能。
+ *非线性分类器*：非线性分类器通过非线性函数来实现分类，例如多层感知器（MLP）和支持向量机（SVM）等。这些分类器能够处理更复杂的分类问题，但通常需要更多的计算资源和训练时间。
+ *集成学习分类器*：集成学习方法通过组合多个弱分类器的输出来提高分类性能。常见的集成学习方法包括AdaBoost、随机森林和梯度提升树（GBDT）等。

=== 分类器的设计方法

+ *一对多（One-vs-Rest, OvR）* ：这种方法通过为每个类别训练一个分类器，将多类问题转化为多个二分类问题。每个分类器负责区分一个特定类别与其他所有类别。这种方法简单易实现，但可能需要训练多个分类器。
+ *逐对（One-vs-One, OvO）* ：这种方法通过为每对类别训练一个分类器，将多类问题转化为多个二分类问题。每个分类器负责区分两个特定类别。这种方法在类别数量较多时可能需要训练较多的分类器，但每个分类器的决策边界更清晰，分类结果更可靠。
+ *直接多类分类器*：这种方法直接设计多类分类器，例如多类支持向量机（Multiclass SVM）和多层感知器（MLP）。这些分类器能够直接处理多类问题，无需将多类问题分解为多个二分类问题。这种方法通常能够提供更好的分类性能，但设计和训练过程较为复杂。

=== 分类器的训练与评估

+ *特征提取与选择*：为了提高分类器的性能，通常需要对输入数据进行特征提取和选择。特征提取是将原始数据转换为更简洁的特征表示，而特征选择则是从提取的特征中选择最相关的特征。特征选择的方法包括*过滤法*和*包裹法*。
+ *分类器训练*：分类器训练是根据训练数据调整分类器的参数，使其能够更好地拟合训练数据。训练过程通常涉及优化算法，如梯度下降法、随机梯度下降法（SGD）等。分类器的训练目标是使分类器在训练数据上的分类误差最小化。
+ *分类决策*：分类决策是将训练好的分类器应用于新的未知样本，进行分类。分类决策的过程通常涉及计算分类器的输出，并根据输出结果进行分类。分类器的输出可以是类别标签，也可以是类别概率。

=== 分类器的性能评估

+ *交叉验证*：交叉验证是一种评估分类器性能的方法，通过将数据集划分为多个子集，轮流使用不同的子集作为训练集和验证集，评估分类器的性能。交叉验证可以减少数据划分的随机性，提高评估结果的可靠性。
+ *置信区间估计*：置信区间估计是一种评估分类器性能的方法，通过计算分类器的错误率的置信区间，评估分类器的性能。置信区间估计可以提供分类器性能的统计意义，帮助评估分类器的可靠性。
+ 错误率估计：错误率估计是评估分类器性能的基本方法，通过计算分类器在测试数据上的错误率，评估分类器的性能。错误率估计可以提供分类器性能的直观评估，但需要足够的测试数据支持。

== 支持向量机

/ 支持向量机: 一种用于分类和回归的机器学习算法，其核心思想是通过最大化分类间隔来提高模型的推广能力。SVM 通过引入*核函数*，可以*将原始数据映射到高维空间，从而将非线性问题转化为线性问题*，实现更有效的分类。

=== 基本原理

SVM 的核心在于寻找一个*最优的分类超平面*，使得两类样本之间的分类间隔最大化。这个*分类间隔*是指分类面与最近样本之间的距离。通过最大化这个间隔，SVM 能够在有限样本的情况下获得良好的推广能力。

=== 核函数的作用

为了处理非线性可分的问题，SVM 引入了*核函数（Kernel Function）*。核函数的作用是*将原始数据映射到一个高维特征空间，使得在高维空间中数据变得线性可分*。通过核函数，SVM 可以在不显式计算高维特征的情况下，间接地进行非线性变换。常用的核函数包括*多项式核、径向基函数（RBF）核和 Sigmoid 核*。

=== SVM 的优化问题

SVM 的目标是找到一个最优的分类超平面，使得分类间隔最大化。这个过程可以通过求解一个二次优化问题来实现。对于线性可分的情况，SVM 的优化问题可以表示为：
$ max_alpha sum_(i = 1)^n alpha_i - 1 / 2 sum_(i, j = 1)^n alpha_i alpha_j y_i y_j(x_i dot x_j) $
其中，$alpha_i$是拉格朗日乘子，$y_i$是样本的类别标签，$x_i$是样本的特征向量。

== 核函数

/ 核函数: $K(x, x')$是一个对称函数，它满足 Mercer 条件，即对于任意的$phi.alt != 0$且$integral phi.alt^2(x) d x < infinity$，有：
$ integral.double K(x, x') phi.alt(x) phi.alt(x') d x d x' $ 
这表明，只要满足 Mercer 条件，核函数就可以构建一个非线性的支持向量机。通过核函数，SVM 可以在不显式计算高维特征的情况下，间接地在高维空间中进行分类。

常用的核函数包括：
+ *多项式核函数*：$K(x, x') = ((x, x') + 1)^q$，实现的是$q$阶的多项式判别函数。
+ *径向基（RBF）核函数*：$K(x, x') = exp(- (|| x - x' ||^2) / sigma^2)$，实现与径向基网络形式相同的决策函数。
+ *Sigmoid 核函数*：$K(x, x') = tanh(nu(x dot x') + c)$，在$nu$和$c$满足一定取值条件的情况下等价于包含一个隐层的多层感知器神经网络。

== 特征选择

/ 特征选择: 目的是从一组给定的特征中选择出最有利于分类或聚类的特征子集，以降低特征空间的维数，提高模型的性能和效率。特征选择不仅有助于减少计算复杂度，还能避免“维度灾难”（即特征过多导致模型过拟合或计算资源浪费）。

=== 特征选择的分类

+ *过滤法（Filter Methods）*: 
  过滤法是一种独立于分类器的特征选择方法，它通过统计指标（如类内类间距离、方差、信息增益等）来评估特征之间的相关性或与类别之间的可分性，从而选择最优特征。过滤法的优点是计算效率高，但缺点是无法考虑特征之间的相互作用，因此可能无法得到最优的特征组合。

+ *包裹法（Wrapper Methods）*: 
  包裹法将特征选择与分类器的性能直接结合，通过迭代地选择或剔除特征来优化分类器的性能。例如，递归特征消除（RFE）和递归支持向量机（R-SVM）等方法，通过评估特征在分类器中的贡献来选择特征。包裹法能够更好地利用分类器的性能，但计算成本较高。
+ *嵌入法（Embedded Methods）*: 
  嵌入法是在分类器的设计过程中直接集成特征选择的目标。例如，L1正则化（Lasso 回归）通过在目标函数中引入对特征数目的惩罚项，强制模型使用尽可能少的特征来达到最佳性能。嵌入法能够同时优化分类和特征选择，但对分类器的结构有较高要求。

== 流形学习

流形学习（manifold learning）是一种非监督学习方法，其核心思想是*假设高维数据中存在低维的内在结构*，即数据点在高维空间中可能分布在某个低维流形上。这种低维结构反映了数据的内在几何特性，而不仅仅是随机噪声或高维空间中的随机分布。通过识别和建模这种低维结构，流形学习旨在从高维数据中提取出有意义的信息，从而实现降维、可视化或分类等任务。

具体来说，流形学习的目标是*找到一个低维嵌入空间*，使得在这个空间中，数据点之间的距离关系尽可能地*保留原始高维空间中的相似性*。

t-SNE（t-distributed stochastic neighbor embedding）是一种基于流形学习的降维方法，它通过将高维数据映射到低维空间，同时保留数据点之间的局部相似性。t-SNE *通过计算每个数据点与其邻居之间的相似性，并在低维空间中调整这些点的位置*，使得相似的数据点在低维空间中保持较近的距离，而不相似的点则保持较远的距离。

== k-means 算法 (K 均值, C 均值)

!待确认

K-means 算法是一种常用的聚类算法，其核心思想是通过迭代寻找$c$个聚类的划分方案，使得用这$c$个聚类的均值来代表相应各聚类样本时所得到的总体误差最小。C 均值算法有时也被称为 K 均值算法，在向量量化和图像分割等领域有广泛应用。

K-means 算法的基本步骤如下：
+ 初始化：确定初始聚类中心$m_1, m_2, dots, m_c$，通常使用随机选择方法。
+ 分配：将每个样本分配到最近的聚类中心。
+ 更新：重新计算每个聚类的均值作为新的聚类中心。
+ 迭代：重复分配和更新步骤，直到聚类中心不再显著变化或达到预设的迭代次数。
K-means 算法的目标是通过不断调整样本的类别归属来求解最小误差平方和准则下的最优结果。该算法在处理高维数据时可能会受到噪声和初始值选择的影响，因此在实际应用中需要谨慎选择初始聚类点和聚类数目$c$。

== 深度学习

深度学习是一种机器学习方法，通常指具有多层结构的神经网络模型。它最早在1986年被提出，但直到21世纪第二个十年才真正流行起来。深度学习的核心在于*通过多层非线性变换来自动提取数据的特征*，从而实现对复杂模式的识别和学习。这种多层结构使得深度学习能够处理高维数据，并在图像识别、自然语言处理、语音识别等领域取得了显著的成果。

深度学习的典型模型包括*卷积神经网络（CNN）、循环神经网络（RNN）、深度信念网络（DBN）、自编码器等*。其中，卷积神经网络通过卷积运算和非线性激活函数来提取图像特征，而循环神经网络则适用于处理序列数据。深度信念网络（DBN）是第一个非卷积的深度神经网络模型，它通过逐层构建的贪婪学习算法进行训练，能够有效解决多层感知器的训练困难问题。此外，深度学习还引入了生成对抗网络（GAN）等模型，用于生成新的数据样本。

深度学习的一个重要特点是*“表示学习”（representation learning）*，即*通过神经网络自动学习数据的特征表示，而不是依赖人工设计的特征*。这种自动特征提取的能力使得深度学习在许多实际应用中表现出色，例如图像识别、语音识别和自然语言处理。然而，深度学习也面临一些挑战，如过拟合问题、优化困难和理论基础不足等。为了解决这些问题，研究者们提出了多种技巧和方法，如*随机舍弃（dropout）、归一化、数据增强*等。

== 激活函数

/ 激活函数: 神经网络中用于引入非线性关系的关键组成部分。在神经网络中，每一层的神经元通常会对输入进行加权求和，然后通过一个函数将结果映射到输出。这个函数就是激活函数，它决定了神经元的输出是否被激活，以及如何响应输入的变化。

=== 激活函数的作用

+ 引入非线性：
  激活函数使得神经网络能够学习和表示复杂的非线性关系。如果没有激活函数，无论神经网络有多少层，其整体输出都只能是输入的线性组合，无法处理非线性问题。因此，激活函数是神经网络实现非线性映射的基础。
+ 控制输出范围：
  不同的激活函数会对输出值的范围进行限制。这种限制有助于神经网络在训练过程中保持数值的稳定性。
+ 影响梯度传播：
  激活函数的导数决定了梯度在反向传播过程中的传播情况。如果激活函数的导数在某些区域趋近于 0，就会导致梯度消失问题，使得网络难以训练。
+ 提升模型性能：
  适当的激活函数可以提升模型的收敛速度和泛化能力。
+ 影响训练过程：
  激活函数的选择还会影响训练过程中的梯度下降效果。此外，激活函数的使用通常与归一化技术（如批量归一化）结合使用，以进一步提升训练效率和模型性能。

=== 常见的激活函数

#table(
  columns: (auto, 2fr, 3fr), 
  fill: (x, y) => if y == 0 {luma(80%)}, 
  align: center+horizon, 
  [*名称*], [*公式*], [*说明*], 
  [Sigmoid], $ sigma(x) = 1 / (1 + e^(-x)) $, [Sigmoid 函数将输出限制在 0 到 1 之间，常用于二分类任务中的输出层。然而，由于其在两侧的导数趋近于 0，容易导致梯度消失问题。], 
  [tanh], $ tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x)) $, [tanh 函数将输出限制在 -1 到 1 之间，与 Sigmoid 函数相比，其输出均值更接近 0，有助于梯度下降更接近自然梯度，从而加快收敛速度。], 
  [ReLU], $ f(x) = max(0, x) $, [ReLU 函数在正区间导数恒为 1，可以缓解梯度消失问题，但其在负区间没有输出，可能导致“神经元死亡”现象。为了解决这一问题，研究者提出了多种改进形式，如 Leaky ReLU、Parametric ReLU（PReLU）和 Maxout 等。], 
  [Softmax], $ "Softmax"(x_i) = e^(x_i) / (sum_j e^(x_j)) $, [Softmax 函数通常用于多分类任务的输出层，它将输出转换为概率分布。 ]
)

== 梯度消失与梯度爆炸

梯度消失与梯度爆炸是深度学习中常见的训练问题，主要发生在神经网络的反向传播过程中。它们指的是在训练过程中，误差梯度在反向传播时出现异常的放大或缩小现象，从而影响模型的训练效果。

=== 梯度消失（Vanishing Gradient）

/ 梯度消失: 在反向传播过程中，误差梯度随着网络层数的增加而逐渐减小，最终趋近于零。这种现象会导致网络中较深的层无法接收到有效的梯度信息，从而无法进行有效的参数更新，使得网络难以训练出好的结果。

原因：在传统的神经网络中，尤其是使用Sigmoid等激活函数时，其导数在输入值接近0时非常小，导致梯度在反向传播过程中逐渐衰减。当网络层数较多时，梯度会越来越小，最终趋于零，使得深层网络的参数无法被有效更新。

影响：梯度消失会使得网络难以学习到复杂的特征，尤其是在深层网络中，深层的参数无法被有效更新，导致模型性能下降。

=== 梯度爆炸（Exploding Gradient）

/ 梯度爆炸: 在反向传播过程中，误差梯度随着网络层数的增加而逐渐增大，最终变得非常大。这种现象会导致网络中的参数更新变得不稳定，甚至溢出，使得训练过程无法进行。

原因：在反向传播过程中，误差梯度会与权值矩阵相乘。如果权值矩阵的绝对值较大，或者网络层数较多，误差梯度可能会在反向传播过程中不断放大，最终导致梯度爆炸。

影响：梯度爆炸会导致参数更新变得不稳定，甚至溢出，使得训练过程无法进行。这通常发生在处理长序列数据的RNN（循环神经网络）中，尤其是在时间序列较长的情况下。

=== 解决方法

+ 使用*不同的激活函数*：例如ReLU函数，其导数在正区间恒为1，可以有效缓解梯度消失问题
。
+ 引入*LSTM*（长短时记忆网络） ：LSTM通过引入记忆状态和控制门（输入门、遗忘门、输出门），可以有效控制梯度的流动，从而缓解梯度消失和梯度爆炸问题。
+ *批量归一化（Batch Normalization）* ：通过在激活函数之前对输入进行归一化，可以加速训练过程并缓解梯度消失问题。
+ 调整网络结构：例如使用*残差连接（Residual Connection）*，可以缓解梯度消失问题，使得深层网络更容易训练。

== 交叉验证

/ 交叉验证（Cross-Validation, CV）: 一种在机器学习和统计学中广泛使用的评估方法，用于估计分类器或模型的泛化能力。其核心思想是通过将数据集划分为多个子集，轮流使用这些子集作为训练集和测试集，从而多次评估模型的性能，最终通过平均结果来减少因数据划分不均带来的方差影响。

=== 交叉验证的基本思想
在现有总体样本不变的情况下，随机选用一部分样本作为临时的训练集，用剩余样本作为临时的测试集，得到一个错误率估计；然后随机选用另外一部分样本作为临时训练集，其余样本作为临时测试集，再得到一个错误率估计...如此反复多次，最后将各个错误率求平均，得到交叉验证错误率（cross-validation error rate，CV error）。

=== 交叉验证的典型做法

交叉验证的典型做法是所谓$n$倍交叉验证法（n-fold cross-validation）。其具体步骤如下：
+ 数据划分：将全部样本随机地划分为$n$个等份。
+ 轮流测试：在一轮实验中，轮流抽出其中的$1$份样本作为测试样本，用其余$n - 1$份作为训练样本，得到$n$个错误率。
+ 平均错误率：将这些错误率进行平均，作为一轮交叉验证的错误率。
+ 多轮划分：由于对样本的每一次划分是随意的，人们往往进行多轮这样的划分（例如$k$轮），得到多个交叉验证错误率估计，最后将多个估计再求平均。

这种做法又称为$k$轮$n$倍交叉验证。人们经常用的$n$值有$3, 5, 10$等，分别称为三倍交叉验证（3-fold cross-validation）、五倍交叉验证（5-fold cross-validation）、十倍交叉验证（10-fold cross-validation）等。

=== 交叉验证的特殊形式

交叉验证的一种特殊形式是所谓的留一法交叉验证（leave-one-out cross-validation, LOOCV）。它的做法是不把样本进行分组，而是每轮实验拿出一个样本来作为测试样本，用其余的$n - 1$个样本作为训练样本集，训练分类器，测试对抽出的那个样本的分类是否正确；在下一轮实验中，把之前测试的样本放回，拿出另外一个样本作为测试样本，用剩余的$n - 1$个样本作训练，再对抽出的样本作测试；依此类推，直到每个样本都被作为测试样本一次。全部$n$轮实验完成后，统计总共出现的测试错误数（不妨记作$m$），$m$占总样本数的比例就是留一法交叉验证错误率。

可以证明，交叉验证法获得的错误估计是对错误率的一种最大似然估计。

= 模式识别系统典型构成

模式识别系统的典型构成通常包括以下几个主要部分：*原始数据的获取和预处理、特征提取与选择、分类或聚类、后处理*。

1. 原始数据的获取和预处理

这是模式识别系统的起点，涉及从实际问题中获取原始数据，并对其进行必要的预处理。预处理的目的是为了提高数据的质量，使其更适合后续的特征提取和分类或聚类。例如，在图像识别中，原始数据可能包括图像的像素值，预处理可能包括灰度化、去噪、归一化等操作。在语音识别中，原始数据可能是声波信号，预处理可能包括采样、分帧、加窗等操作。

2. 特征提取与选择

特征提取是从原始数据中提取出能够有效区分不同类别的特征。这些特征可以是数值型的，也可以是非数值型的，如颜色、形状等。特征提取的方法包括*主成分分析（PCA）、Karhunen-Loève变换（K-L变换）、多维尺度法（MDS）*等。特征选择则是从提取出的特征中选择出最相关的特征，以减少计算复杂度并提高分类或聚类的准确性。特征选择的方法包括*分段定界算法、遗传算法、包裹法*等。

3. 分类或聚类

分类是监督模式识别的核心任务，其目的是将输入数据分配到预定义的类别中。分类方法包括*基于模型的分类方法（如隐马尔可夫模型、贝叶斯网络）、基于决策树的分类方法（如ID3算法）、基于神经网络的分类方法（如多层感知器、卷积神经网络）*等。聚类是非监督模式识别的核心任务，其目的是将输入数据划分为不同的类别，而不需要预先定义类别。聚类方法包括*K均值算法（K-means）、模糊C均值算法（FCM）、自组织映射（SOM）*等. 

4. 后处理

后处理是对分类或聚类结果进行进一步的优化和解释。例如，在分类任务中，后处理可能包括对分类结果进行评估，如计算分类准确率、召回率等；在聚类任务中，后处理可能包括对聚类结果进行解释，如分析聚类类与研究目标之间的关系，根据领域知识对聚类结果进行进一步的处理。

- 监督模式识别：通常包括分析问题、原始特征获取、特征提取与选择、*分类器设计、分类决策*等步骤。
- 非监督模式识别：通常包括分析问题、原始特征获取、特征提取与选择、*聚类分析、结果解释*等步骤。

= 贝叶斯估计

贝叶斯估计是一种基于贝叶斯定理的统计推断方法，用于在已知先验分布和似然函数的情况下，估计参数的后验分布，并从中提取参数的估计值。它与最大似然估计（MLE）不同，贝叶斯估计将参数视为随机变量，结合先验知识和观测数据，通过最小化期望风险来估计最优参数。

== 算法原理及步骤

贝叶斯估计的核心思想是利用贝叶斯定理，将后验概率分布表示为先验概率分布与似然函数的乘积，再除以归一化常数。具体来说，贝叶斯估计的公式如下：$ p(theta | x) = (p(x | theta) p(theta)) / p(x) $
其中：
- $p(theta | x)$是后验概率分布，表示在观测数据$x$下参数$theta$的分布。
- $p(x | theta)$是似然函数，表示在参数$theta$下观测数据$x$的概率。
- $p(theta)$是先验概率分布，表示在没有观测数据之前对参数$theta$的信念。
- $p(x)$是归一化常数，确保后验分布的总和为 1。

贝叶斯估计的目的是通过最大化后验概率$p(theta | x)$来找到最优参数估计。在实际应用中，由于$p(x)$是一个常数，通常可以忽略，因此贝叶斯估计简化为： $ hat(theta) = arg max_theta p(x | theta) p(theta) $

*具体步骤如下*

定义先验分布：选择一个先验分布$p(theta)$，表示在没有观测数据之前对参数$theta$的信念。常见的先验分布包括均匀分布、正态分布等。

定义似然函数：根据观测数据$x$和模型假设，定义似然函数$p(x | theta)$，表示在参数$theta$下观测数据$x$的概率。

计算后验分布：利用贝叶斯定理计算后验分布$p(theta | x)$，即：
$ p(theta | x) = (p(x | theta) p(theta)) / p(x) $
其中，$p(x)$是归一化常数，可以通过积分计算：
$ p(x) = integral p(x | theta) p(theta) d theta $

估计参数：根据后验分布$p(theta | x)$，选择一个参数估计值$hat(theta)$。常见的估计方法包括：
- 最大后验估计（MAP）：选择使后验概率$p(theta | x)$最大的参数值。
- 期望最大化（EM）算法：用于处理隐变量的情况，通过迭代优化似然函数。

验证与评估：评估估计参数的性能，可以通过交叉验证、误差分析等方法进行。

== 朴素贝叶斯估计的计算应用 (计算题)

朴素贝叶斯估计是一种基于贝叶斯定理的分类方法，它假设特征之间相互独立，从而简化了联合概率分布的计算。这种方法在文本分类、垃圾邮件过滤、推荐系统等领域有广泛应用。下面将详细介绍朴素贝叶斯估计的计算应用，并通过一个具体的例子来说明其计算过程。

=== 原理

朴素贝叶斯估计的核心思想是利用贝叶斯定理，*将后验概率分布表示为先验概率分布与似然函数的乘积*。具体来说，朴素贝叶斯估计的公式如下：

$ P(omega_j | x) = (P(x | omega_j) P(omega_j)) / (sum_(i = 1)^c P(x | omega_i) P(omega_i)) $
 
其中：
- $P(omega_j | x)$是在观测数据$x$下类别$omega_j$的后验概率。
- $P(x | omega_j)$是在类别$omega_j$下观测数据$x$的概率（似然函数）。
- $P(omega_j)$是类别$omega_j$的先验概率。
- 分母是所有类别的后验概率的总和，用于归一化。

=== 步骤

+ 定义先验概率：根据训练数据，计算每个类别的先验概率$P(omega_j)$。通常，先验概率可以通过训练数据中各类样本的比例来估计。
+ 定义似然函数：假设每个特征在给定类别下服从某种分布（如正态分布或伯努利分布），并计算每个特征在给定类别下的条件概率$P(x_i | omega_j)$。
+ 计算后验概率：利用贝叶斯公式计算每个类别的后验概率$P(omega_j | x)$。
+ 决策：选择后验概率最大的类别作为最终的分类结果。

=== 例题

假设我们有一个客户数据集，包含客户的年龄、性别、收入和是否购买车的信息。我们希望使用朴素贝叶斯估计来预测一个新客户是否可能会买车。

1. 数据准备

假设我们有以下数据：
#align(
  center, 
  table(
    fill: (x, y) => if y == 0 {luma(80%)}, 
    align: center+horizon, 
    columns: 4, 
    [*年龄*], [*性别*], [*收入*], [*是否购买车*], 
    [25], [男], [低], [否], 
    [30], [女], [中], [是], 
    [35], [男], [高], [是], 
    [40], [女], [中], [是], 
    [45], [男], [高], [是],
  )
)

2. 定义先验概率

假设我们有两类：$omega_1$表示“购买车”，$omega_2$表示“不购买车”。根据数据，我们估计先验概率：
$ P(omega_1) = 3 / 5, P(omega_2) = 2 / 5 $
 
3. 定义似然函数

假设每个特征在给定类别下服从正态分布。例如，年龄在“购买车”类别下的均值为 35，方差为 10；在“不购买车”类别下的均值为 30，方差为 10。收入在“购买车”类别下的均值为 30，方差为 10；在“不购买车”类别下的均值为 20，方差为 10。

4. 计算后验概率

假设我们有一个新客户的数据：年龄 35，性别 女，收入 中。我们需要计算该客户属于“购买车”和“不购买车”的后验概率。

- 年龄：在“购买车”类别下的概率为$P(35 | omega_1)$，在“不购买车”类别下的概率为$P(35 | omega_2)$。
- 性别：假设性别为“女”在“购买车”类别下的概率为 0.5，而在“不购买车”类别下的概率为 0.5。
- 收入：在“购买车”类别下的概率为$P("中" | omega_1)$，在“不购买车”类别下的概率为$P("中" | omega_2)$。
计算每个特征的条件概率：
$
  P(35 | omega_1) = 1 / sqrt(2 pi dot 10) exp(- (35 - 35)^2 / (2 dot 10)) = 1 / sqrt(20 pi) \
  P(35 | omega_2) = 1 / sqrt(2 pi dot 10) exp(- (35 - 30)^2 / (2 dot 10)) = 1 / sqrt(20 pi) e^(- 5 / 4) \ 
  P("女" | omega_1) = P("女" | omega_2) = P("中" | omega_1) = P("中" | omega_2) = 0.5
$

计算后验概率：
$
  P(omega_1 | x) = (P(35 | omega_1) dot P("女" | omega_1) dot P("中" | omega_1) dot P(omega_1)) / P(x) \
  P(omega_2 | x) = (P(35 | omega_2) dot P("女" | omega_2) dot P("中" | omega_2) dot P(omega_2)) / P(x)
$
 
由于$P(x)$是归一化常数，可以忽略，因此我们只需比较分子部分：
$
  P(omega_1 | x) prop P(35 | omega_1) dot P("女" | omega_1) dot P("中" | omega_1) dot P(omega_1) \ 
  P(omega_2 | x) prop P(35 | omega_2) dot P("女" | omega_2) dot P("中" | omega_2) dot P(omega_2)
$

计算具体值：
$
  P(omega_1 | x) prop 1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2 dot 3 / 5 \ 
  P(omega_2 | x) prop 1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2 dot 2 / 5 
$


由于$1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2$是相同的，显然$P(omega_1 | x) > P(omega_2 | x)$，因此该客户更可能属于“购买车”类别。

= 特征提取/降维

== 主成分分析

/ 主成分分析（Principal Component Analysis, PCA）: 一种经典的线性降维方法，其核心思想是通过线性变换将高维数据映射到低维空间，同时保留数据的主要变化信息。PCA 的目标是*找到一组正交的新特征（主成分）*，这些新特征是*原始特征的线性组合，并且彼此之间不相关*。

步骤如下：

+ 计算协方差矩阵：对数据进行中心化处理，计算协方差矩阵。
+ 特征值分解：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。
+ 选择主成分：根据特征值的大小，选择前$k$个最大的特征值对应的特征向量作为主成分。
+ 投影到新空间：将原始数据投影到由这些主成分构成的新空间中，实现降维。

PCA 是一种非监督学习方法，不考虑样本的类别信息，*仅关注数据的方差最大化*。在很多情况下，PCA 被用于图像识别、人脸识别等任务中，例如通过“本征脸”方法提取人脸图像的特征 。

优点：
- 简单高效，计算成本低。
- 保留了数据的主要变化信息。
- 适用于高维数据的可视化。

缺点：
- 无法处理非线性关系。
- 对噪声敏感，可能丢失部分重要信息。

== 多维尺度法

/ 多维尺度法（Multidimensional Scaling, MDS）: 一种用于数据可视化的方法，其目标是将高维数据映射到低维空间（通常是二维或三维），同时*尽可能保留数据点之间的距离关系*。MDS 有度量型和非度量型两种类型，前者关注定量距离，后者关注定性关系。

步骤如下：
+ 计算距离矩阵：计算所有样本之间的距离或不相似度。
+ 映射到低维空间：通过优化算法，将样本映射到低维空间，使得映射后的距离尽可能接近原始距离。
MDS 的核心思想是通过*最小化给定距离与表示距离之间的误差*来实现数据的可视化。在模式识别中，MDS 可以用于分析样本之间的相似性或不相似性，例如在心理学、经济学等领域有广泛应用。

优点：
- 保留了样本之间的相对距离关系。
- 适用于非线性数据的可视化。

缺点：
- 计算复杂度较高。
- 对噪声和异常值敏感。

== 两种线性降维方法的区别

#align(
  center,
  table(
    fill: (x, y) => if y == 0 {luma(80%)},
    align: center + horizon,
    columns: 3,
    inset: 8pt, 
    [*特性*], [*主成分分析（PCA）*], [*多维尺度法（MDS）*],
    [目标], [保留数据的主要方差，实现降维], [保留样本之间的相对距离，实现可视化],
    [方法], [线性变换，基于协方差矩阵], [非线性映射，基于距离矩阵],
    [监督性], [非监督学习], [通常为非监督学习],
    [适用场景], [高维数据的降维和特征提取], [数据的可视化和相似性分析],
    [计算复杂度], [较低], [较高],
    [对噪声的敏感性], [较高], [较高],
    [是否考虑类别信息], [否], [否],
  )
)


= 类别可分性判据

== 在刻画特征对分类的贡献时需要希望满足那些条件

类别可分性判据是特征选择和特征提取中的重要概念，用于衡量特征对分类任务的贡献。在刻画特征对分类的贡献时，通常希望满足以下条件：

+ 与错误率的单调关系：判据应与分类器的错误率（或其上界）具有单调关系，即判据值越大，分类性能越好。这样可以确保判据能够较好地反映分类目标。
+ 可加性：当特征独立时，判据对特征应该具有可加性，即多个特征的联合判据等于各个特征判据的和。这有助于在特征选择中综合考虑多个特征的贡献。
+ 度量特性：判据应满足非负性、对角线为零以及对称性。具体来说，当类别i与类别j相同时，判据值为零；当类别不同且判据值越大，表示分离程度越高。
+ 单调性：理想的判据应满足加入新的特征不会使判据值减小，即判据值随着特征数目的增加而单调不减。这有助于保证特征选择的稳定性。

== 常见的四种类别可分性判据

+ 基于*类内类间距离*的可分性判据：这类判据通过计算类内离散度和类间离散度来衡量特征的可分性。例如，Fisher线性判别方法通过最大化类间离散度并最小化类内离散度来确定最佳投影方向。这类判据直观易实现，但难以与分类错误率建立直接联系。
+ 基于*概率分布*的可分性判据：这类判据考虑了样本的分布情况，可以与错误率建立联系。例如，Bhattacharyya距离、Chernoff界限和散度等概率距离度量，以及基于熵的可分性判据。这些判据能够更好地反映样本在特征空间中的分布情况。
+ 基于*熵*的可分性判据：通过信息论中的熵概念来衡量特征对分类的有效性。具体来说，后验概率的熵越低，表示样本的类别分布越集中，特征越有利于分类。基于熵的可分性判据通过积分后验概率的熵来评估特征的整体可分性。
+ 基于*统计检验*的可分性判据：利用统计学中的假设检验方法，如 t检验和秩和检验，来判断特征在两类样本间是否存在显著差异。这些方法可以给出一个统计量来反映两类样本间的差别，并给出一个p-值来反映这种差异的统计显著性。

= 非监督模式识别/聚类算法

== k-means 算法

k-means 是一种经典的聚类算法，其基本思想是将数据划分为$k$个簇，使得每个簇内的样本尽可能相似，而簇之间尽可能不相似。该算法通过迭代优化目标函数（通常是最小化样本到其所属簇中心的平方距离之和）来实现聚类。

优点：
- 算法简单，易于实现。
- 对大规模数据集具有较好的效率。
- 适用于数据分布较为均匀的场景。

缺点：
- 需要预先指定聚类数目$k$，这在实际应用中可能难以确定。
- 对初始中心的选择敏感，容易陷入局部最优。
- 无法处理噪声和异常值，对非球形分布的数据效果较差。

== 模糊C均值（FCM）算法

模糊C均值 是 k-means 的一种扩展，它允许样本同时属于多个簇，并赋予每个样本到各个簇的隶属度。这种模糊性使得 FCM 在处理重叠或边界模糊的簇时表现更好。

核心思想：
- 通过隶属度函数$mu_(i j)$表示样本$x_i$到簇$j$的隶属程度。
- 通过迭代优化目标函数$ J = sum_(j = 1)^c sum_(i = 1)^n [mu_(i j)]^2 || x_i - m_j ||^2 $，其中$m_j$是簇$j$的中心。

优点：
- 能够处理重叠的簇，提供更细粒度的分类结果。
- 通过调整隶属度参数$k$，可以控制聚类的模糊程度。

缺点：
- 对初始中心和隶属度参数敏感。
- 计算复杂度较高，尤其在大规模数据集上。

== ISODATA 算法

ISODATA（Iterative Self-Organizing Data Analysis Technique）是一种改进的 k-means 算法，它通过动态调整聚类数目和中心，以适应数据分布的变化。

核心思想：
- 通过设定参数（如合并参数、分裂参数）自动调整聚类数目。
- 如果某个簇的样本数量过少，则将其合并到相邻簇中；如果簇之间距离较远，则将其分裂为两个簇。

优点：
- 无需预先指定聚类数目，能够自动调整聚类数目。
- 通过合并和分裂操作，能够更好地适应数据分布的变化。

缺点：
- 参数设置较为复杂，需要根据具体问题进行调整。
- 对噪声和异常值的鲁棒性较差。

== DBSCAN 算法

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，它通过识别高密度区域来定义簇，能够有效处理噪声和异常值。

核心思想：
- 定义两个关键参数：$epsilon.alt$（邻域半径）和$"MinPts"$（最小样本数）。
- 一个簇是由其邻域内至少包含$"MinPts"$个样本的区域构成的。
- 通过密度来定义簇，能够发现任意形状的簇。

优点：
- 无需预先指定聚类数目，能够自动发现任意形状的簇。
- 对噪声和异常值具有较强的鲁棒性。
- 适用于密度分布不均匀的数据。

缺点：
- 参数$epsilon.alt$和$"MinPts"$的选择对结果影响较大。
- 在高维数据中效果较差。

== 密度峰值聚类（DPC）算法

密度峰值聚类 是一种基于密度的聚类算法，它通过计算每个样本的局部密度和距离最近的高密度区域的距离来定义簇。

核心思想：
- 一个簇由两个条件定义：高密度和低距离。
- 高密度的样本通常位于簇的中心，而低距离的样本则靠近簇的中心。

优点：
- 无需预先指定聚类数目，能够自动发现簇。
- 对噪声和异常值具有较强的鲁棒性。
- 适用于高维数据。

缺点：
- 参数选择对结果影响较大。
- 在某些复杂数据分布中效果可能不如 DBSCAN。

= 循环神经网络

/ 循环神经网络（Recurrent Neural Network，简称 RNN）: 一种专门用于处理序列数据的神经网络模型。它能够捕捉时间序列中的依赖关系，因此在自然语言处理、语音识别、时间序列预测、图像描述生成等领域有广泛应用。

== RNN 的基本结构

RNN 的核心思想是通过循环连接来处理时间序列数据。每个时刻的输入不仅依赖于当前的输入，还依赖于上一时刻的隐藏状态。这种结构使得 RNN 能够“记住”之前的信息，并在后续的处理中加以利用。

在 RNN 的基本结构中，每个节点代表一个隐藏层单元（也称为门控单元），通常用$h_t$表示。这些单元通过循环连接相互连接，形成一个闭环结构，允许信息在时间序列中持续流动。输入向量$x_t$和上一时刻的隐藏状态$h_(t - 1)$被串联起来，经过非线性层的线性变换得到当前时刻的隐藏状态$h_t$。整个结构体现了 RNN 的核心特性：记忆过去的信息并利用这些信息来预测未来。

== RNN 的输出形式

RNN 的输出形式可以根据任务的不同而变化。常见的输出形式包括：
- 逐条输出：对每个时间序列输入生成一个时间序列输出。
- 最终状态分类：在一个时间序列样本输入完毕后，以神经元最终状态向量为特征进行分类。
- 中间输出组合：在每个时间点产生一个中间输出，并组合起来构成特征向量进行分类决策。
- 静态输入生成时间序列输出：从静态输入中生成时间序列输出。

== RNN 的前身：Hopfield 网络

RNN 的前身是 Hopfield 网络，它是一种反馈网络，用于实现联想记忆功能。Hopfield 网络由一组带有反馈连接的阈值逻辑单元神经元组成，每个神经元的输出值称为“状态”。Hopfield 网络对信息的处理是一个动态的过程，通过迭代运算不断更新神经元的状态，最终收敛到一个稳定的状态向量或几个循环的状态向量之一。Hopfield 网络的训练方法之一是根据 Hebb 学习规则设计的，该规则模拟了自然神经系统中神经元之间连接强度变化的规律。

== RNN 的局限性

尽管 RNN 在处理序列数据方面表现出色，但它也存在一些局限性。例如，RNN 在处理长序列时容易出现“梯度消失”或“梯度爆炸”问题，这使得网络难以捕捉到较远时间步之间的依赖关系。为了解决这一问题，研究者提出了改进的 RNN 模型，如长短时记忆网络（LSTM）和门控循环单元（GRU）。

== RNN 的应用

RNN 在多个领域都有广泛应用。例如：
- 自然语言处理：RNN 可以用于文本生成、机器翻译、情感分析等任务。
- 语音识别：RNN 能够处理语音信号中的时序信息，用于语音识别和语音合成。
- 时间序列预测：RNN 可以用于股票价格预测、天气预测等时间序列预测任务。
- 图像描述生成：RNN 可以与卷积神经网络（CNN）结合，用于图像描述生成。例如，CNN 可以对图像进行多层特征提取，产生一个抽象的输出向量，这个向量可以作为 RNN 的初始状态，同时用一个局部符号输入给 RNN 以启动一个句子，按照上面的迭代过程即可产生出描述图像的词组或句子片段。

== RNN 的改进模型

为了克服 RNN 的局限性，研究者提出了多种改进模型。其中， *长短时记忆网络（LSTM）* 是最著名的改进模型之一。LSTM 引入了记忆单元（memory cell）和门控机制（input gate, forget gate, output gate），从而能够更好地控制信息的流动，避免梯度消失问题。

在 LSTM 中，有两个随时间传递的状态向量：一个是隐状态$h_t$，类似于 RNN 中的神经元状态；另一个是记忆状态$C_t$。LSTM 将前一时刻的隐状态$h_(t - 1)$和当前时刻的输入$x_t$进行串联，形成 LSTM 当前的集成信号$s_t = h_(t - 1) | x_t$ 。LSTM 在处理序列数据时的记忆和遗忘机制. 

= 模式识别系统的评价方法

== 监督模式识别的评价方法

/ 训练错误率（Training Error Rate）: 在训练集上对样本进行分类时，分类错误的样本占总样本数的比例。它是最简单的错误率估计方法，但存在明显的缺陷：由于训练集已经包含在分类器的训练过程中，因此训练错误率不能反映分类器在新数据上的推广能力。例如，如果训练集中的样本类别分布不均，分类器可能会记住某些样本的类别，从而导致训练错误率为0，但实际在新数据上可能表现不佳。

/ 测试错误率（Test Error Rate）: 在独立的测试集上对样本进行分类时，分类错误的样本占总样本数的比例。测试集与训练集相互独立，因此测试错误率能够更真实地反映分类器在新数据上的表现。然而，测试集的样本数量有限，可能导致估计的方差较大
。为了提高估计的准确性，通常需要使用交叉验证等方法。

/ 交叉验证（Cross-Validation）: 一种常用的错误率估计方法，尤其适用于样本数量较少的情况。其基本思想是将训练集划分为多个子集，轮流使用其中一部分作为测试集，其余部分作为训练集，多次重复后取平均值作为最终的错误率估计。常见的交叉验证方法包括：

  / k折交叉验证（k-Fold Cross-Validation） : 将训练集划分为k个等份，每次使用k-1个子集作为训练集，1个子集作为测试集，重复k次后取平均。

  / 留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV） : 每次只用一个样本作为测试样本，其余样本作为训练集，重复N次后取平均。留一法的估计偏差较小，但方差较大。

  / n倍交叉验证: 在k轮n倍交叉验证中，每次使用n个子集作为测试集，其余n-1个子集作为训练集，重复k次后取平均。

/ 自举法与0.632估计（Bootstrap and 0.632 Estimate）: 自举法是一种通过有放回地从原始样本集中抽取样本，生成新样本集的方法。每个新样本集用于训练分类器，并预测未被抽到的样本，从而估计错误率。由于自举法中存在重复样本，估计的错误率通常会偏保守。为了结合训练错误率和自举错误率，提出了0.632估计，该估计是这两种估计的加权平均值。0.632估计在样本数量较少时表现较好，能够减少估计的偏差。

/ 置信区间估计: 在实际应用中，如果样本数量充足，可以采用置信区间的方法来估计错误率的置信区间。置信区间反映了错误率的不确定性，随着样本数量的增加，置信区间会逐渐缩小。此外，还可以通过扰动权重等方法来估计错误率的置信区间。

== 非监督模式识别的评价方法

/ 紧致性（Compactness）: 聚类内部样本的紧密程度。通常，紧致性可以通过类内方差或平方误差和来衡量。类内方差越小，说明聚类越紧密。例如，可以使用以下公式计算紧致性：$ V(C) = sqrt(1 / N sum_(C_k in C) sum_(c_1 in C_k) delta(c_1, mu_c)) $
 
其中，$mu_c$是类$C_k$的均值，$delta$是样本与均值之间的距离。

/ 连接性质（Connectivity）: 聚类之间的连贯性。通常，连接性质可以通过类间距离或样本之间的相似性来衡量。例如，可以使用类间距离或两类最近样本之间的距离来计算两类间的距离。连接性质越大，说明聚类越连贯。

/ 分离度（Separation）: 聚类之间的分离程度。通常，分离度可以通过类间距离或样本之间的相似性来衡量。例如，可以使用两类中心间的距离或两类最近样本之间的距离来计算两类间的距离。分离度越大，说明聚类越明显。