#import "../template/conf.typ": conf
#import "../template/components.typ": *

#show: conf.with(
  title: [
    模式识别与机器学习
  ],
  authors: (
    (
      name: [Gang.],
      affiliation: [Hangzhou Dianzi University],
      email: "jerry050512@outlook.com",
    ),
  )
)

#show "!待确认": align(
  center, 
  rect(
    text(
      "待确认", 
      size: 15pt, 
      stroke: red,
      weight: "thin"
    )
  )
)

= 考试重点

== 题型

- 名词解释: 10 (30)
- 问答题: 7 (50) (最后一题计算)
- 分析论述: 1 (20)

考试时间地点: 6.19 18:00-20:00 6教北102

== 基本概念

#align(
  center, 
  grid(
    align: left, 
    columns: (1fr, 1fr), 
    [
      - 非监督模式识别
      - 参数估计 (概率密度函数)
      - 最小错误率贝叶斯决策
      - 分类器
      - 支持向量机
      - 核函数
      - 特征选择
    ], 
    [
      - 流形学习
      - k-means 算法
      - 深度学习
      - 激活函数
      - 梯度消失与梯度爆炸
      - 交叉验证
    ]
  )
)

==  算法原理

#align(
  center, 
  grid(
    align: left, 
    columns: (1fr, 1fr), 
    [
      - 模式识别系统典型构成
      - 贝叶斯估计
        - 算法原理及步骤
        - 朴素贝叶斯估计的计算应用 (计算题)
      - 特征提取/降维
        - 主成分分析
        - 多维尺度法
        - 上述两种线性降维方法的区别
      - 类别可分性判据
        - 在刻画特征对分类的贡献时需要希望满足那些条件
        - 常见的四种类别可分性判据
    ], 
    [
      - 非监督模式识别/聚类算法
        - k-means, 模糊C均值, ISODATA, DBSCAN, 密度峰值聚类等多种聚类算法的重点及区别
      - 循环神经网络
      - 模式识别系统的评价方法
        - 监督模式识别: 训练错误率, 测试错误率, 交叉验证, 自举法与.632法
        - 非监督模式识别: 紧致性, 连接性质与分离度. 
      - 模式识别系统知识的综合运用
        - 根据实际问题灵活运用所学模式识别系统相关知识
    ]
  )
)

#pagebreak()

= 快速复习指南

== 考试重点回顾
本部分旨在帮助您回顾考试的核心概念和算法原理. 

=== 基本概念

/ 非监督模式识别: 理解其定义、目的以及与监督模式识别的区别（是否有类别标签、学习目标、评估方法等）. 熟悉基于模型的聚类、基于距离的聚类（K-均值、ISODATA）、模糊聚类（FCM）、自组织映射（SOM）和基于密度的聚类（DBSCAN）等方法. 
/ 最小错误率贝叶斯决策: 理解其目标（最小化分类错误概率）和基本原理. 掌握贝叶斯公式以及似然比和对数似然比在二分类问题中的应用. 
/ 参数估计 (概率密度函数): 区分参数估计和非参数估计的根本区别（是否假设概率密度函数形式）. 
/ 参数估计: 掌握最大似然估计（MLE）和贝叶斯估计的基本思想、计算步骤及优缺点. 理解无偏性、有效性和一致性作为估计好坏的判断标准. 
/ 非参数估计: 了解直方图法、Parzen 窗法和 k-近邻估计的基本原理. 
/ 分类器: 理解分类器的定义. 掌握分类器的分类（线性、非线性、集成学习）、设计方法（一对多、逐对、直接多类分类器）、训练与评估（特征提取与选择、训练、决策）以及性能评估（交叉验证、置信区间估计、错误率估计）. 
/ 支持向量机 (SVM): 理解其核心思想（最大化分类间隔）, 掌握基本原理（寻找最优分类超平面、松弛变量、非线性映射）和核函数的作用. 了解 SVM 的优化问题及其优缺点. 
/ 核函数: 理解核函数的定义和 Mercer 条件. 熟悉常用的核函数类型（多项式核、径向基（RBF）核、Sigmoid 核）及其特点. 
/ 特征选择: 理解特征选择的目的（降维、提高性能和效率、避免“维度灾难”）. 掌握特征选择的分类（过滤法、包裹法、嵌入法）及其优缺点. 
/ 流形学习: 理解流形学习的核心思想（高维数据中的低维内在结构）和目标. 了解 t-SNE 作为一种流形学习降维方法的作用. 
/ k-means 算法: 掌握 K-means 算法的核心思想、基本步骤及其优缺点. 
/ 深度学习: 理解深度学习的定义（多层结构神经网络模型）和核心（通过多层非线性变换自动提取特征）. 了解典型模型（CNN、RNN、DBN、自编码器、GAN）和“表示学习”概念. 
/ 激活函数: 理解激活函数在神经网络中的作用（引入非线性、控制输出范围、影响梯度传播、提升模型性能）. 熟悉常见的激活函数（Sigmoid、tanh、ReLU、Softmax）及其特点和适用场景. 
/ 梯度消失与梯度爆炸: 理解梯度消失和梯度爆炸的定义、原因和影响. 掌握解决方法（使用不同激活函数、引入 LSTM、批量归一化、调整网络结构）. 
/ 交叉验证: 理解交叉验证的基本思想（多次评估模型泛化能力）. 掌握典型做法（n 倍交叉验证、k 轮 n 倍交叉验证）和特殊形式（留一法交叉验证）. 

=== 算法原理

/ 模式识别系统典型构成:  掌握模式识别系统的五个主要部分: 信息获取、预处理、特征提取、分类器设计、分类决策, 并理解监督和非监督模式识别系统的构成区别. 
/ 贝叶斯估计:  回顾贝叶斯估计的原理. 
/ 朴素贝叶斯估计的计算应用:  掌握朴素贝叶斯估计的核心思想（特征独立性假设）、贝叶斯公式应用、计算步骤以及平滑矫正（Laplace 平滑）处理. 
/ 特征提取/降维: 主成分分析 (PCA):  掌握 PCA 的核心思想（线性降维、保留主要变化信息）、步骤、优缺点和适用场景. 
/ 多维尺度法 (MDS):  掌握 MDS 的目标（保留距离关系、可视化）、步骤、优缺点和适用场景. 
/ 两种线性降维方法的区别:  能够比较 PCA 和 MDS 在目标、方法、监督性、适用场景、计算复杂度和对噪声敏感性等方面的差异. 
/ 类别可分性判据: 在刻画特征对分类的贡献时需要满足的条件:  理解判据应满足与错误率的单调关系、可加性、度量特性和单调性. 
/ 常见的四种类别可分性判据:  了解基于类内类间距离、基于概率分布、基于熵和基于统计检验的可分性判据. 
/ 非监督模式识别/聚类算法: k-means 算法:  掌握其原理、优缺点和适用性. 
/ 模糊 C 均值（FCM）算法:  了解其与 k-means 的区别（模糊隶属度）、核心思想和优缺点. 
/ ISODATA 算法:  了解其作为 k-means 改进版的特点（动态调整聚类数目和中心）和优缺点. 
/ DBSCAN 算法:  理解其基于密度的聚类原理（邻域半径、最小样本数）、优缺点和适用性（处理噪声、发现任意形状簇）. 
/ 密度峰值聚类（DPC）算法:  了解其核心思想（局部密度和距离）、优缺点. 
/ 循环神经网络 (RNN):  掌握 RNN 的基本结构（循环连接、隐藏状态）、输出形式（逐条输出、最终状态分类、中间输出组合、静态输入生成时间序列输出）、局限性（梯度消失/爆炸）和应用. 了解 Hopfield 网络作为 RNN 前身的概念. 熟悉 LSTM 等改进模型如何解决 RNN 的局限性. 
/ 模式识别系统的评价方法: 监督模式识别的评价方法:  理解训练错误率、测试错误率、交叉验证（k 折、留一法、n 倍）和自举法与 0.632 估计的原理和适用性. 掌握置信区间估计的概念. 
/ 非监督模式识别的评价方法:  理解紧致性、连接性质和分离度这三个指标在评估聚类结果中的作用和计算方式. 

== 小测验

请用2-3句话简要回答以下问题. 
+ 非监督模式识别与监督模式识别最主要的区别是什么？
+ 在最小错误率贝叶斯决策中, 似然比 $l(x)$ 反映了什么？决策规则通常如何基于此进行判断？
+ 参数估计和非参数估计在估计概率密度函数时, 对函数形式的假设有何不同？
+ 最大似然估计（MLE）和贝叶斯估计在对待待估计参数的本质上有何不同？
+ 支持向量机（SVM）引入核函数的主要目的是什么？
+ 简述特征选择中“过滤法”与“包裹法”的主要区别. 
+ 循环神经网络（RNN）的基本结构如何使其能够处理序列数据？
+ 激活函数在神经网络中扮演了什么角色？举例说明其一个主要作用. 
+ 梯度消失问题会对深度学习模型的训练产生什么影响？
+ 在评估监督模式识别系统时, 为什么测试错误率比训练错误率更能反映模型的泛化能力？

== 小测验答案

+ 非监督模式识别在训练时没有已知类别标签, 其学习目标是发现数据的内在结构并进行聚类；而监督模式识别则拥有已知类别标签, 学习目标是根据这些标签进行分类. 这导致两者在模型设计和评估方法上也有显著差异. 
+ 似然比 $l(x)$ 反映了在不同类别中观察到特定特征值 $x$ 的相对可能性. 在二分类问题中, 决策规则通常是比较 $l(x)$ 与一个阈值 $lambda$（由先验概率决定）, 若 $l(x) >= lambda$ 则决策为第一类, 否则为第二类. 
+ 参数估计假设概率密度函数的具体形式是已知的, 只是其中的参数未知, 需要通过数据来估计这些参数. 而非参数估计则不假设概率密度函数的具体形式, 而是直接利用训练数据来估计其分布. 
+ 最大似然估计将待估计的参数视为固定的未知量, 旨在找到使观测数据出现概率最大的参数值. 而贝叶斯估计则将待估计的参数视为随机变量, 并通过结合先验分布和似然函数来估计参数的后验分布. 
+ 支持向量机引入核函数的主要目的是为了处理非线性可分的问题. 核函数能够将原始数据映射到一个高维特征空间, 使得数据在该高维空间中变得线性可分, 从而允许 SVM 在不显式计算高维特征的情况下进行非线性分类. 
+ 过滤法是一种独立于分类器的特征选择方法, 它通过统计指标评估特征与类别之间的相关性或可分性. 而包裹法则将特征选择与分类器的性能直接结合, 通过迭代地选择或剔除特征来优化分类器的性能, 计算成本较高但能更好地利用分类器. 
+ RNN 通过循环连接来处理时间序列数据. 每个时刻的输入不仅依赖于当前的输入, 还依赖于上一时刻的隐藏状态, 这种结构使得 RNN 能够“记住”之前的信息, 并在后续的处理中加以利用, 从而捕捉时间序列中的依赖关系. 
+ 激活函数是神经网络中引入非线性关系的关键组成部分. 例如, 它使得神经网络能够学习和表示复杂的非线性关系, 如果没有激活函数, 无论神经网络有多少层, 其整体输出都只能是输入的线性组合. 
+ 梯度消失问题会导致误差梯度在反向传播过程中随着网络层数的增加而逐渐减小, 最终趋近于零. 这使得网络中较深的层无法接收到有效的梯度信息, 从而无法进行有效的参数更新, 导致模型难以学习到复杂的特征, 性能下降. 
+ 测试错误率是在独立的、未参与训练的数据集上计算得到的, 因此它能更真实地反映分类器在面对新数据时的实际表现和泛化能力. 而训练错误率由于模型在训练集上进行了参数调整, 可能存在过拟合现象, 不能准确反映模型对未知数据的性能. 

== 问答题 (Essay Questions)

请选择并回答以下任五道题目. 

+ 详细阐述最大似然估计（MLE）和贝叶斯估计在参数估计中的基本原理、计算步骤以及它们各自的优势和局限性. 
+ 比较并对比监督模式识别和非监督模式识别的主要特点, 包括它们的学习目标、数据要求、典型算法以及评估方法的差异. 
+ 深入探讨支持向量机（SVM）中核函数的作用及其重要性. 请列举并简要说明至少三种常用的核函数, 并解释核函数如何解决非线性分类问题. 
+ 分析梯度消失和梯度爆炸问题在深度学习训练中产生的原因、对模型性能的影响, 并详细阐述至少三种解决这些问题的方法. 
+ 详述模式识别系统的典型构成, 并解释每个组成部分的功能和在整个系统中的作用. 
+ 比较主成分分析（PCA）和多维尺度法（MDS）这两种线性降维方法的异同, 包括它们各自的目标、实现方法、适用场景以及优缺点. 
+ 解释交叉验证的基本思想和目的. 详细描述 n 倍交叉验证法的步骤, 并探讨留一法交叉验证作为其特殊形式的特点和应用场景. 
+ 探讨在刻画特征对分类的贡献时, 类别可分性判据需要满足的条件. 举例说明至少两种常见的类别可分性判据, 并简要解释其衡量标准. 
+ 详细阐述循环神经网络（RNN）的基本结构及其处理序列数据的机制. 讨论 RNN 在处理长序列数据时存在的局限性, 并介绍长短时记忆网络（LSTM）如何克服这些局限. 
+ 选择并详细阐述两种非监督模式识别/聚类算法（例如: K-means, FCM, ISODATA, DBSCAN, DPC）, 比较它们的核心思想、优点和缺点, 并说明它们各自适用于哪些类型的数据分布. 

== 关键术语词汇表 (Glossary of Key Terms)

/ 模式识别 (Pattern Recognition): 机器学习的一个分支, 旨在识别数据中的模式和规律, 并将其归类到预定义的类别或发现其内在结构. 
/ 机器学习 (Machine Learning): 一种人工智能的方法, 使计算机系统能够从数据中学习, 而无需进行显式编程. 
/ 非监督模式识别 (Unsupervised Pattern Recognition): 在没有已知类别标签的情况下, 通过分析数据的内在结构或相似性, 将数据划分为若干类别或聚类的机器学习方法. 
/ 最小错误率贝叶斯决策 (Minimum Error Rate Bayesian Decision): 一种决策规则, 目标是使分类错误的概率最小, 通常基于贝叶斯公式计算后验概率进行决策. 
/ 参数估计 (Parametric Estimation): 假设概率密度函数的形式已知, 通过训练数据估计表征函数的未知参数. 
/ 非参数估计 (Nonparametric Estimation): 不假设概率密度函数的形式, 直接利用训练数据对概率密度进行估计的方法. 
/ 最大似然估计 (Maximum Likelihood Estimation, MLE): 一种参数估计方法, 旨在找到使观测数据出现概率最大的参数值. 
/ 贝叶斯估计 (Bayesian Estimation): 一种参数估计方法, 将参数视为随机变量, 结合先验分布和似然函数估计后验分布. 
/ 分类器 (Classifier): 一种函数或模型, 能够将输入数据映射到特定的类别. 
/ 线性分类器 (Linear Classifier): 通过线性函数实现分类的分类器, 例如线性判别函数. 
/ 非线性分类器 (Nonlinear Classifier): 通过非线性函数实现分类的分类器, 能够处理更复杂的分类问题. 
/ 支持向量机 (Support Vector Machine, SVM): 一种用于分类和回归的机器学习算法, 通过最大化分类间隔来提高模型的推广能力, 并可通过核函数处理非线性问题. 
/ 核函数 (Kernel Function): 一个对称函数, 满足 Mercer 条件, 用于将原始数据映射到高维特征空间, 使得在高维空间中数据变得线性可分. 
/ 特征选择 (Feature Selection): 从一组给定的特征中选择出最有利于分类或聚类的特征子集, 以降低特征空间的维数, 提高模型性能和效率. 
/ 过滤法 (Filter Methods): 一种独立于分类器的特征选择方法, 通过统计指标评估特征与类别之间的相关性或可分性. 
/ 包裹法 (Wrapper Methods): 一种将特征选择与分类器性能直接结合的方法, 通过迭代地选择或剔除特征来优化分类器性能. 
/ 嵌入法 (Embedded Methods): 一种在分类器设计过程中直接集成特征选择目标的方法, 例如通过正则化实现. 
/ 流形学习 (Manifold Learning): 一种非监督学习方法, 假设高维数据中存在低维的内在结构（流形）, 旨在提取有意义的信息. 
/ k-means 算法 (K-means Algorithm): 一种经典的聚类算法, 通过迭代寻找聚类划分方案, 使得用聚类均值代表样本时总体误差最小. 
/ 深度学习 (Deep Learning): 一种具有多层结构的神经网络模型, 通过多层非线性变换自动提取数据特征. 
/ 表示学习 (Representation Learning): 深度学习的一个重要特点, 指通过神经网络自动学习数据的特征表示, 而非依赖人工设计. 
/ 激活函数 (Activation Function): 神经网络中用于引入非线性关系的关键组成部分, 决定神经元输出是否被激活以及如何响应输入变化. 
/ 梯度消失 (Vanishing Gradient): 在反向传播过程中, 误差梯度随着网络层数的增加而逐渐减小, 最终趋近于零的现象. 
/ 梯度爆炸 (Exploding Gradient): 在反向传播过程中, 误差梯度随着网络层数的增加而逐渐增大, 最终变得非常大的现象. 
/ 交叉验证 (Cross-Validation, CV): 一种评估分类器或模型泛化能力的方法, 通过将数据集划分为多个子集, 轮流作为训练集和测试集进行评估. 
/ k 折交叉验证 (k-Fold Cross-Validation): 将数据集划分为 k 个等份, 每次用 k-1 份训练, 1 份测试, 重复 k 次取平均. 
/ 留一法交叉验证 (Leave-One-Out Cross-Validation, LOOCV): 交叉验证的一种特殊形式, 每次只用一个样本作为测试样本, 其余作为训练集. 
/ 主成分分析 (Principal Component Analysis, PCA): 一种经典的线性降维方法, 通过线性变换将高维数据映射到低维空间, 同时保留数据的主要变化信息. 
/ 多维尺度法 (Multidimensional Scaling, MDS): 一种用于数据可视化的方法, 将高维数据映射到低维空间, 同时尽可能保留数据点之间的距离关系. 
/ 类别可分性判据 (Class Separability Criteria): 用于衡量特征对分类任务贡献的指标. 
/ 模糊 C 均值 (Fuzzy C-means, FCM): k-means 的一种扩展, 允许样本同时属于多个簇, 并赋予每个样本到各个簇的隶属度. 
/ ISODATA 算法 (Iterative Self-Organizing Data Analysis Technique): 一种改进的 k-means 算法, 通过动态调整聚类数目和中心以适应数据分布变化. 
/ DBSCAN 算法 (Density-Based Spatial Clustering of Applications with Noise): 一种基于密度的聚类算法, 通过识别高密度区域来定义簇, 能够有效处理噪声和任意形状的聚类. 
/ 密度峰值聚类 (Density Peak Clustering, DPC): 一种基于密度的聚类算法, 通过计算每个样本的局部密度和距离最近的高密度区域的距离来定义簇. 
/ 循环神经网络 (Recurrent Neural Network, RNN): 一种专门用于处理序列数据的神经网络模型, 能够捕捉时间序列中的依赖关系. 
/ Hopfield 网络 (Hopfield Network): 一种反馈网络, RNN 的前身, 用于实现联想记忆功能. 
/ 长短时记忆网络 (Long Short-Term Memory, LSTM): RNN 的改进模型, 引入记忆单元和门控机制, 有效控制信息流动, 解决梯度消失问题. 
/ 训练错误率 (Training Error Rate): 在训练集上分类错误的样本占总样本数的比例. 
/ 测试错误率 (Test Error Rate): 在独立的测试集上分类错误的样本占总样本数的比例. 
/ 自举法 (Bootstrap): 一种通过有放回地从原始样本集中抽取样本, 生成新样本集的方法, 用于估计错误率. 
/ 0.632 估计 (0.632 Estimate): 结合训练错误率和自举错误率的加权平均值, 用于减少错误率估计的偏差. 
/ 置信区间估计 (Confidence Interval Estimation): 通过计算分类器错误率的置信区间来评估分类器性能的方法, 反映错误率的不确定性. 
/ 紧致性 (Compactness): 评估非监督模式识别（聚类）的指标, 指聚类内部样本的紧密程度. 
/ 连接性质 (Connectivity): 评估非监督模式识别（聚类）的指标, 指聚类之间的连贯性. 
/ 分离度 (Separation): 评估非监督模式识别（聚类）的指标, 指聚类之间的分离程度. 

#pagebreak()

= 基本概念

== 非监督模式识别

/ 非监督模式识别 (Unsupervised Pattern Recognition): 在没有已知类别标签的情况下, 通过分析数据的内在结构或相似性, 将数据划分为若干类别或聚类的机器学习方法

大致可分为以下两类: 
- 基于概率密度函数估计的方法 
- 基于样本间相似性度量的方法

#align(
  center, 
  grid(
    columns: (1fr, 1fr, 1fr), 
    inset: 5pt, 
    align: horizon,
    card(
      title: [基于模型的聚类]
    )[基于模型的聚类方法假设样本在特征空间中的概率分布, 并寻找样本分布密度的单峰区域进行聚类. ], 
    card(
      title: [基于距离的聚类]
    )[
      基于距离的聚类方法直接利用样本之间的距离或相似性来划分聚类. 例如, K-均值（K-means）算法通过计算样本到聚类中心的距离, 将样本分配到最近的聚类中. ISODATA 方法则是一种动态聚类算法, 可以根据数据密度自动调整聚类的数量. 
    ], 
    card(
      title: [模糊聚类]
    )[
      模糊聚类方法允许样本同时属于多个聚类, 每个样本对各个聚类的隶属度由一个模糊度表示. 模糊C-均值（FCM）算法是模糊聚类的典型代表, 它通过优化目标函数来调整样本的隶属度. 
    ],
    card(
      title: [自组织映射（SOM）]
    )[
      自组织映射是一种无监督学习方法, 通过神经网络的自组织特性, 将高维数据映射到低维网格上, 从而发现数据的内在结构. SOM 网络通过竞争学习机制, 使网络中的神经元逐渐适应输入数据的分布. 
    ], 
    card(
      title: [基于密度的聚类]
    )[
      基于密度的聚类方法（如DBSCAN）通过计算样本的局部密度来划分聚类, 能够有效识别噪声点和任意形状的聚类. 
    ]
  )
)

#table(
  columns: (auto, 1fr, 1fr), 
  fill: (x, y) => if y == 0 {luma(80%)}, 
  align: center+horizon, 
  [*特征*], [*监督模式识别*], [*非监督模式识别*], 
  [是否有类别标签], [是], [否], 
  [学习目标], [分类], [聚类], 
  [评估方法], [准确率、错误率等], [聚类质量指标、稳定性评估], 
  [数据要求], [需要已知类别样本], [无需已知类别标签], 
  [模型设计], [分类器（如SVM、神经网络）], [聚类算法（如K-均值、DBSCAN）], 
  [数据分布假设], [有明确假设], [更加灵活], 
)

== 最小错误率贝叶斯决策

/ 先验概率: 根据以往经验和分析得到的概率
/ 贝叶斯公式: 
#emph_box[$ P(omega_i | x) = (p(x | x_i)P(omega_i)) / p(x) $]

最小错误率贝叶斯决策的目标是*使分类错误的概率最小*. 具体来说, 它要求在所有可能的决策中, 选择使错误率最小的决策规则. 错误率的定义为: 
#emph_box[$ P(c) = integral P(c | x) p(x) d x $]
其中, $P(c | x)$是在特征$x$下属于类别$c$的后验概率, $p(x)$是特征$x$的先验概率密度函数. 通过最小化$P(c)$, 可以得到最小错误率贝叶斯决策的最优分类规则. 

/ 最小错误率贝叶斯决策: 对于二分类问题 
#emph_box[$ "If" P(omega_1 | x) >= P(omega_2 | x), "Then" x in omega_1 $]

其中, 后验概率用*贝叶斯公式*求得
$ P(omega_i | x) = (p(x | x_i)P(omega_i)) / p(x) = (p(x | omega_1) P(omega_2)) / (p(x | omega_1) P(omega_1) + p(x | omega_2) P(omega_2)) $

由于先验概率$P(omega_i)$是确定的, 人们经常把决策规则整理成如下形式: 
#emph_box[$ "If" l(x) = P(x | omega_1) / P(x | omega_2) >= lambda = P(omega_2) / P(omega_1), "Then" x in cases(omega_1, omega_2) $]
对于每一个样本$x$, 计算其似然比$l(x)$, 并与阈值$lambda$比较, 若$l(x) >= lambda$, 则决策为第一类, 否则决策为第二类. 

/ 似然比: 
概率比$l(x)$反映了在$omega_1$类中观察到特征值$x$的相对可能性, 也称为*似然度*, 而$l(x)$本身被称为*似然比（likelihood ratio）*  . 

为了简化计算, 人们通常使用*对数似然比*$h(x) = - ln [l(x)] = ln P(x | omega_1) + ln P(x | omega_2)$ . 此时, 决策规则可以表示为: 
#emph_box[$ 1 / 2 h(x) >= ln P(omega_1) / P(omega_2) $]
若满足该条件, 则$x$被决策为第一类, 否则为第二类. 

== 参数估计 (概率密度函数)

/ 参数估计（probability density function estimation）: 概率密度函数的形式已知, 而表征函数的参数未知, 需要通过训练数据来估计. 

目标是根据已知的训练样本, 估计出描述这些样本分布的概率密度函数$p(x)$
- 矩估计、*最大似然估计、Bayes估计*

/ 非参数估计: 概率密度函数的形式未知, 也不作假设, 利用训练数据直接对概率密度进行估计. 
- Parzen窗法和$k_n$-近邻法

=== 参数估计（Parametric Estimation）

参数估计假设概率密度函数的形式是已知的, 但其中的部分或全部参数未知. 
两种形式为: *点估计与区间估计*. 

估计好坏的判断标准: *无偏性* (估计量等于期望), *有效性* (方差小), *一致性* (每次估计量都解决真实值). 

/ 区间估计: 
$ P_theta (hat(theta)_L <= theta <= hat(theta)_U) >= 1 - alpha $
置信水平为$1-alpha$的置信区间

1. *最大似然估计（Maximum Likelihood Estimation, MLE）*

其基本思想是: 

假设有若干样本, 分为不同类别, 在同一类内部的若干数据满足同样的概率密度函数. 例如, 有5+5两类共10个数据, 前5个满足正态分布, 后5个满足伯努利分布. 

基于这种假设, 对同一类内部的数据建立联合概率密度函数$L(theta)$. 

$ L(theta) = product_(i = 1)^N p(x_i | theta) $

在$L(theta)$中只有$theta$是未知量, 解方程$(d L(theta))/(d theta)=0$可以得到结果. 

*正态分布*下最大似然参数估计公式为: 
- 均值: $hat(mu) = 1 / N sum_(i = 1)^N x_i$
- 方差: $hat(sigma)^2 = 1 / N sum_(i = 1)^N (x_i - hat(mu))^2$

2. *贝叶斯估计（Bayesian Estimation）*

贝叶斯估计与最大似然估计不同, 它将参数视为随机变量, 并结合先验分布$p(theta)$和似然函数$p(x | theta)$, 通过贝叶斯定理估计后验分布 $p(theta | x)$. 

计算步骤如下: 

(1).根据对问题的认识或猜测确定$p(theta)$

(2).求出样本集的联合分布

$ p(X|theta)=product_(i=1)^N p(x_i|theta) $

(3).求出$theta$的后验概率

$ p(theta|X) = (p(X|theta)p(theta))/(integral_Theta p(X|theta)p(theta)d theta) $

(4).根据上式, $theta$估计量: 

$ theta^(star) =integral_(Theta) theta p(theta|X)d theta $

考虑到我们的最终目的不是求$theta$, 而是做分类, 即求样本出现在样本集的概率$p(x|X)$(贝叶斯决策), 可以直接通过以下表达式计算. 

$ p(x|X) = integral_Theta p(x|theta)p(theta|X)d theta $

话题回归对参数的讨论. 引入样本数量$N$, 即样本集$X → X^N$. 可以得到

$ p(theta|X^N) = (p(x_N|theta)p(theta|X^(N-1)))/(integral p(x_N|theta)p(theta|X^(N-1))d theta) $

随着$N$的增加, $p(theta|X^N)$会收敛于在参数真实值上的一个脉冲函数, 这样的过程称为贝叶斯学习. 此外样本概率密度函数也可以类似的逼近真实的密度函数
$ p(x|X^(N→ infinity)) = p(x) $

综合两种参数估计方法, 参数估计都是得到$theta$的值, 最终目的可以是计算$p(x|X)$用以分类(贝叶斯决策). 

*两种估计的主要区别*

- 最大似然:  待估计的参数当作*固定的未知量*
- 贝叶斯: 把待估计的参数看作*随机变量*, 根据观测数据对参数的分布进行估计（还须考虑参数的先验分布）


=== 非参数估计（Nonparametric Estimation）

非参数估计不假设概率密度函数的具体形式, 而是直接根据训练样本估计其分布. 

#grid(
  columns: (1fr, 1fr), 
  inset: 5pt, 
  card(title: [直方图法 \ （Histogram Method）])[直方图法是最简单的非参数估计方法. 它将样本空间划分为若干个小格（bin）, 统计每个小格内的样本数, 然后将每个小格的样本数除以样本总数和小格体积, 得到概率密度估计. ], 
  grid.cell(
    rowspan: 2, 
    card(title: [Parzen 窗法 \ （Parzen Window Method）])[Parzen 窗法是一种基于核函数的非参数估计方法. 它通过在每个样本点周围定义一个核函数（如高斯核）, 并计算所有样本点对该点的贡献, 从而估计概率密度. 其公式为: $ hat(p) (x) = 1 /N sum_(i = 1)^N K((x - x_i) / h) $其中, $K$是核函数, $h$是平滑参数（窗口宽度）, $N$ 是样本总数. Parzen 窗法能够提供平滑的概率密度估计, 并且在样本数足够大时, 估计结果会收敛到真实密度函数. ]
  ),
  card(title: [k-近邻估计 \ （k-Nearest Neighbor Estimation）])[k-近邻估计是一种基于局部密度的非参数估计方法. 它通过计算每个样本点的k个最近邻的平均距离, 来估计该点的概率密度. 这种方法在处理高密度区域和低密度区域时表现良好. ], 
)

=== 参数估计与非参数估计比较

#table(
  columns: (auto, 1fr, 1fr), 
  fill: (x, y) => if y == 0 { luma(80%) }, 
  align: center+horizon, 
  [*特征*], [*参数估计*], [*非参数估计*], 
  [假设], [概率密度函数形式已知], [不假设概率密度函数形式], 
  [估计目标], [估计参数, 如$mu, sigma^2$], [估计概率密度函数$p(x)$], 
  [优点], [估计结果更精确, 适合已知分布形式], [无需假设分布形式, 适用于未知分布], 
  [缺点], [需要先验知识, 对分布形式敏感], [对样本数量要求较高, 计算量较大], 
  [常用方法], [最大似然估计、贝叶斯估计], [直方图法、k-近邻估计、Parzen 窗法], 
  [应用场景], [已知分布形式], [未知分布形式]
)



== 分类器

/ 分类器: 一种函数或模型, 它能够将输入数据映射到特定的类别

=== 分类器的分类

+ *线性分类器*: 这类分类器通过线性函数来实现分类, 例如线性判别函数$g(x) = omega^T x + omega_0$, 其中$omega$是权向量, $x$是输入特征向量, $omega_0$是偏置项, 该线性函数称为超平面*$H$*，平面把样本空间分为两类(面上认为两类均可或拒绝). 线性分类器在处理简单数据分布时效果较好，但在处理复杂数据时可能需要通过特征变换或非线性方法来提高性能. 
+ *非线性分类器*: 非线性分类器通过非线性函数来实现分类，例如多层感知器（MLP）和支持向量机（SVM）等. 这些分类器能够处理更复杂的分类问题，但通常需要更多的计算资源和训练时间. 
+ *集成学习分类器*: 集成学习方法通过组合多个弱分类器的输出来提高分类性能. 常见的集成学习方法包括AdaBoost、随机森林和梯度提升树（GBDT）等. 

=== 线性分类器

/ 线性判别函数: 

/ Fisher线性判别分析（linear discriminant analysis, LDA）: 把所有样本都投影到一维空间（直线）, 要找到一个最合适的投影轴, 使两类样本在该轴上投影之间的距离尽可能远, 而每一类样本的投影尽可能紧凑

/ 感知器: 有增广样本特征向量$bold(y)=mat(1;x_1;x_2;dots.v;x_d)$, 然后我们对$bold(y)$做修正: $ bold(y)_i = cases( bold(y)_i in "第一类" arrow.r bold(y)_i ,bold(y)_i in "第二类" arrow.r -bold(y)_i) $所以如果样本可分则$alpha^T bold(y)>0$对增广权向量$alpha$有解. 考虑到计算量, 对$alpha$实际上是最优化技术中的*梯度下降法*去求解. 如果训练模型是*线性可分*的, 感知器训练算法在有限次迭代后, 可以收敛到正确分类的解向量. 

既然是最优化方法, 则有目标函数及约束条件: 显然, 错误的$alpha$会使得$alpha^T bold(y)lt.eq.slant 0$, 所以只要最小化函数$ J_p (alpha)= sum (-alpha^T bold(y)) $

考虑到噪声误差等因素会引入余量$b>0$用$alpha^T bold(y)>b$代替$alpha^T bold(y)>0$

感知器准则要求全部样本是线性可分的, 若样本线性不可分, 用感知器会不收敛. 


/ 最小平方误差判别: 该方法可用于处理线性不可分的样本集.对每个样本,设定一个“理想”的判别函数输出值,以最小平方误差为准则求最优权向量

$ a^* = arg min_a J_s (bold(a)) = arg min_a || Y bold(a) - bold(b) ||^2 $

=== 非线性分类器

/ 分段线性距离分类器: 用多段线性函数来逼近, 用多个线性分类器片段来实现非线性分类

/ 二次判别函数: 
$ g(x) = x^T W x + w^T x + w_0 $

/ 多层感知器神经网络（Multi-layer perception, MLP）: 一种通用的非线性分类器设计方法, 具有从训练数据中学习任意复杂的非线性映射的能力, 也包括实现复杂的非线性分类判别函数


=== 分类器的设计方法

+ *一对多（One-vs-Rest, OvR）* : 这种方法通过为每个类别训练一个分类器, 将多类问题转化为多个二分类问题. 每个分类器负责区分一个特定类别与其他所有类别. 这种方法简单易实现, 但可能需要训练多个分类器. 
+ *逐对（One-vs-One, OvO）* : 这种方法通过为每对类别训练一个分类器, 将多类问题转化为多个二分类问题. 每个分类器负责区分两个特定类别. 这种方法在类别数量较多时可能需要训练较多的分类器, 但每个分类器的决策边界更清晰, 分类结果更可靠. 
+ *直接多类分类器*: 这种方法直接设计多类分类器, 例如多类支持向量机（Multiclass SVM）和多层感知器（MLP）. 这些分类器能够直接处理多类问题, 无需将多类问题分解为多个二分类问题. 这种方法通常能够提供更好的分类性能, 但设计和训练过程较为复杂. 

=== 分类器的训练与评估

+ *特征提取与选择*: 为了提高分类器的性能, 通常需要对输入数据进行特征提取和选择. 特征提取是将原始数据转换为更简洁的特征表示, 而特征选择则是从提取的特征中选择最相关的特征. 特征选择的方法包括*过滤法*和*包裹法*. 
+ *分类器训练*: 分类器训练是根据训练数据调整分类器的参数, 使其能够更好地拟合训练数据. 训练过程通常涉及优化算法, 如梯度下降法、随机梯度下降法（SGD）等. 分类器的训练目标是使分类器在训练数据上的分类误差最小化. 
+ *分类决策*: 分类决策是将训练好的分类器应用于新的未知样本, 进行分类. 分类决策的过程通常涉及计算分类器的输出, 并根据输出结果进行分类. 分类器的输出可以是类别标签, 也可以是类别概率. 

=== 分类器的性能评估


+ *交叉验证*：交叉验证是一种评估分类器性能的方法,通过将数据集划分为多个子集,轮流使用不同的子集作为训练集和验证集,评估分类器的性能.交叉验证可以减少数据划分的随机性,提高评估结果的可靠性.
+ *置信区间估计*：置信区间估计是一种评估分类器性能的方法,通过计算分类器的错误率的置信区间,评估分类器的性能,置信区间估计可以提供分类器性能的统计意义,帮助评估分类器的可靠性.
+ *错误率估计*：错误率估计是评估分类器性能的基本方法,通过计算分类器在测试数据上的错误率,评估分类器的性能,错误率估计可以提供分类器性能的直观评估,但需要足够的测试数据支持,


== 支持向量机

/ 支持向量机: 一种用于分类和回归的机器学习算法, 其核心思想是通过最大化分类间隔来提高模型的推广能力. SVM 通过引入*核函数*, 可以*将原始数据映射到高维空间, 从而将非线性问题转化为线性问题*, 实现更有效的分类. 

=== 基本原理

SVM 的核心在于寻找一个*最优的分类超平面*, 使得两类样本之间的分类间隔最大化. 这个*分类间隔*是指分类面与最近样本之间的距离. 通过最大化这个间隔, SVM 能够在有限样本的情况下获得良好的推广能力. 

具体来说就是*在线性可分时, 在原空间寻找两类样本的最优分类超平面*. *在线性不可分时, 加入松弛变量并通过使用非线性映射将低维度输入空间的样本映射到高维度空间使其变为线性可分*, 这样就可以在该特征空间中寻找最优分类超平面. 


=== 核函数的作用

为了处理非线性可分的问题, SVM 引入了*核函数（Kernel Function）*. 核函数的作用是*将原始数据映射到一个高维特征空间, 使得在高维空间中数据变得线性可分*. 通过核函数, SVM 可以在不显式计算高维特征的情况下, 间接地进行非线性变换. 常用的核函数包括*多项式核、径向基函数（RBF）核和 Sigmoid 核*. 

=== SVM 的优化问题

SVM 的目标是找到一个最优的分类超平面, 使得分类间隔最大化. 这个过程可以通过求解一个二次优化问题来实现. 对于线性可分的情况, SVM 的优化问题可以表示为: 
$ max_alpha sum_(i = 1)^n alpha_i - 1 / 2 sum_(i, j = 1)^n alpha_i alpha_j y_i y_j(x_i dot x_j) $
其中, $alpha_i$是拉格朗日乘子, $y_i$是样本的类别标签, $x_i$是样本的特征向量. 

=== 优缺点

*可解释性强*, 采用核技巧之后, 可以处理*非线性*分类/回归任务, 避免了“*维数灾难*”. 

训练时间长, 当支持向量的数量较大时, 预测计算*复杂度较高*. 只适合*小批量样本*的任务. 

== 核函数

/ 核函数: $K(x, x')$是一个对称函数, 它满足 Mercer 条件, 即对于任意的$phi.alt != 0$且$integral phi.alt^2(x) d x < infinity$, 有: 
$ integral.double K(x, x') phi.alt(x) phi.alt(x') d x d x' $ 
这表明, 只要满足 Mercer 条件, 核函数就可以构建一个非线性的支持向量机. 通过核函数, SVM 可以在不显式计算高维特征的情况下, 间接地在高维空间中进行分类. 

常用的核函数包括: 
+ *多项式核函数*: $K(x, x') = ((x, x') + 1)^q$, 实现的是$q$阶的多项式判别函数. 
+ *径向基（RBF）核函数*: $K(x, x') = exp(- (|| x - x' ||^2) / sigma^2)$, 实现与径向基网络形式相同的决策函数. 
+ *Sigmoid 核函数*: $K(x, x') = tanh(nu(x dot x') + c)$, 在$nu$和$c$满足一定取值条件的情况下等价于包含一个隐层的多层感知器神经网络. 

具体问题具体分析, 很难有一个一般性的选择, 通常手工尝试几种选择就可以找出比较合适的参数. 但一般情况下先尝试*简单的核函数*, 如线性核；当结果不满意时才考虑非线性核 \
如果是RBF核函数, 则应先选择*宽度比较大*的核, 宽度越大越接近线性, 然后再尝试减小宽度, 增加非线性程度. 


== 特征选择

/ 特征选择: 目的是从一组给定的特征中选择出最有利于分类或聚类的*特征子集*, 以降低特征空间的维数, 提高模型的性能和效率. 
特征选择不仅有助于减少计算复杂度, 还能避免“维度灾难”（即特征过多导致模型过拟合或计算资源浪费）. 

/ 特征提取: 用映射(或变换)的方法把原始特征*变换*为较少的新特征, 也称为特征变换、压缩

=== 特征的评价准则

常见类别可分性判据: 基于*距离、概率分布、熵函数、统计检验*

- 基于距离
  - 点与点的距离
  - 点到点集的距离
  - 类内及总体的均值矢量
  - 类内距离
  - 类内离差矩阵
  - 两类之间的距离
  - 各类模式之间的总的均方距离
- 基于概率分布
  - Bhattacharyya 判据
  - Chernoff 判据
  - 散度
- 基于熵
  - Shannon熵
  - Kolmogorov 熵, Topological 熵, Boltzmann熵等
- 基于统计检验
  - t-检验
  - 秩和检验


=== 特征选择的分类

+ *过滤法（Filter Methods）*: 
  过滤法是一种独立于分类器的特征选择方法, 它通过统计指标（如类内类间距离、方差、信息增益等）来评估特征之间的相关性或与类别之间的可分性, 从而选择最优特征. 过滤法的优点是计算效率高, 但缺点是无法考虑特征之间的相互作用, 因此可能无法得到最优的特征组合. 

+ *包裹法（Wrapper Methods）*: 
  包裹法将特征选择与分类器的性能直接结合, 通过迭代地选择或剔除特征来优化分类器的性能. 例如, 递归特征消除（RFE）和递归支持向量机（R-SVM）等方法, 通过评估特征在分类器中的贡献来选择特征. 包裹法能够更好地利用分类器的性能, 但计算成本较高. 

== 流形学习

/ 流形学习（manifold learning）: 一种非监督学习方法, 其核心思想是*假设高维数据中存在低维的内在结构*, 即数据点在高维空间中可能分布在某个低维流形上. 这种低维结构反映了数据的内在几何特性, 而不仅仅是随机噪声或高维空间中的随机分布. 通过识别和建模这种低维结构, 流形学习旨在从高维数据中提取出有意义的信息, 从而实现降维、可视化或分类等任务. 

具体来说, 流形学习的目标是*找到一个低维嵌入空间*, 使得在这个空间中, 数据点之间的距离关系尽可能地*保留原始高维空间中的相似性*. 

流形学习包括*ISOMAP, LLE, LPP*算法等. 

== k-means 算法 (K 均值, C 均值)

!待确认

K-means 算法是一种常用的聚类算法, 其核心思想是通过迭代寻找$c$个聚类的划分方案, 使得用这$c$个聚类的均值来代表相应各聚类样本时所得到的总体误差最小. C 均值算法有时也被称为 K 均值算法, 在向量量化和图像分割等领域有广泛应用. 

K-means 算法的基本步骤如下: 
+ 初始化: 确定初始聚类中心$m_1, m_2, dots, m_c$, 通常使用随机选择方法. 
+ 分配: 将每个样本分配到最近的聚类中心. 
+ 更新: 重新计算每个聚类的均值作为新的聚类中心. 
+ 迭代: 重复分配和更新步骤, 直到聚类中心不再显著变化或达到预设的迭代次数. 
K-means 算法的目标是通过不断调整样本的类别归属来求解最小误差平方和准则下的最优结果. 该算法在处理高维数据时可能会受到噪声和初始值选择的影响, 因此在实际应用中需要谨慎选择初始聚类点和聚类数目$c$. 

== 深度学习

深度学习是一种机器学习方法, 通常指具有多层结构的神经网络模型. 它最早在1986年被提出, 但直到21世纪第二个十年才真正流行起来. 深度学习的核心在于*通过多层非线性变换来自动提取数据的特征*, 从而实现对复杂模式的识别和学习. 这种多层结构使得深度学习能够处理高维数据, 并在图像识别、自然语言处理、语音识别等领域取得了显著的成果. 

深度学习的典型模型包括*卷积神经网络（CNN）、循环神经网络（RNN）、深度信念网络（DBN）、自编码器等*. 其中, 卷积神经网络通过卷积运算和非线性激活函数来提取图像特征, 而循环神经网络则适用于处理序列数据. 深度信念网络（DBN）是第一个非卷积的深度神经网络模型, 它通过逐层构建的贪婪学习算法进行训练, 能够有效解决多层感知器的训练困难问题. 此外, 深度学习还引入了生成对抗网络（GAN）等模型, 用于生成新的数据样本. 

深度学习的一个重要特点是*“表示学习”（representation learning）*, 即*通过神经网络自动学习数据的特征表示, 而不是依赖人工设计的特征*. 这种自动特征提取的能力使得深度学习在许多实际应用中表现出色, 例如图像识别、语音识别和自然语言处理. 然而, 深度学习也面临一些挑战, 如过拟合问题、优化困难和理论基础不足等. 为了解决这些问题, 研究者们提出了多种技巧和方法, 如*随机舍弃（dropout）、归一化、数据增强*等. 

== 激活函数

/ 激活函数: 神经网络中用于引入非线性关系的关键组成部分. 在神经网络中, 每一层的神经元通常会对输入进行加权求和, 然后通过一个函数将结果映射到输出. 这个函数就是激活函数, 它决定了神经元的输出是否被激活, 以及如何响应输入的变化. 

=== 激活函数的作用

+ 引入非线性: 
  激活函数使得神经网络能够学习和表示复杂的非线性关系. 如果没有激活函数, 无论神经网络有多少层, 其整体输出都只能是输入的线性组合, 无法处理非线性问题. 因此, 激活函数是神经网络实现非线性映射的基础. 
+ 控制输出范围: 
  不同的激活函数会对输出值的范围进行限制. 这种限制有助于神经网络在训练过程中保持数值的稳定性. 
+ 影响梯度传播: 
  激活函数的导数决定了梯度在反向传播过程中的传播情况. 如果激活函数的导数在某些区域趋近于 0, 就会导致梯度消失问题, 使得网络难以训练. 
+ 提升模型性能: 
  适当的激活函数可以提升模型的收敛速度和泛化能力. 
+ 影响训练过程: 
  激活函数的选择还会影响训练过程中的梯度下降效果. 此外, 激活函数的使用通常与归一化技术（如批量归一化）结合使用, 以进一步提升训练效率和模型性能. 

=== 常见的激活函数

#table(
  columns: (auto, 2fr, 3fr), 
  fill: (x, y) => if y == 0 {luma(80%)}, 
  align: center+horizon, 
  [*名称*], [*公式*], [*说明*], 
  [Sigmoid], $ sigma(x) = 1 / (1 + e^(-x)) $, [Sigmoid 函数将输出限制在 0 到 1 之间, 常用于二分类任务中的输出层. 然而, 由于其在两侧的导数趋近于 0, 容易导致梯度消失问题. ], 
  [tanh], $ tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x)) $, [tanh 函数将输出限制在 -1 到 1 之间, 与 Sigmoid 函数相比, 其输出均值更接近 0, 有助于梯度下降更接近自然梯度, 从而加快收敛速度. ], 
  [ReLU], $ f(x) = max(0, x) $, [ReLU 函数在正区间导数恒为 1, 可以缓解梯度消失问题, 但其在负区间没有输出, 可能导致“神经元死亡”现象. 为了解决这一问题, 研究者提出了多种改进形式, 如 Leaky ReLU、Parametric ReLU（PReLU）和 Maxout 等. ], 
  [Softmax], $ "Softmax"(x_i) = e^(x_i) / (sum_j e^(x_j)) $, [Softmax 函数通常用于多分类任务的输出层, 它将输出转换为概率分布.  ]
)

== 梯度消失与梯度爆炸

梯度消失与梯度爆炸是深度学习中常见的训练问题, 主要发生在神经网络的反向传播过程中. 它们指的是在训练过程中, 误差梯度在反向传播时出现异常的放大或缩小现象, 从而影响模型的训练效果. 

=== 梯度消失（Vanishing Gradient）

/ 梯度消失: 在反向传播过程中, 误差梯度随着网络层数的增加而逐渐减小, 最终趋近于零. 这种现象会导致网络中较深的层无法接收到有效的梯度信息, 从而无法进行有效的参数更新, 使得网络难以训练出好的结果. 

原因: 在传统的神经网络中, 尤其是使用Sigmoid等激活函数时, 其导数在输入值接近0时非常小, 导致梯度在反向传播过程中逐渐衰减. 当网络层数较多时, 梯度会越来越小, 最终趋于零, 使得深层网络的参数无法被有效更新. 

影响: 梯度消失会使得网络难以学习到复杂的特征, 尤其是在深层网络中, 深层的参数无法被有效更新, 导致模型性能下降. 

=== 梯度爆炸（Exploding Gradient）

/ 梯度爆炸: 在反向传播过程中, 误差梯度随着网络层数的增加而逐渐增大, 最终变得非常大. 这种现象会导致网络中的参数更新变得不稳定, 甚至溢出, 使得训练过程无法进行. 

原因: 在反向传播过程中, 误差梯度会与权值矩阵相乘. 如果权值矩阵的绝对值较大, 或者网络层数较多, 误差梯度可能会在反向传播过程中不断放大, 最终导致梯度爆炸. 

影响: 梯度爆炸会导致参数更新变得不稳定, 甚至溢出, 使得训练过程无法进行. 这通常发生在处理长序列数据的RNN（循环神经网络）中, 尤其是在时间序列较长的情况下. 

=== 解决方法

+ 使用*不同的激活函数*: 例如ReLU函数, 其导数在正区间恒为1, 可以有效缓解梯度消失问题. 
+ 引入*LSTM*（长短时记忆网络） : LSTM通过引入记忆状态和控制门（输入门、遗忘门、输出门）, 可以有效控制梯度的流动, 从而缓解梯度消失和梯度爆炸问题. 
+ *批量归一化（Batch Normalization）* : 通过在激活函数之前对输入进行归一化, 可以加速训练过程并缓解梯度消失问题. 
+ 调整网络结构: 例如使用*残差连接（Residual Connection）*, 可以缓解梯度消失问题, 使得深层网络更容易训练. 

== 交叉验证

/ 交叉验证（Cross-Validation, CV）: 一种在机器学习和统计学中广泛使用的评估方法, 用于估计分类器或模型的泛化能力. 其核心思想是通过将数据集划分为多个子集, 轮流使用这些子集作为训练集和测试集, 从而多次评估模型的性能, 最终通过平均结果来减少因数据划分不均带来的方差影响. 

=== 交叉验证的基本思想
在现有总体样本不变的情况下, 随机选用一部分样本作为临时的训练集, 用剩余样本作为临时的测试集, 得到一个错误率估计；然后随机选用另外一部分样本作为临时训练集, 其余样本作为临时测试集, 再得到一个错误率估计...如此反复多次, 最后将各个错误率求平均, 得到交叉验证错误率（cross-validation error rate, CV error）. 

=== 交叉验证的典型做法

交叉验证的典型做法是所谓$n$倍交叉验证法（n-fold cross-validation）. 其具体步骤如下: 
+ 数据划分: 将全部样本随机地划分为$n$个等份. 
+ 轮流测试: 在一轮实验中, 轮流抽出其中的$1$份样本作为测试样本, 用其余$n - 1$份作为训练样本, 得到$n$个错误率. 
+ 平均错误率: 将这些错误率进行平均, 作为一轮交叉验证的错误率. 
+ 多轮划分: 由于对样本的每一次划分是随意的, 人们往往进行多轮这样的划分（例如$k$轮）, 得到多个交叉验证错误率估计, 最后将多个估计再求平均. 

这种做法又称为$k$轮$n$倍交叉验证. 人们经常用的$n$值有$3, 5, 10$等, 分别称为三倍交叉验证（3-fold cross-validation）、五倍交叉验证（5-fold cross-validation）、十倍交叉验证（10-fold cross-validation）等. 

=== 交叉验证的特殊形式

交叉验证的一种特殊形式是所谓的留一法交叉验证（leave-one-out cross-validation, LOOCV）. 它的做法是不把样本进行分组, 而是每轮实验拿出一个样本来作为测试样本, 用其余的$n - 1$个样本作为训练样本集, 训练分类器, 测试对抽出的那个样本的分类是否正确；在下一轮实验中, 把之前测试的样本放回, 拿出另外一个样本作为测试样本, 用剩余的$n - 1$个样本作训练, 再对抽出的样本作测试；依此类推, 直到每个样本都被作为测试样本一次. 全部$n$轮实验完成后, 统计总共出现的测试错误数（不妨记作$m$）, $m$占总样本数的比例就是留一法交叉验证错误率. 

可以证明, 交叉验证法获得的错误估计是对错误率的一种最大似然估计. 

= 模式识别系统典型构成

模式识别系统的典型构成通常包括以下五个主要部分: *信息获取, 预处理, 特征提取, 分类器设计, 分类决策*. 

1. 原始数据的获取和预处理

这是模式识别系统的起点, 涉及从实际问题中获取原始数据, 并对其进行必要的预处理. 预处理的目的是为了提高数据的质量, 使其更适合后续的特征提取和分类或聚类. 例如, 在图像识别中, 原始数据可能包括图像的像素值, 预处理可能包括灰度化、去噪、归一化等操作. 在语音识别中, 原始数据可能是声波信号, 预处理可能包括采样、分帧、加窗等操作. 

2. 特征提取与选择

特征提取是从原始数据中提取出能够有效区分不同类别的特征. 这些特征可以是数值型的, 也可以是非数值型的, 如颜色、形状等. 特征提取的方法包括*主成分分析（PCA）、Karhunen-Loève变换（K-L变换）、多维尺度法（MDS）*等. 特征选择则是从提取出的特征中选择出最相关的特征, 以减少计算复杂度并提高分类或聚类的准确性. 特征选择的方法包括*分段定界算法、遗传算法、包裹法*等. 

3. 分类或聚类

分类是监督模式识别的核心任务, 其目的是将输入数据分配到预定义的类别中. 分类方法包括*基于模型的分类方法（如隐马尔可夫模型、贝叶斯网络）、基于决策树的分类方法（如ID3算法）、基于神经网络的分类方法（如多层感知器、卷积神经网络）*等. 聚类是非监督模式识别的核心任务, 其目的是将输入数据划分为不同的类别, 而不需要预先定义类别. 聚类方法包括*K均值算法（K-means）、模糊C均值算法（FCM）、自组织映射（SOM）*等. 

4. 后处理

后处理是对分类或聚类结果进行进一步的优化和解释. 例如, 在分类任务中, 后处理可能包括对分类结果进行评估, 如计算分类准确率、召回率等；在聚类任务中, 后处理可能包括对聚类结果进行解释, 如分析聚类类与研究目标之间的关系, 根据领域知识对聚类结果进行进一步的处理. 

- 监督模式识别: 通常包括分析问题、原始特征获取、特征提取与选择、*分类器设计、分类决策*等步骤. 
- 非监督模式识别: 通常包括分析问题、原始特征获取、特征提取与选择、*聚类分析、结果解释*等步骤. 

= 贝叶斯估计

贝叶斯估计在前文有介绍，这里不做赘述。

== 朴素贝叶斯估计的计算应用 (计算题)

朴素贝叶斯估计是一种基于贝叶斯定理的分类方法, 它*假设特征之间相互独立*, 从而简化了联合概率分布的计算. 这种方法在文本分类、垃圾邮件过滤、推荐系统等领域有广泛应用. 下面将详细介绍朴素贝叶斯估计的计算应用, 并通过一个具体的例子来说明其计算过程. 

=== 原理

朴素贝叶斯估计的核心思想是利用贝叶斯定理, 将后验概率分布表示为先验概率分布与似然函数的乘积. 具体来说, 贝叶斯公式如下：

$ P(omega_j | x) = (P(x | omega_j) P(omega_j)) / (sum_(i = 1)^c P(x | omega_i) P(omega_i)) $
 
其中: 
- $P(omega_j | x)$是在观测数据$x$下类别$omega_j$的后验概率. 
- $P(x | omega_j)$是在类别$omega_j$下观测数据$x$的概率（似然函数）. 
- $P(omega_j)$是类别$omega_j$的先验概率. 
- 分母是所有类别的后验概率的总和, 用于归一化. 

朴素贝叶斯估计做出了假设*特征之间相互独立*，有$p(x_i x_j|omega_k)=p(x_i|omega_k)p(x_j|omega_k)$


=== 步骤

+ 定义先验概率: 根据训练数据, 计算每个类别的先验概率$P(omega_j)$. 通常, 先验概率可以通过训练数据中各类样本的比例来估计. 
+ 定义似然函数: 假设每个特征在给定类别下服从某种分布（如正态分布或伯努利分布）, 并计算每个特征在给定类别下的条件概率$P(x_i | omega_j)$. 
+ 计算后验概率: 利用贝叶斯公式计算每个类别的后验概率$P(omega_j | x)$. 
+ 决策: 选择后验概率最大的类别作为最终的分类结果. 

=== 例题

假设我们有一个客户数据集, 包含客户的年龄、性别、收入和是否购买车的信息. 我们希望使用朴素贝叶斯估计来预测一个新客户是否可能会买车. 

1. 数据准备

假设我们有以下数据: 
#align(
  center, 
  table(
    fill: (x, y) => if y == 0 {luma(80%)}, 
    align: center+horizon, 
    columns: 4, 
    [*年龄*], [*性别*], [*收入*], [*是否购买车*], 
    [25], [男], [低], [否], 
    [30], [女], [中], [是], 
    [35], [男], [高], [是], 
    [40], [女], [中], [是], 
    [45], [男], [高], [是],
  )
)

2. 定义先验概率

假设我们有两类: $omega_1$表示“购买车”, $omega_2$表示“不购买车”. 根据数据, 我们估计先验概率: 
$ P(omega_1) = 4 / 5, P(omega_2) = 1 / 5 $
 
3. 定义似然函数

假设每个特征在给定类别下服从正态分布. 例如, 年龄在“购买车”类别下的均值为 35, 方差为 10；在“不购买车”类别下的均值为 30, 方差为 10. 收入在“购买车”类别下的均值为 30, 方差为 10；在“不购买车”类别下的均值为 20, 方差为 10. 

这里并不是一定假设服从正态分布。例如，$P("男性"|omega_1)= 2/4 $这种算法。

4. 计算后验概率

假设我们有一个新客户的数据: 年龄 35, 性别 女, 收入 中. 我们需要计算该客户属于“购买车”和“不购买车”的后验概率. 

- 年龄: 在“购买车”类别下的概率为$P(35 | omega_1)$, 在“不购买车”类别下的概率为$P(35 | omega_2)$. 
- 性别: 假设性别为“女”在“购买车”类别下的概率为 0.5, 而在“不购买车”类别下的概率为 0.5. 
- 收入: 在“购买车”类别下的概率为$P("中" | omega_1)$, 在“不购买车”类别下的概率为$P("中" | omega_2)$. 
计算每个特征的条件概率: 
$
  P(35 | omega_1) = 1 / sqrt(2 pi dot 10) exp(- (35 - 35)^2 / (2 dot 10)) = 1 / sqrt(20 pi) \
  P(35 | omega_2) = 1 / sqrt(2 pi dot 10) exp(- (35 - 30)^2 / (2 dot 10)) = 1 / sqrt(20 pi) e^(- 5 / 4) \ 
  P("女" | omega_1) = P("女" | omega_2) = P("中" | omega_1) = P("中" | omega_2) = 0.5
$

计算后验概率: 
$
  P(omega_1 | x) = (P(35 | omega_1) dot P("女" | omega_1) dot P("中" | omega_1) dot P(omega_1)) / P(x) \
  P(omega_2 | x) = (P(35 | omega_2) dot P("女" | omega_2) dot P("中" | omega_2) dot P(omega_2)) / P(x)
$
 
由于$P(x)$是归一化常数, 可以忽略, 因此我们只需比较分子部分: 
$
  P(omega_1 | x) prop P(35 | omega_1) dot P("女" | omega_1) dot P("中" | omega_1) dot P(omega_1) \ 
  P(omega_2 | x) prop P(35 | omega_2) dot P("女" | omega_2) dot P("中" | omega_2) dot P(omega_2)
$

计算具体值: 
$
  P(omega_1 | x) prop 1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2 dot 3 / 5 \ 
  P(omega_2 | x) prop 1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2 dot 2 / 5 
$


由于$1 / sqrt(20 pi) dot 1 / 2 dot 1 / 2$是相同的, 显然$P(omega_1 | x) > P(omega_2 | x)$, 因此该客户更可能属于“购买车”类别. 

=== 补充

对于直接使用数量计算概率的算法

当训练样本较少时可能存在某些概率值过低，比较接近0的情况，可能会影响未来判断。这里引入一种称为平滑矫正(又称Laplace平滑)处理这种问题。

对于类别$p(omega_i)$，分母加类别数分子加1；对于某一类别下的某一特征$p(x_i|omega_j)$，分母加该特征可能的取值数分子加1。

= 特征提取/降维

== 主成分分析

/ K-L (Karhunen-Loeve)变换: 最优正交线性变换, 相应的特征提取方法被称为PCA方法

/ 主成分分析（Principal Component Analysis, PCA）: 一种经典的线性降维方法, 其核心思想是通过线性变换将高维数据映射到低维空间, 同时保留数据的主要变化信息. PCA 的目标是*找到一组正交的新特征（主成分）*, 这些新特征是*原始特征的线性组合, 并且彼此之间不相关*. 

步骤如下: 

+ 计算协方差矩阵: 对数据进行中心化处理, 计算协方差矩阵. 
+ 特征值分解: 对协方差矩阵进行特征值分解, 得到特征值和对应的特征向量. 
+ 选择主成分: 根据特征值的大小, 选择前$k$个最大的特征值对应的特征向量作为主成分. 
+ 投影到新空间: 将原始数据投影到由这些主成分构成的新空间中, 实现降维. 

PCA 是一种非监督学习方法, 不考虑样本的类别信息, *仅关注数据的方差最大化*. 在很多情况下, PCA 被用于图像识别、人脸识别等任务中, 例如通过“本征脸”方法提取人脸图像的特征 . 

优点: 
- 简单高效, 计算成本低. 
- 保留了数据的主要变化信息. 
- 适用于高维数据的可视化. 

缺点: 
- 无法处理非线性关系. 
- 对噪声敏感, 可能丢失部分重要信息. 

== 多维尺度法

/ 多维尺度法（Multidimensional Scaling, MDS）: *(MDS的汉语翻译各有不同,需要注意)*一种用于数据可视化的方法，其目标是将高维数据在低维空间（通常是二维或三维）表示出来，主要依据*"尽可能保留数据点之间的距离关系"*表示。MDS 有度量型和非度量型两种类型，前者关注定量距离，后者关注定性关系。

步骤如下: 
+ 计算距离矩阵: 计算所有样本之间的距离或不相似度. 
+ 映射到低维空间: 通过优化算法, 将样本映射到低维空间, 使得映射后的距离尽可能接近原始距离. 
MDS 的核心思想是通过*最小化给定距离与表示距离之间的误差*来实现数据的可视化. 在模式识别中, MDS 可以用于分析样本之间的相似性或不相似性, 例如在心理学、经济学等领域有广泛应用. 

优点: 
- 保留了样本之间的相对距离关系. 
- 适用于非线性数据的可视化. 

缺点: 
- 计算复杂度较高. 
- 对噪声和异常值敏感. 

== 两种线性降维方法的区别

#align(
  center,
  table(
    fill: (x, y) => if y == 0 {luma(80%)},
    align: center + horizon,
    columns: 3,
    inset: 8pt, 
    [*特性*], [*主成分分析（PCA）*], [*多维尺度法（MDS）*],
    [目标], [保留数据的主要方差, 实现降维], [保留样本之间的相对距离, 实现可视化],
    [方法], [线性变换, 基于协方差矩阵], [非线性映射, 基于距离矩阵],
    [监督性], [非监督学习], [通常为非监督学习],
    [适用场景], [高维数据的降维和特征提取], [数据的可视化和相似性分析],
    [计算复杂度], [较低], [较高],
    [对噪声的敏感性], [较高], [较高],
    [是否考虑类别信息], [否], [否],
  )
)


= 类别可分性判据

== 在刻画特征对分类的贡献时需要希望满足那些条件

类别可分性判据是特征选择和特征提取中的重要概念, 用于衡量特征对分类任务的贡献. 在刻画特征对分类的贡献时, 通常希望满足以下条件: 

+ 与错误率的单调关系: 判据应与分类器的错误率（或其上界）具有单调关系, 即判据值越大, 分类性能越好. 这样可以确保判据能够较好地反映分类目标. 
+ 可加性: 当特征独立时, 判据对特征应该具有可加性, 即多个特征的联合判据等于各个特征判据的和. 这有助于在特征选择中综合考虑多个特征的贡献. 
+ 度量特性: 判据应满足非负性、对角线为零以及对称性. 具体来说, 当类别i与类别j相同时, 判据值为零；当类别不同且判据值越大, 表示分离程度越高. 
+ 单调性: 理想的判据应满足加入新的特征不会使判据值减小, 即判据值随着特征数目的增加而单调不减. 这有助于保证特征选择的稳定性. 

== 常见的四种类别可分性判据

+ 基于*类内类间距离*的可分性判据: 这类判据通过计算类内离散度和类间离散度来衡量特征的可分性. 例如, Fisher线性判别方法通过最大化类间离散度并最小化类内离散度来确定最佳投影方向. 这类判据直观易实现, 但难以与分类错误率建立直接联系. 
+ 基于*概率分布*的可分性判据: 这类判据考虑了样本的分布情况, 可以与错误率建立联系. 例如, Bhattacharyya距离、Chernoff界限和散度等概率距离度量, 以及基于熵的可分性判据. 这些判据能够更好地反映样本在特征空间中的分布情况. 
+ 基于*熵*的可分性判据: 通过信息论中的熵概念来衡量特征对分类的有效性. 具体来说, 后验概率的熵越低, 表示样本的类别分布越集中, 特征越有利于分类. 基于熵的可分性判据通过积分后验概率的熵来评估特征的整体可分性. 
+ 基于*统计检验*的可分性判据: 利用统计学中的假设检验方法, 如 t检验和秩和检验, 来判断特征在两类样本间是否存在显著差异. 这些方法可以给出一个统计量来反映两类样本间的差别, 并给出一个p-值来反映这种差异的统计显著性. 

= 非监督模式识别/聚类算法

== k-means 算法

k-means 是一种经典的聚类算法, 其基本思想是将数据划分为$k$个簇, 使得每个簇内的样本尽可能相似, 而簇之间尽可能不相似. 该算法通过迭代优化目标函数（通常是最小化样本到其所属簇中心的平方距离之和）来实现聚类. 

优点: 
- 算法简单, 易于实现. 
- 对大规模数据集具有较好的效率. 
- 适用于数据分布较为均匀的场景. 

缺点: 
- 需要预先指定聚类数目$k$, 这在实际应用中可能难以确定. 
- 对初始中心的选择敏感, 容易陷入局部最优. 
- 无法处理噪声和异常值, 对非球形分布的数据效果较差. 

== 模糊C均值（FCM）算法

模糊C均值 是 k-means 的一种扩展, 它允许样本同时属于多个簇, 并赋予每个样本到各个簇的隶属度. 这种模糊性使得 FCM 在处理重叠或边界模糊的簇时表现更好. 

核心思想: 
- 通过隶属度函数$mu_(i j)$表示样本$x_i$到簇$j$的隶属程度. 
- 通过迭代优化目标函数
$ J = sum_(j = 1)^c sum_(i = 1)^n [mu_(i j)]^2 || x_i - m_j ||^2 $其中$m_j$是簇$j$的中心. 

训练方法: 
- 设计聚类数目以及超参数$m$, 初始化关系矩阵$mu_(i j)$
- 更新聚类中心C
$ c_j = (sum_(i = 1)^N mu_(i j)^m x_i) / (sum_(i = 1)^N mu_(i j)^m) $
- 更新$mu_(i j)$
- 重复步骤（2）、（3）直到收敛两次计算得到关系矩阵U的差距较

优点: 
- 能够处理重叠的簇, 提供更细粒度的分类结果. 
- 通过调整隶属度参数$k$, 可以控制聚类的模糊程度. 

缺点: 
- 对初始中心和隶属度参数敏感. 
- 计算复杂度较高, 尤其在大规模数据集上. 

== ISODATA 算法

ISODATA（Iterative Self-Organizing Data Analysis Technique）是一种改进的 k-means 算法, 它通过动态调整聚类数目和中心, 以适应数据分布的变化. 

核心思想: 
- 通过设定参数（如合并参数、分裂参数）自动调整聚类数目. 
- 如果某个簇的样本数量过少, 则将其合并到相邻簇中；如果簇之间距离较远, 则将其分裂为两个簇. 

算法流程: 
1. 初始化: 设置聚类数$K$（可以与期望类数不一样）, 与$K$均值相同的方法确定初始中心, 同时设定7个初始参数；
2. 用最小距离法对全体样本进行聚类: 把所有样本分到距离中心最近的类中；
3. 去掉那些类别中样本数小于$N$的类别: 若某个类样本数过少, 则去掉这一类, 根据各样本到其他类中心的距离分别合入其他类, 置$k=k-1$；
4. 更新聚类中心;
5. 计算每个类的类内平均距离；
6. 判断停止、分裂或合并: 判断若是最后一次迭代, 程序终止, 否则分裂或者合并

优点: 
- 无需预先指定聚类数目, 能够自动调整聚类数目. 
- 通过合并和分裂操作, 能够更好地适应数据分布的变化. 

缺点: 
- 参数设置较为复杂, 需要根据具体问题进行调整. 
- 对噪声和异常值的鲁棒性较差. 

== DBSCAN 算法

/ DBSCAN（Density-Based Spatial Clustering of Applications with Noise）: 是一种*基于密度*的聚类算法, 它通过识别高密度区域来定义簇, 能够有效处理噪声和异常值. 

核心思想: 
- 定义两个关键参数: $epsilon.alt$（邻域半径）和$"MinPts"$（最小样本数）. 
- 一个簇是由其邻域内至少包含$"MinPts"$个样本的区域构成的. 
- 通过密度来定义簇, 能够发现任意形状的簇. 

优点: 
- 无需预先指定聚类数目, 能够自动发现任意形状的簇. 
- 对噪声和异常值具有较强的鲁棒性. 
- 适用于密度分布不均匀的数据. 

缺点: 
- 参数$epsilon.alt$和$"MinPts"$的选择对结果影响较大. 
- 在高维数据中效果较差. 

== 密度峰值聚类（DPC）算法

/ 密度峰值聚类: 一种*不需要迭代*的, 可以一次性找到聚类中心的聚类方法. 一种基于密度的聚类算法, 它通过计算每个样本的局部密度和距离最近的高密度区域的距离来定义簇. 

核心思想: 
- 聚类中心的密度（Density）应当比较大
- 聚类中心应当离比其密度更大的点较远

优点: 
- 无需预先指定聚类数目, 能够自动发现簇. 
- 对噪声和异常值具有较强的鲁棒性. 
- 适用于高维数据. 

缺点: 
- 参数选择对结果影响较大. 
- 在某些复杂数据分布中效果可能不如 DBSCAN. 

= 循环神经网络

/ 循环神经网络（Recurrent Neural Network, 简称 RNN）: 一种专门用于处理序列数据的神经网络模型. 它能够捕捉时间序列中的依赖关系, 因此在自然语言处理、语音识别、时间序列预测、图像描述生成等领域有广泛应用. 

== RNN 的基本结构

RNN 的核心思想是通过循环连接来处理时间序列数据. 每个时刻的输入不仅依赖于当前的输入, 还依赖于上一时刻的隐藏状态. 这种结构使得 RNN 能够“记住”之前的信息, 并在后续的处理中加以利用. 

在 RNN 的基本结构中, 每个节点代表一个隐藏层单元（也称为门控单元）, 通常用$h_t$表示. 这些单元通过循环连接相互连接, 形成一个闭环结构, 允许信息在时间序列中持续流动. 在某些情况下, RNN会*引导信息从输出单元返回隐藏单元, 并且隐藏层内的节点可以自连也可以互连*. 输入向量$x_t$和上一时刻的隐藏状态$h_(t - 1)$被串联起来, 经过非线性层的线性变换得到当前时刻的隐藏状态$h_t$. 整个结构体现了 RNN 的核心特性: 记忆过去的信息并利用这些信息来预测未来. 

== RNN 的输出形式

RNN 的输出形式可以根据任务的不同而变化. 常见的输出形式包括: 
- 逐条输出: 对每个时间序列输入生成一个时间序列输出. 
- 最终状态分类: 在一个时间序列样本输入完毕后, 以神经元最终状态向量为特征进行分类. 
- 中间输出组合: 在每个时间点产生一个中间输出, 并组合起来构成特征向量进行分类决策. 
- 静态输入生成时间序列输出: 从静态输入中生成时间序列输出. 

== RNN 的前身: Hopfield 网络

RNN 的前身是 Hopfield 网络, 它是一种反馈网络, 用于实现联想记忆功能. Hopfield 网络由一组带有反馈连接的阈值逻辑单元神经元组成, 每个神经元的输出值称为“状态”. Hopfield 网络对信息的处理是一个动态的过程, 通过迭代运算不断更新神经元的状态, 最终收敛到一个稳定的状态向量或几个循环的状态向量之一. Hopfield 网络的训练方法之一是根据 Hebb 学习规则设计的, 该规则模拟了自然神经系统中神经元之间连接强度变化的规律. 

== RNN 的局限性

尽管 RNN 在处理序列数据方面表现出色, 但它也存在一些局限性. 例如, RNN 在处理长序列时容易出现“梯度消失”或“梯度爆炸”问题, 这使得网络难以捕捉到较远时间步之间的依赖关系. 为了解决这一问题, 研究者提出了改进的 RNN 模型, 如长短时记忆网络（LSTM）和门控循环单元（GRU）. 

== RNN 的应用

RNN 在多个领域都有广泛应用. 例如: 
- 自然语言处理: RNN 可以用于文本生成、机器翻译、情感分析等任务. 
- 语音识别: RNN 能够处理语音信号中的时序信息, 用于语音识别和语音合成. 
- 时间序列预测: RNN 可以用于股票价格预测、天气预测等时间序列预测任务. 
- 图像描述生成: RNN 可以与卷积神经网络（CNN）结合, 用于图像描述生成. 例如, CNN 可以对图像进行多层特征提取, 产生一个抽象的输出向量, 这个向量可以作为 RNN 的初始状态, 同时用一个局部符号输入给 RNN 以启动一个句子, 按照上面的迭代过程即可产生出描述图像的词组或句子片段. 

== RNN 的改进模型

为了克服 RNN 的局限性, 研究者提出了多种改进模型. 其中,  *长短时记忆网络（LSTM）* 是最著名的改进模型之一. LSTM 引入了记忆单元（memory cell）和门控机制（input gate, forget gate, output gate）, 从而能够更好地控制信息的流动, 避免梯度消失问题. 

在 LSTM 中, 有两个随时间传递的状态向量: 一个是隐状态$h_t$, 类似于 RNN 中的神经元状态；另一个是记忆状态$C_t$. LSTM 将前一时刻的隐状态$h_(t - 1)$和当前时刻的输入$x_t$进行串联, 形成 LSTM 当前的集成信号$s_t = h_(t - 1) | x_t$ . LSTM 在处理序列数据时的记忆和遗忘机制. 

= 模式识别系统的评价方法

== 监督模式识别的评价方法

/ 训练错误率（Training Error Rate）: 在训练集上对样本进行分类时, 分类错误的样本占总样本数的比例. 它是最简单的错误率估计方法, 但存在明显的缺陷: 由于训练集已经包含在分类器的训练过程中, 因此训练错误率不能反映分类器在新数据上的推广能力. 例如, 如果训练集中的样本类别分布不均, 分类器可能会记住某些样本的类别, 从而导致训练错误率为0, 但实际在新数据上可能表现不佳. 

/ 测试错误率（Test Error Rate）: 在独立的测试集上对样本进行分类时, 分类错误的样本占总样本数的比例. 测试集与训练集相互独立, 因此测试错误率能够更真实地反映分类器在新数据上的表现. 然而, 测试集的样本数量有限, 可能导致估计的方差较大
. 为了提高估计的准确性, 通常需要使用交叉验证等方法. 

/ 交叉验证（Cross-Validation）: 一种常用的错误率估计方法, 尤其适用于样本数量较少的情况. 其基本思想是将训练集划分为多个子集, 轮流使用其中一部分作为测试集, 其余部分作为训练集, 多次重复后取平均值作为最终的错误率估计. 常见的交叉验证方法包括: 

  / k折交叉验证（k-Fold Cross-Validation） : 将训练集划分为k个等份, 每次使用k-1个子集作为训练集, 1个子集作为测试集, 重复k次后取平均. 

  / 留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV） : 每次只用一个样本作为测试样本, 其余样本作为训练集, 重复N次后取平均. 留一法的估计偏差较小, 但方差较大. 

  / n倍交叉验证: 在k轮n倍交叉验证中, 每次使用n个子集作为测试集, 其余n-1个子集作为训练集, 重复k次后取平均. 

/ 自举法与0.632估计（Bootstrap and 0.632 Estimate）: 自举法是一种通过有放回地从原始样本集中抽取样本, 生成新样本集的方法. 每个新样本集用于训练分类器, 并预测未被抽到的样本, 从而估计错误率. 由于自举法中存在重复样本, 估计的错误率通常会偏保守. 为了结合训练错误率和自举错误率, 提出了0.632估计, 该估计是这两种估计的加权平均值. 0.632估计在样本数量较少时表现较好, 能够减少估计的偏差. 

/ 置信区间估计: 在实际应用中, 如果样本数量充足, 可以采用置信区间的方法来估计错误率的置信区间. 置信区间反映了错误率的不确定性, 随着样本数量的增加, 置信区间会逐渐缩小. 此外, 还可以通过扰动权重等方法来估计错误率的置信区间. 

== 非监督模式识别的评价方法

/ 紧致性（Compactness）: 聚类内部样本的紧密程度. 通常, 紧致性可以通过类内方差或平方误差和来衡量. 类内方差越小, 说明聚类越紧密. 例如, 可以使用以下公式计算紧致性: $ V(C) = sqrt(1 / N sum_(C_k in C) sum_(c_1 in C_k) delta(c_1, mu_c)) $
 
其中, $mu_c$是类$C_k$的均值, $delta$是样本与均值之间的距离. 

/ 连接性质（Connectivity）: 聚类之间的连贯性. 通常, 连接性质可以通过类间距离或样本之间的相似性来衡量. 例如, 可以使用类间距离或两类最近样本之间的距离来计算两类间的距离. 连接性质越大, 说明聚类越连贯. 

/ 分离度（Separation）: 聚类之间的分离程度. 通常, 分离度可以通过类间距离或样本之间的相似性来衡量. 例如, 可以使用两类中心间的距离或两类最近样本之间的距离来计算两类间的距离. 分离度越大, 说明聚类越明显. 
