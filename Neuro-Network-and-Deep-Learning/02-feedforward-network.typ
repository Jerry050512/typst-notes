#import "../template/components.typ": card

= 前馈神经网络

/ 人工神经网络（Artificial Neural Network，ANN）: 是指一系列受生物学和神经科学启发的数学模型．这些模型主要是通过对人脑的神经元网络进行抽象，
构建人工神经元，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络．(简称神经网络或神经模型) 

神经网络最早是作为一种主要的*连接主义模型*．20世纪80年代中后期，最流行的一种连接主义模型是分布式并行处理模型，其有3个主要特性：
+ 信息表示是分布式的（非局部的）；
+ 记忆和知识是存储在单元之间的连接上；
+ 通过逐渐改变单元之间的连接强度来学习新的知识．

人工神经网络主要由大量神经元以及它们之间的有向连接构成,因此考虑三个方面: 
+ 神经元的激活规则 (激活函数); 
+ 网络的拓扑结构 (连接方式);
+ 学习算法. 

/ 前馈神经网络: 也称多层感知机, 各神经元分别属于不同的层．每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层．第0层称为*输入层*，最后一层称为*输出层*，其他中间层称为*隐藏层*．整个网络中*无反馈*，信号从输入层向输出层单向传播

$ z^((l)) = W^((l)) dot a^((l)) + b^((l)) $
$ a^((l)) = f_l(z^((l))) $

在应用时, 通过FNN将样本映射后输入到一个分类器实现分类, 例如使用 logistic 函数处理二分类, 使用 softmax 函数处理多分类问题.

== 神经元到常见的网络结构

生物神经元通常由一个细胞体、多条树突和一条轴突组成．树突是神经元的输入端，轴突是神经元的输出端．一个神经元可以接受多个输入信号，并通过轴突将输出信号传递给其他神经元．

/ 净输入: $z in RR$ 表示一个神经元所获得输入信号$bold(x)$的加权和，即$z = bold(omega)^Tau bold(x) + b$
/ 活性值: $a in RR$ 表示神经元对输入信号的反应，即神经元对输入信号的综合处理结果．$a = f(z)$ 其中 $f(dot.c)$ 为激活函数. 

激活函数的性质: 
+ 连续可导的非线性函数; 
+ 本身及其导函数尽可能简单; 
+ 值域要在合适的区间; 

#figure(
  table(
    columns: 3, 
    align: center+horizon,
    table.header([*类型*], [*函数*], [*特点*]),
    table.cell(rowspan: 2)[Sigmoid 型], 
    $ sigma(x) = 1 / (1 + exp(-x)) $, table.cell(rowspan: 2)[两端饱和, Tanh函数零中心化, logistic函数恒大于0],
    $ tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)) $, 
    table.cell(rowspan: 5)[ReLU 型], 
    $ "ReLU"(x) = max(0, x) $, table.cell(rowspan: 5)[计算简单高效, 梯度消失(死亡ReLU)问题, 非零中心化, 偏置偏移],
    $ "Leaky ReLU"(x) = cases(x quad "if" x > 0, alpha x quad "if" x <= 0) $, 
    $ "PReLU"_i(x) = cases(x quad "if" x > 0, alpha_i x quad "if" x <= 0) $, 
    $ "ELU"(x) = cases(x quad "if" x > 0, alpha (exp(x) - 1) quad "if" x <= 0) $,
    $ "softplus"(x) = log(1 + exp(x)) $,
    table.cell(rowspan: 2)[基于门控],
    $ "swish"(x) = x / (1 + exp(-x)) $, [], 
    $ "GELU"(x) = x P(X <= x) approx x sigma(1.702 x) $, [当使用Logistic函数来近似时，GELU相当于一种特殊的Swish函数．], 
    [Maxout], $ "maxout"_i (x) = max_j (alpha_j x + b_j) $, [Maxout函数是ReLU函数的推广，可以看作是多个ReLU函数的线性组合．]
  ), 
  caption: [常见的激活函数]
)

#figure(
  table(
    columns: 3, 
    align: center+horizon, 
    table.header([*类型*], [*说明*], [*举例*]), 
    [前馈网络], [前馈网络中各个神经元按接收信息的先后分为不同的组．每一组可以看作一个神经层．每一层中的神经元接收前一层神经元的输出，并输出到下一层神经元．整个网络中的信息是朝一个方向传播，没有反向的信息传播. \ 前馈网络可以看作一个*函数*，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射．这种网络结构简单，易于实现．], [全连接前馈网络, 卷积神经网络], 
    [记忆网络], [也称为反馈网络，网络中的神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息．和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递. \ 为了增强记忆网络的记忆容量，可以引入外部记忆单元和读写机制，用来保存一些网络的中间状态，称为*记忆增强神经网络*], [循环神经网络, Hopfield网络, 神经图灵机, 记忆网络],
    [图网络], [图网络是一种以图结构为基本单元的神经网络．在图网络中，每个节点代表一个神经元，节点之间的边代表神经元之间的连接．图网络可以用来处理具有复杂拓扑结构的数据，如社交网络、分子结构等．], [图卷积神经网络, 图自编码器, 图神经网络]
  ), 
  caption: [常见的网络结构]
)

== 反向传播算法

#align(
  center,
  card[
    $ (partial cal(L)(y, hat(y))) / (partial omega_(i j)^((l))) = (partial z^((l))) / (partial omega_(i j)^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l))) $
    $ (partial cal(L)(y, hat(y))) / (partial b_(i j)^((l))) = (partial z^((l))) / (partial b_(i j)^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l))) $
  ]
)

其中, 
$ (partial z^((l))) / (partial omega_(i j)^((l))) = II_i (a_j^((l-1))) $
$II_i (a_j^((l-1)))$表示第$i$个元素为$a_j^((l-1))$, 其余元素为0的向量

$ (partial z^((l))) / (partial b_(i j)^((l))) = I $
大小为$M_l times M_l$的单位矩阵. 

而误差项$delta^((l))$的递推公式为:
$
  delta^((l)) & = (partial cal(L)(y, hat(y))) / (partial z^((l))) \ 
  & = (partial a^((l))) / (partial z^((l))) dot (partial z^((l+1))) / (partial a^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l+1))) \ 
  & = f_l '(z^((l))) dot.o (delta^((l+1)) dot W^(l+1)^T)
$

误差项$delta^((l))$也间接反映了不同神经元对网络能力的贡献程度，从而比较好地解决了贡献度分配问题.

/ Hadamard积: 矩阵的逐点乘积, $[A dot.o B]_(m n) = a_(m n) dot b_(m n)$

*计算最后一层误差项*
$ 
  delta^((L)) & = (partial cal(L)(y, hat(y))) / (partial z^((L))) \
  & = (partial f_L (z^((L)))) / (partial z^((L))) dot (partial cal(L)(y, hat(y))) / (partial z^((L))) \
  & = f_L '(z^((L))) dot.o (partial cal(L)(y, hat(y))) / (partial hat(y)) \
$

== 自动梯度计算

常见的自动梯度计算方法主要分为*数值微分, 符号微分和自动微分*三类. 

#figure(
  table(
    columns: 3, 
    align: center+horizon,
    table.header([*类型*], [*说明*], [*优缺点*]),
    [数值微分], [数值微分是一种通过计算函数的差分来近似求导数的方法．数值微分的计算精度较低，且计算速度较慢，但在某些情况下仍可以作为一种近似方法使用．], [计算精度较低, 计算速度较慢, $Delta x$难以确定],
    [符号微分], [符号微分是一种通过解析表达式来求导数的方法．符号微分的计算精度较高，且计算速度较快，但在某些情况下可能无法得到解析表达式，或者解析表达式过于复杂，难以计算．], [计算精度较高, 编译时间长, 需要专门的数学计算语言],
    [自动微分], [自动微分是一种通过链式法则来求导数的方法．自动微分的计算精度较高，且计算速度较快，且可以自动处理复杂的复合函数，无需手动计算链式法则．], [计算精度较高, 计算速度较快, 编译时间短, 适用于深度学习等复杂场景. 采用图的方式计算. ]
  ), 
  caption: [常见的自动梯度计算方法]
)

/ 计算图: 计算图是一种有向无环图，用于表示函数的计算过程。在计算图中，每个节点表示一个操作，每个边表示一个输入输出关系。计算图可以用来表示函数的导数计算过程，从而实现自动微分。

#align(
  center, 
  card[要掌握根据计算图去计算导数. ]
)

== 优化问题

了解常见的两类优化问题: 非凸优化问题和梯度消失问题. 
/ 非凸优化问题: 指目标函数不是凸函数的优化问题, 也就是说全在局部最优解而非全局最优解. 
/ 梯度消失问题: 指在神经网络中，梯度在反向传播过程中逐渐减小，导致网络难以训练的问题. 主要在使用sigmoid型函数时, 误差反向传播算法中$delta^((l)) = f_l '(z^((l))) dot.o (delta^((l+1)) dot W^(l+1)^T)$, 其中激活函数的导数的饱和区导数趋近于0, 随着层数加深, 梯度逐渐减小, 导致梯度消失.

在深度神经网络中，减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数，比如ReLU等．解决非凸优化问题，可以采用随机梯度下降法，如Adam等. 或者适当调大学习率，或者使用动量法等, 避免陷入局部最优解. 

