#import "../template/components.typ": card

= 前馈神经网络

/ 人工神经网络 (Artificial Neural Network, ANN) : 是指一系列受生物学和神经科学启发的数学模型．这些模型主要是通过对人脑的神经元网络进行抽象, 
构建人工神经元, 并按照一定拓扑结构来建立人工神经元之间的连接, 来模拟生物神经网络．(简称神经网络或神经模型) 

神经网络最早是作为一种主要的*连接主义模型*．20世纪80年代中后期, 最流行的一种连接主义模型是分布式并行处理模型, 其有3个主要特性: 
+ 信息表示是分布式的 (非局部的); 
+ 记忆和知识是存储在单元之间的连接上; 
+ 通过逐渐改变单元之间的连接强度来学习新的知识．

人工神经网络主要由大量神经元以及它们之间的有向连接构成,因此考虑三个方面: 
+ 神经元的激活规则 (激活函数); 
+ 网络的拓扑结构 (连接方式);
+ 学习算法. 

/ 前馈神经网络: 也称多层感知机, 各神经元分别属于不同的层．每一层的神经元可以接收前一层神经元的信号, 并产生信号输出到下一层．第0层称为*输入层*, 最后一层称为*输出层*, 其他中间层称为*隐藏层*．整个网络中*无反馈*, 信号从输入层向输出层单向传播

$ z^((l)) = W^((l)) dot a^((l)) + b^((l)) $
$ a^((l)) = f_l(z^((l))) $

在应用时, 通过FNN将样本映射后输入到一个分类器实现分类, 例如使用 logistic 函数处理二分类, 使用 softmax 函数处理多分类问题.

== 神经元到常见的网络结构

生物神经元通常由一个细胞体、多条树突和一条轴突组成．树突是神经元的输入端, 轴突是神经元的输出端．一个神经元可以接受多个输入信号, 并通过轴突将输出信号传递给其他神经元．

/ 净输入: $z in RR$ 表示一个神经元所获得输入信号$bold(x)$的加权和, 即$z = bold(omega)^Tau bold(x) + b$
/ 活性值: $a in RR$ 表示神经元对输入信号的反应, 即神经元对输入信号的综合处理结果．$a = f(z)$ 其中 $f(dot.c)$ 为激活函数. 

激活函数的性质: 
+ 连续可导的非线性函数; 
+ 本身及其导函数尽可能简单; 
+ 值域要在合适的区间; 

#figure(
  table(
    columns: 3, 
    align: center+horizon,
    table.header([*类型*], [*函数*], [*特点*]),
    table.cell(rowspan: 2)[Sigmoid 型], 
    $ sigma(x) = 1 / (1 + exp(-x)) $, table.cell(rowspan: 2)[两端饱和, Tanh函数零中心化, logistic函数恒大于0],
    $ tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)) $, 
    table.cell(rowspan: 5)[ReLU 型], 
    $ "ReLU"(x) = max(0, x) $, table.cell(rowspan: 5)[计算简单高效, 梯度消失(死亡ReLU)问题, 非零中心化, 偏置偏移],
    $ "Leaky ReLU"(x) = cases(x quad "if" x > 0, alpha x quad "if" x <= 0) $, 
    $ "PReLU"_i(x) = cases(x quad "if" x > 0, alpha_i x quad "if" x <= 0) $, 
    $ "ELU"(x) = cases(x quad "if" x > 0, alpha (exp(x) - 1) quad "if" x <= 0) $,
    $ "softplus"(x) = log(1 + exp(x)) $,
    table.cell(rowspan: 2)[基于门控],
    $ "swish"(x) = x / (1 + exp(-x)) $, [], 
    $ "GELU"(x) = x P(X <= x) approx x sigma(1.702 x) $, [当使用Logistic函数来近似时, GELU相当于一种特殊的Swish函数．], 
    [Maxout], $ "maxout"_i (x) = max_j (alpha_j x + b_j) $, [Maxout函数是ReLU函数的推广, 可以看作是多个ReLU函数的线性组合．]
  ), 
  caption: [常见的激活函数]
)

#figure(
  table(
    columns: 3, 
    align: center+horizon, 
    table.header([*类型*], [*说明*], [*举例*]), 
    [前馈网络], [前馈网络中各个神经元按接收信息的先后分为不同的组．每一组可以看作一个神经层．每一层中的神经元接收前一层神经元的输出, 并输出到下一层神经元．整个网络中的信息是朝一个方向传播, 没有反向的信息传播. \ 前馈网络可以看作一个*函数*, 通过简单非线性函数的多次复合, 实现输入空间到输出空间的复杂映射．这种网络结构简单, 易于实现．], [全连接前馈网络, 卷积神经网络], 
    [记忆网络], [也称为反馈网络, 网络中的神经元不但可以接收其他神经元的信息, 也可以接收自己的历史信息．和前馈网络相比, 记忆网络中的神经元具有记忆功能, 在不同的时刻具有不同的状态．记忆神经网络中的信息传播可以是单向或双向传递. \ 为了增强记忆网络的记忆容量, 可以引入外部记忆单元和读写机制, 用来保存一些网络的中间状态, 称为*记忆增强神经网络*], [循环神经网络, Hopfield网络, 神经图灵机, 记忆网络],
    [图网络], [图网络是一种以图结构为基本单元的神经网络．在图网络中, 每个节点代表一个神经元, 节点之间的边代表神经元之间的连接．图网络可以用来处理具有复杂拓扑结构的数据, 如社交网络、分子结构等．], [图卷积神经网络, 图自编码器, 图神经网络]
  ), 
  caption: [常见的网络结构]
)

== 反向传播算法

#align(
  center,
  card[
    $ (partial cal(L)(y, hat(y))) / (partial omega_(i j)^((l))) = (partial z^((l))) / (partial omega_(i j)^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l))) $
    $ (partial cal(L)(y, hat(y))) / (partial b_(i j)^((l))) = (partial z^((l))) / (partial b_(i j)^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l))) $
  ]
)

其中, 
$ (partial z^((l))) / (partial omega_(i j)^((l))) = II_i (a_j^((l-1))) $
$II_i (a_j^((l-1)))$表示第$i$个元素为$a_j^((l-1))$, 其余元素为0的向量

$ (partial z^((l))) / (partial b_(i j)^((l))) = I $
大小为$M_l times M_l$的单位矩阵. 

而误差项$delta^((l))$的递推公式为:
$
  delta^((l)) & = (partial cal(L)(y, hat(y))) / (partial z^((l))) \ 
  & = (partial a^((l))) / (partial z^((l))) dot (partial z^((l+1))) / (partial a^((l))) dot (partial cal(L)(y, hat(y))) / (partial z^((l+1))) \ 
  & = f_l '(z^((l))) dot.o (delta^((l+1)) dot W^(l+1)^T)
$

误差项$delta^((l))$也间接反映了不同神经元对网络能力的贡献程度, 从而比较好地解决了贡献度分配问题.

/ Hadamard积: 矩阵的逐点乘积, $[A dot.o B]_(m n) = a_(m n) dot b_(m n)$

*计算最后一层误差项*
$ 
  delta^((L)) & = (partial cal(L)(y, hat(y))) / (partial z^((L))) \
  & = (partial f_L (z^((L)))) / (partial z^((L))) dot (partial cal(L)(y, hat(y))) / (partial z^((L))) \
  & = f_L '(z^((L))) dot.o (partial cal(L)(y, hat(y))) / (partial hat(y)) \
$

== 自动梯度计算

常见的自动梯度计算方法主要分为*数值微分, 符号微分和自动微分*三类. 

#figure(
  table(
    columns: 3, 
    align: center+horizon,
    table.header([*类型*], [*说明*], [*优缺点*]),
    [数值微分], [数值微分是一种通过计算函数的差分来近似求导数的方法．数值微分的计算精度较低, 且计算速度较慢, 但在某些情况下仍可以作为一种近似方法使用．], [计算精度较低, 计算速度较慢, $Delta x$难以确定],
    [符号微分], [符号微分是一种通过解析表达式来求导数的方法．符号微分的计算精度较高, 且计算速度较快, 但在某些情况下可能无法得到解析表达式, 或者解析表达式过于复杂, 难以计算．], [计算精度较高, 编译时间长, 需要专门的数学计算语言],
    [自动微分], [自动微分是一种通过链式法则来求导数的方法．自动微分的计算精度较高, 且计算速度较快, 且可以自动处理复杂的复合函数, 无需手动计算链式法则．], [计算精度较高, 计算速度较快, 编译时间短, 适用于深度学习等复杂场景. 采用图的方式计算. ]
  ), 
  caption: [常见的自动梯度计算方法]
)

/ 计算图: 计算图是一种有向无环图, 用于表示函数的计算过程. 在计算图中, 每个节点表示一个操作, 每个边表示一个输入输出关系. 计算图可以用来表示函数的导数计算过程, 从而实现自动微分. 

#align(
  center, 
  card[要掌握根据计算图去计算导数. ]
)

== 优化问题

了解常见的两类优化问题: 非凸优化问题和梯度消失问题. 
/ 非凸优化问题: 指目标函数不是凸函数的优化问题, 也就是说全在局部最优解而非全局最优解. 
/ 梯度消失问题: 指在神经网络中, 梯度在反向传播过程中逐渐减小, 导致网络难以训练的问题. 主要在使用sigmoid型函数时, 误差反向传播算法中$delta^((l)) = f_l '(z^((l))) dot.o (delta^((l+1)) dot W^(l+1)^T)$, 其中激活函数的导数的饱和区导数趋近于0, 随着层数加深, 梯度逐渐减小, 导致梯度消失.

在深度神经网络中, 减轻梯度消失问题的方法有很多种．一种简单有效的方式是使用导数比较大的激活函数, 比如ReLU等．解决非凸优化问题, 可以采用随机梯度下降法, 如Adam等. 或者适当调大学习率, 或者使用动量法等, 避免陷入局部最优解. 

== 考题与考点预测

=== 核心考点预测

本章重点考察对神经网络基础组件的理解以及核心算法的数学原理. 

+ *神经元与激活函数 (High Frequency)*
  - *MP神经元模型*：理解净输入 $z$ 与活性值 $a$ 的关系. 
  - *激活函数的性质*：
    - *Sigmoid型函数*：$sigma(x)$ 与 $tanh(x)$. 需掌握其函数形状、值域 (Sigmoid为 $(0, 1)$, Tanh为 $(-1, 1)$) 以及导数特性 (两端饱和导致梯度消失). 
    - *ReLU及其变体*：$"ReLU"(x) = max(0, x)$. 需掌握其优势 (计算高效、缓解梯度消失、稀疏性) 及“死亡ReLU”问题. 了解 Leaky ReLU、ELU、Softplus 的定义. 
  - *通用近似定理*：理解具有至少一个隐藏层的前馈神经网络可以逼近任意连续函数. 

+ *前馈神经网络结构*
  - *参数计算*：给定输入层、隐藏层、输出层的神经元数量, 能够准确计算权重矩阵 $W$ 和偏置向量 $b$ 的维度及参数总数. 
  - *全连接层*：理解第 $l$ 层的净输入 $z^((l))$ 和输出 $a^((l))$ 的计算公式：$z^((l)) = W^((l)) a^((l-1)) + b^((l))$. 

+ *反向传播算法 (Backpropagation) - 核心难点*
  - *链式法则*：BP算法的数学基础, 需熟练掌握复合函数的求导. 
  - *误差项 ($delta$) 的定义与传递*：
    - 定义：$delta^((l)) = partial cal(L) / partial z^((l))$. 
    - 递推公式：$delta^((l)) = f'_l (z^((l))) dot ((W^((l+1)))^T delta^((l+1)))$. 注意矩阵转置和逐元素相乘. 
  - *参数梯度*：利用误差项计算权重和偏置的梯度：$partial cal(L) / partial W^((l)) = delta^((l)) (a^((l-1)))^T$. 

+ *优化与正则化*
  - *梯度消失问题*：产生原因 (Sigmoid导数最大值为0.25, 深层网络链式连乘导致梯度衰减) 及解决方案 (使用ReLU等). 
  - *参数初始化*：不能全0初始化的原因 (对称权重现象导致神经元同质化). 

=== 考题预测

==== 选择题

1. (基础概念) 下列关于激活函数的说法, 错误的是：
  - A. Sigmoid 函数的输出范围是 $(0, 1)$, 可解释为概率. 
  - B. Tanh 函数是零中心化的 (Zero-Centered), 收敛速度通常快于 Sigmoid. 
  - C. ReLU 函数在 $x > 0$ 时导数为 1, 可以完全解决梯度消失问题. 
  - D. Softplus 函数是 ReLU 的平滑版本, 但计算量相对较大. 
  *解析*：C. ReLU 缓解了梯度消失, 但并未完全解决 (如在深层网络中或 $x < 0$ 时), 且存在“死亡 ReLU”问题. 

2. (参数计算) 假设一个前馈神经网络, 输入层有 $D$ 个神经元, 隐藏层有 $M$ 个神经元, 输出层有 $K$ 个神经元. 若包含偏置项, 该网络第一层 (输入到隐藏) 和第二层 (隐藏到输出) 的可学习参数总数为：
  - A. $D times M + M times K$
  - B. $(D + 1) times M + (M + 1) times K$
  - C. $(D times M + 1) + (M times K + 1)$
  - D. $D times (M + 1) + M times (K + 1)$
  *解析*：B. 权重参数为 $D times M + M times K$, 偏置参数为 $M + K$. 总计 $(D times M + M) + (M times K + K)$. 

3. (反向传播) 在反向传播算法中, 若损失函数为 $cal(L)$, 第 $l$ 层的误差项为 $delta^((l))$, 则第 $l$ 层权重矩阵 $W^((l))$ 的梯度 $partial cal(L) / partial W^((l))$ 等于：
  - A. $delta^((l)) (a^((l-1)))^T$
  - B. $(a^((l-1)))^T delta^((l))$
  - C. $W^((l)) delta^((l))$
  - D. $delta^((l)) dot f'(z^((l)))$
  *解析*：A. 根据教材公式 (4.68), 梯度为误差项与上一层激活值向量的外积. 

4. (优化问题) 下列哪项不是导致梯度消失问题的主要原因？
  - A. 网络层数过深
  - B. 使用 Sigmoid 激活函数
  - C. 权值初始化过小
  - D. 使用 ReLU 激活函数
  *解析*：D. 使用 ReLU 是缓解梯度消失的手段之一. 

==== 简答题

1. *激活函数性质*：请简述激活函数在神经网络中的作用. 如果将多层前馈神经网络中的所有激活函数都替换为线性函数 $f(x) = c x$, 网络会有什么变化？
  *参考答案*：
  - *作用*：激活函数引入了非线性因素, 使得神经网络能够逼近任意复杂的非线性函数 (依据通用近似定理). 
  - *线性化的后果*：如果使用线性函数, 无论网络有多少层, 其输出都是输入的线性组合. 
  - *结论*：整个多层网络将等价于一个单层线性模型 (如感知器), 从而失去处理非线性问题 (如异或问题) 的能力. 

2. *梯度消失*：请结合 Sigmoid 函数的导数特性, 解释梯度消失问题产生的原因, 并列举一种解决方案. 
  *参考答案*：
  - *导数特性*：Sigmoid 函数的导数为 $sigma'(x) = sigma(x)(1 - sigma(x))$, 其最大值仅为 0.25. 
  - *产生原因*：在反向传播中, 误差项是通过链式法则向前传递的, 涉及多个导数项的连乘. 如果网络很深, 多个小于 0.25 的数值相乘会迅速趋近于 0, 导致靠近输入层的梯度极其微弱, 参数无法有效更新. 
  - *解决方案*：
    - 使用 ReLU 激活函数 (在 $x>0$ 时导数为 1) ；
    - 使用 Batch Normalization；
    - 使用残差连接 (ResNet). 

3. *参数初始化*：为什么在训练神经网络时, 不能将所有权重参数 $W$ 初始化为 0？
  *参考答案*：
  - *前向传播*：如果所有权重初始化为 0, 同一层的所有神经元接收到的输入相同, 激活输出也完全相同. 
  - *反向传播*：由于输出相同, 它们获得的梯度更新也完全相同. 
  - *后果*：导致“对称权重”现象, 同一层的神经元始终在学习完全相同的特征, 网络退化为每层只有一个神经元, 无法学习复杂的模式. 

==== 计算题

1. *计算图与导数*
  给定计算图对应的函数 $f(x, w, b) = 1 / (exp(-(w x + b)) + 1)$. 
  设输入 $x = 1$, 参数初始值 $w = 0, b = 0$. 
  (1) 画出该函数的计算图 (将计算分解为基本算子). 
  (2) 利用反向传播 (链式法则) 计算 $f$ 对 $w$ 的偏导数 $partial f / partial w$. 

  *解题思路*：
  令 $h_1 = -(w x + b)$, $h_2 = exp(h_1)$, $h_3 = h_2 + 1$, $f = 1 / h_3$. 

  *前向计算*：
  $
  w x + b = 0 times 1 + 0 = 0 arrow.r h_1 = 0 \
  h_2 = exp(0) = 1 \
  h_3 = 1 + 1 = 2 \
  f = 1 / 2 = 0.5
  $

  *反向计算*：
  $
  partial f / partial h_3 &= -1 / h_3^2 = -0.25 \
  partial h_3 / partial h_2 &= 1 \
    arrow.r partial f / partial h_2 &= -0.25 \
  partial h_2 / partial h_1 &= exp(h_1) = 1 \
    arrow.r partial f / partial h_1 &= -0.25 \
  partial h_1 / partial w &= -x = -1 \
    arrow.r partial f / partial w &= (-0.25) times (-1) = 0.25
  $

2. *反向传播推导*
  设第 $l$ 层的误差项为 $delta^((l))$, 第 $l+1$ 层的误差项为 $delta^((l+1))$. 
  已知 $z^((l+1)) = W^((l+1)) a^((l)) + b^((l+1))$ 且 $a^((l)) = f_l (z^((l)))$. 
  请证明递推公式：
  $ delta^((l)) = f'_l (z^((l))) dot ((W^((l+1)))^T delta^((l+1))) $
  其中 $dot$ 表示逐元素相乘 (Hadamard积). 

  *证明要点*：
  根据链式法则展开：
  $
  delta^((l)) = (partial cal(L)) / (partial z^((l)))
  = (partial cal(L)) / (partial z^((l+1))) times (partial z^((l+1))) / (partial a^((l))) times (partial a^((l))) / (partial z^((l)))
  $

  代入各项定义：
  1. 损失函数关于下一层净输入的导数即为下一层误差项：
  $ (partial cal(L)) / (partial z^((l+1))) = delta^((l+1)) $

  2. 下一层净输入关于当前层激活值的导数 (注意维度匹配, 反向传播需转置): 
  $ (partial z^((l+1))) / (partial a^((l))) = W^((l+1)) arrow.r ("反向传递时使用") (W^((l+1)))^T $

  3. 当前层激活值关于当前层净输入的导数：
  $ (partial a^((l))) / (partial z^((l))) = f'_l (z^((l))) $

  合并得到最终公式：
  $ delta^((l)) = f'_l (z^((l))) dot ((W^((l+1)))^T delta^((l+1))) $