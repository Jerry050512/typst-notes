#import "../template/conf.typ": conf
#import "../template/components.typ": *

#show: conf.with(
  title: [
    深度学习与神经网络 习题
  ],
  authors: (
    (
      name: [Gang.],
      affiliation: [Hangzhou Dianzi University],
      email: "jerry050512@outlook.com",
    ),
  )
)

#set figure(
  numbering: none
)

= 往年真题

来源: #link("https://zhuanlan.zhihu.com/p/606945292")[知乎: 2023HDU深度学习考试试题及复习重点], #link("https://zhuanlan.zhihu.com/p/455025127")[知乎: 2021-2022题目]

== 选择

1.  (多选) 人工智能的主要领域可分为【 】\
  A.感知 B.学习 C.认知 D.以上都不对
2.  (多选) 关于轴突和树突下列正确的是【 】\
  A.轴突可以有多个 \
  B.树突可以有多个 \
  C.树突可以接受刺激传递兴奋 \
  D.轴突可以传递兴奋
3. 下列关于第L层神经元的计算正确的是【 】 \
  A.先计算上一层的净活性值, 再计算出第L层的活性值 \
  B.计算上一层的净活性值的平均值, 再计算出第L层的活性值 \
  C.先计算上一层的活性值, 再计算出第L层的净活性值 \
  D.以上都不对 \
  #emoji.key 净活性值也称净输入值. 
4.  (给了一张图) 求前馈神经网络隐藏层的某一层的W和B的维度. 
5. 给了卷积核的尺寸, 填充和步长等信息, 计算卷积后的特征尺寸. 例如, 在一个卷积神经网络中, 某层的输入特征图尺寸为 $32 times 32$. 现使用一个尺寸为 $5 times 5$ 的卷积核进行卷积运算. 已知该层的步长 (Stride) 设置为 2, 零填充 (Padding) 设置为 2.  请计算该层输出的特征图尺寸.  \
  #emoji.key $ floor((M + 2P - K) / S) + 1 $
6.  (多选) 卷积的三个特性是【 】 \
  A.局部连接 B.权重共享 C.局部不变 D.汇聚
7. 下列适用于词性标注的是【 】 \
  A.卷积神经网络 \
  B.前馈神经网络 \
  C.循环神经网络 \
  D.神经图灵机
8. 下列属于批量归一化的优点的是【 】\
  A. 统计量的计算仅取决于当前单个样本, 完全不依赖批量 (Batch) 的大小 \
  B. 在处理变长序列的循环神经网络 (RNN) 中表现出卓越的稳定性 \
  C. 训练时通过小批量的均值和方差引入随机噪声, 具有一定的正则化效果 \
  D. 测试阶段无需记录训练时的全局统计量, 直接计算当前层输出即可 \
  #emoji.key *层归一化 (LN) *在处理变长序列数据时表现更稳定
9. 神经图灵机和端到端神经网络的区别【】 \
  A.只读；读写 \
  B.读写；只读 \
  C.都只只读 \
  D.都能读写
10.  (多选) 下列属于参数密度估计会遇到的问题是【 】\
  A.模型选择问题 \
  B.不可观测变量问题 \
  C.维度灾难问题 \
  D.核函数的宽度选择困难 \
  #emoji.key 核函数和窗方法是非参数密度估计. 
11. 相比长短期记忆网络 (LSTM), 门控循环单元 (GRU) 的结构更加精简, 它主要包含哪两个门? 【 】 \
  A. 输入门 (Input Gate)、遗忘门 (Forget Gate) \
  B. 重置门 (Reset Gate)、更新门 (Update Gate) \
  C. 输入门 (Input Gate)、输出门 (Output Gate) \
  D. 遗忘门 (Forget Gate)、重置门 (Reset Gate) \
12. 在小批量梯度下降算法的单次迭代中, 正确的参数更新操作顺序是【 】 \
  ① 随机初始化参数 θ \
  ② 将训练集随机打乱 (Shuffle) \
  ③ 计算该小批量的平均梯度 \
  ④ 更新参数 θ \
  ⑤ 选取一个包含 K 个样本的小集合 (Mini-batch) \
  A. ①→②→③→④→⑤ \
  B. ②→①→⑤→③→④ \
  C. ①→②→⑤→③→④ \
  D. ①→⑤→②→④→③
13. 某卷积层输入通道数为 3, 输出通道数为 16, 卷积核尺寸为 $3 times 3$. 若考虑偏置项, 该层可学习的参数总数为【 】\
  A. 144 \
  B. 432 \
  C. 448 \
  D. 48 \
  #emoji.key $ (U times V) times D times P + P $
14. 下列哪种模型通常不被归类为深度学习模型? 【 】 \
  A. 卷积神经网络 (CNN) \
  B. 循环神经网络 (RNN) \
  C. 单层感知器 (Perceptron) \
  D. 深度信念网络 (DBN)
15. 超参数是定义模型结构和训练算法的参数, 无法通过梯度下降自动更新. 下列属于超参数的是【 】 \
  A. 连接权重矩阵 W \
  B. 偏置向量 b \
  C. 学习率 α 与正则化系数 λ \
  D. 神经元的活性值 a
16. 在自然语言处理任务 (如词性标注) 中, 通常选择循环神经网络 (RNN) 而非前馈网络的主要原因是【 】 \
  A. RNN 的计算复杂度更低, 训练速度更快 \
  B. 一个词的词性通常由上下文决定, RNN 能够捕捉序列的顺序依赖性 \
  C. RNN 只能处理长度固定的序列, 符合分词要求 \
  D. RNN 的参数在所有时间步是不共享的, 表达能力更强 \

*Answer Sheet*
#table(
  columns: 11, 
  fill: (x, y) => if (calc.even(y)) {luma(80%)},
  [*Number*], [*1*], [*2*], [*3*], [*4*], [*5*], [*6*], [*7*], [*8*], [*9*], [*10*],
  [*Answer*], [ABC], [BCD], [C], [], [$16 times 16$], [ABD], [C], [C], [B], [ABC], 
  [*Number*], [*11*], [*12*], [*13*], [*14*], [*15*], [*16*], [*17*], [*18*], [*19*], [*20*],
  [*Answer*], [B], [C], [C], [C], [C], [B]
)

== 简答题

1.  (5分) 人工智能, 机器学习, 深度学习和神经网络的区别和联系是什么? 

人工智能、机器学习、深度学习和神经网络之间存在着层层递进的包含关系与密切的技术联系. 从范围上看, *人工智能 (AI) 是最宽泛的概念, 其目标是让机器具有人类的智能*. *机器学习 (ML) 是实现人工智能的一种重要子领域, 指从有限的观测数据中通过算法总结出一般性规律, 并利用这些规律对未知数据进行预测的方法*.  \
*深度学习 (DL) 则是机器学习的一个子集, 其核心在于通过多步非线性特征转换 (即“深度”模型) 自动从原始数据中学习有效的特征表示*. 深度学习与传统机器学习的主要区别在于, 传统方法通常需要人工经验进行特征提取和转换 (特征工程), 而深度学习通过端到端学习, 让模型自动跨越“语义鸿沟”, 直接从底层特征学习到高层语义表示.  \
*神经网络 (NN) 是深度学习最主要的模型载体*. 它受人脑神经系统启发, 由大量人工神经元构成, 通过调整神经元之间的连接权重来存储知识. *神经网络和深度学习并不完全等价*, 但由于神经网络能够通过误差反向传播算法较好地解决“贡献度分配问题” (即确定各参数对输出结果的影响), 它成为了深度学习中主要采用的模型. 当一个神经网络具备多层结构 (通常超过两层) 时, 其学习过程即可被视作深度学习.  \
总结而言, 人工智能是远景目标, 机器学习是实现手段, 深度学习是其中一种通过模拟多层抽象来实现自动特征学习的高级范式, 而神经网络则是支撑深度学习最核心的数学模型与架构. 

2.  (6分) 激活函数需要具有哪些性质? 

为了增强神经网络的表示和学习能力, 激活函数通常需要具备以下性质: 
+ 首先, 激活函数应是*连续可导的非线性函数*, 非线性性质能增强网络的表示能力, 而可导性则允许模型利用数值优化的方法来学习参数. 
+ 其次, *激活函数及其导函数的形式应尽可能简单*, 这有利于提高网络整体的计算效率. 
+ 最后, 激活函数导函数的值域需要在*一个合适的区间内*, 避免因过大或过小而影响模型训练的效率和稳定性. 

3. 对于循环神经网络, 不采用门控机制, 如何改进? 

对于循环神经网络 (RNN), 若不采用门控机制, 主要通过*优化训练策略、参数初始化以及改进网络结构*来缓解长程依赖中的*梯度消失和爆炸问题*.  \
针对梯度爆炸, 最有效的方法是采用*梯度截断* (当梯度的模大于一定阈值时强制截断) 或*权重衰减* (通过 L1 或 L2 正则化限制参数取值范围), 以维持系统的稳定性.  \
针对梯度消失, 改进方式主要包括: 
+ 一是*改进激活函数*, 使用 ReLU 等导数较大的非饱和激活函数代替 Sigmoid 型函数, 以减小误差项在时间步回传时的衰减；
+ 二是*优化参数初始化*, 采用正交初始化方法, 使权重矩阵在初始化时满足正交性, 从而在信息前向传播和误差反向传播中具有范数保持性；
+ 三是*改进模型结构*, 引入类似残差连接的线性依赖机制, 使隐状态更新遵循 $h_t = h_(t-1) + g(x_t, h_(t-1); theta)$, 这种结构能使隐状态间的偏导数接近单位矩阵, 从而有效缓解梯度消失. 

4. 简述记忆网络的典型结构. 

记忆网络的典型结构由*主网络 (控制器) 、外部记忆单元、读取模块和写入模块*四部分组成. 
- 主网络负责核心信息处理及与外界交互, 并生成用于操作记忆的查询向量. 
- 外部记忆单元将信息组织为一组记忆片段 (向量组), 其组织方式可以是集合、树、栈或队列等, 旨在突破神经网络固有的容量限制. 
- 读取模块根据查询向量, 利用注意力机制进行“软性”的内容寻址并提取相关信息；
- 写入模块则依据控制器的指令更新存储内容. 这种结构通过将计算与存储分离, 使模型能以较少的参数大幅增加记忆容量. 

5. GRU的全称是什么? 相对于LSTM它有什么改进? 根据下图写出状态更新公式. 

#figure(
  image("assets/practice/gru.png"), 
  caption: [GRU网络的循环单元结构]
)

GRU的全称为门控循环单元 (Gated Recurrent Unit) . 相对于LSTM, 其主要改进在于结构更加精简: 它将LSTM中的“细胞状态”与“隐藏状态”合并为统一的隐状态 $h_t$, 并将门控机制简化为重置门和更新门两个门控, 从而在保持缓解梯度消失问题的同时, 有效降低了计算复杂度和参数量. 

根据典型的GRU结构, 其状态更新公式主要由以下步骤组成: 
+ 首先计算重置门 $r_t$ 和更新门 $z_t$ 的活性值, 公式分别为 $ r_t = sigma(W_r x_t + U_r h_(t-1) + b_r) $  $ z_t = sigma(W_z x_t + U_z h_(t-1) + b_z) $
+ 随后利用重置门控制上一个时刻状态的贡献程度, 计算候选隐状态 $ tilde(h)_t = tanh(W_h x_t + U_h (r_t dot.o h_(t-1)) + b_h) $
+ 最后通过更新门对历史状态和候选状态进行线性插值, 得到当前时刻的最终隐状态 $ h_t = z_t dot.o h_(t-1) + (1 - z_t) dot.o tilde(h)_t $

6. 什么是尺度不变性? 数据预处理有什么意义? 有哪些方法? 

尺度不变性是指*一个机器学习算法在缩放全部或部分特征后, 其学习和预测过程不受影响的特性*.  \
数据预处理的意义主要在于*提升神经网络的训练效率与模型稳定性*. 如果输入特征的尺度差异过大, 会增加参数初始化的难度, 并导致损失函数的等高线呈不均匀的椭球状, 使得梯度下降时的搜索方向偏离最优路径, 减慢收敛速度. 通过预处理使特征处于相同尺度, 可以改善优化地形, 使梯度方向更接近最优搜索方向, 从而大幅*提高优化效率*. 此外, 对于最近邻等尺度敏感模型, *预处理能防止尺度大的特征在距离计算中占据主导地位*.  \
常用的数据预处理方法包括: 
+ *最小最大值归一化*: 通过线性缩放将特征的取值范围归一到 [0,1] 或 [−1,1] 之间. 
+ *标准化 (Z值归一化) *: 将每一维特征都调整为均值为 0、方差为 1 的标准分布. 
+ *白化*: 通过主成分分析 (PCA) 等手段消除特征之间的相关性, 并使特征具有相同的方差, 从而降低数据冗余. 

7.  (6分) 什么是无监督学习, 包括哪几类? 并简要说明. 

无监督学习 (Unsupervised Learning) 是指*直接从原始数据中学习并发现其中隐藏的有价值信息*, 如有效的特征、类别、结构或概率分布等, 其学习过程不借助人工标注的标签或反馈信息. 典型的无监督学习主要包括以下三类: 
+ *无监督特征学习*: 旨在从无标签的训练数据中自动学习并挖掘有效的特征或表示, 常用于数据降维、可视化或作为监督学习前期的数据预处理, 代表方法有主成分分析 (PCA) 和自编码器 (AE) 等. 
+ *概率密度估计*: 根据一组训练样本来估计样本空间的概率密度函数. 它分为假设数据服从某种已知分布 (如高斯分布) 的参数密度估计, 以及不假设具体分布形式、直接利用样本估计密度的非参数密度估计 (如核密度估计) . 
+ *聚类*: 根据特定的准则 (如组内样本相似性高于组间相似性) 将一组样本划分为不同的簇或组, 常见的算法包括 K-Means 算法和谱聚类等. 

8. 什么是终身学习的概念, 跟归纳迁移学习多任务学习的区别? 

*终身学习 (亦称持续学习) 是指机器学习系统能够仿效人类的持续学习能力, 通过不断累积历史任务中学到的经验和知识来辅助新任务的学习, 并确保这些知识持续累积且不被遗忘. *其学习过程中的核心挑战是避免“灾难性遗忘”, 即在学习新任务的过程中, 防止对历史任务至关重要的参数信息被覆盖或破坏. 

终身学习与归纳迁移学习的主要区别在于目标侧重点不同: *归纳迁移学习旨在利用源领域的知识来优化特定目标任务的性能, 其过程通常是单向的, 并不强调知识的长期累积. *相比之下, 终身学习将知识的持续沉淀与在未来任务中的复用视为核心目标. 

终身学习与多任务学习的区别则主要体现在学习的时序性与数据可见性上: *多任务学习是在已知所有相关任务的前提下, 利用所有任务的数据进行并行的联合训练, 以实现任务间的表示共享. *而终身学习则是按照任务出现的先后顺序, 持续地、逐个地进行学习, 并不要求在学习当前任务时能够同时获取所有历史任务的数据. 

9. 请简述神经网络发展的五个主要阶段及其核心标志. 

神经网络的发展历程可分为以下五个阶段:  \
第一阶段为*模型提出期* (1943年-1969年), 由McCulloch和Pitts提出MP模型开启序幕, 随后Rosenblatt提出的感知器模型及其学习算法实现了模拟人类感知能力的初步突破.  \
第二阶段为*冰河期* (1969年-1983年), 因感知器无法处理“异或”回路问题且当时计算机算力不足, 研究进入长达十余年的停滞状态.  \
第三阶段为*反向传播算法引起的复兴期* (1983年-1995年), 反向传播算法 (BP算法) 的流行解决了多层神经网络的学习问题, 同时Hopfield网络和卷积神经网络 (LeNet-5) 在此时期取得了显著成功.  \
第四阶段为*流行度降低期* (1995年-2006年), 支持向量机 (SVM) 等统计学习理论兴起, 神经网络因优化困难、理论基础不清晰及解释性差等缺点再次陷入低潮.  \
第五阶段为*深度学习崛起期* (2006年至今), Hinton提出通过逐层预训练解决深层网络训练难题, 随后伴随大规模并行计算 (GPU) 的普及和海量数据的支持, 深度学习在语音识别和图像分类等任务中取得巨大成功, 标志着神经网络迎来第三次高潮. 

10. 简述批量归一化和层归一化的区别. 

批量归一化 (BN) 与层归一化 (LN) 的主要区别在于*计算归一化统计量的维度不同*. BN 是在小批量 (Batch) 维度上计算均值和方差, 针对单个神经元跨样本进行归一化；而 LN 是在单个样本的所有特征维度上计算统计量, 针对同层的所有神经元进行归一化. 

从依赖性来看, *BN 的效果强依赖于批量大小, 在小批量场景或动态网络 (如 RNN) 中表现较差；而 LN 的均值和方差仅取决于当前单个样本, 独立于批量大小, 因此在处理变长序列数据时具有更好的稳定性. *在应用场景上, BN 通常适用于卷积神经网络 (CNN) 及计算机视觉任务, 而 LN 则更适合于循环神经网络 (RNN) 及自然语言处理任务. 

11. 简述神经网络优化算法的主要类别及其核心思路. 

神经网络优化算法主要通过*调整学习率和修正梯度估计*两个方向来提升训练的稳定性和收敛速度.  \
学习率调整类方法旨在通过动态设置步长来平衡搜索效率, 包括*基础的学习率衰减策略 (如余弦衰减、分段常数衰减) 、学习率预热, 以及针对每个参数自适应调节步长的 AdaGrad、RMSprop 和 AdaDelta 等算法*.  \
梯度估计修正类方法则通过引入历史梯度信息来平滑参数更新轨迹, 有效应对随机梯度中的噪声, 代表性方法有*动量法、Nesterov 加速梯度以及结合了动量与自适应学习率优点的 Adam 算法*.  \
此外, *梯度截断*也是一种重要的启发式手段, 通过限制梯度模值来有效缓解训练中的梯度爆炸问题. 

12. 请简述神经网络参数初始化的意义, 并说明为何不能将权重全部初始化为0, 以及常用的初始化方法有哪些. 

参数初始化的意义在于*直接影响神经网络的优化效率与泛化能力*. 由于神经网络的参数学习是一个高维非凸优化问题, 合适的初始值能帮助网络收敛到泛化能力更高的局部最优解, 并有效缓解深层网络中的梯度消失或爆炸问题. 神经网络通常不能将权重参数全部初始化为 0, 因为这会导致“对称权重现象”, 使得第一遍前向计算时隐藏层所有神经元的激活值完全相同, 反向传播时权重的更新也完全一致, *导致神经元失去区分性*.  \
常用的参数初始化方法主要分为三类: 
+ 一是*预训练初始化*, 即利用在大规模数据上训练好的模型参数作为目标任务的初始值进行精调；
+ 二是*随机初始化*, 包括基于固定方差的采样、旨在保持每层输入输出方差一致的方差缩放初始化 (如适用于 Tanh 函数的 Xavier 初始化和适用于 ReLU 函数的 He 初始化), 以及常用于循环神经网络以保持范数不变的正交初始化；
+ 三是*固定值初始化*, 即根据经验对偏置等特定参数赋予常数值, 如将 LSTM 遗忘门的偏置设为较大的正值以缓解梯度消失. 

13. 请简述图灵机与神经图灵机 (NTM) 的关系, 并说明神经图灵机的核心组成部分及其寻址机制. 

图灵机是一种用于模拟任何可计算问题的抽象数学模型, 由无限长纸带、读写头、状态寄存器和一套控制规则组成. *神经图灵机 (NTM) 则是将神经网络的模式识别能力与传统计算机的符号处理能力相结合的增强模型. * \
神经图灵机的核心结构由*控制器、读写头和外部存储器*三部分构成. 控制器通常采用循环神经网络 (如 LSTM) 来处理输入并生成控制信号；读写头在控制器的指挥下对内存进行操作；外部存储器则是一个大的矩阵, 用于存储历史信息.  \
在寻址机制上, 神经图灵机采用了混合寻址方式: 一是*基于内容的寻址*, 通过计算查询向量与记忆单元中存储向量的相似度来提取信息；二是*基于位置的寻址*, 通过循环平移实现对内存地址的偏移或随机访问. 为了实现端到端的模型训练, 其读写操作被设计为“软性”的可微操作, 这使得模型能够学会简单的程序逻辑, 如序列复制和排序等任务. 

14. 请简述自训练 (Self-training) 与协同训练 (Co-training) 的基本思想, 并说明两者的主要联系与区别. 

自训练是一种典型的半监督学习算法, 其基本思想是利用标注数据训练模型, 并对无标注样本进行预测, 随后将预测置信度较高的样本及其伪标签加入训练集, 通过不断重复此过程来提升模型性能. 协同训练则是自训练的一种改进方法, 它要求数据具有两个相对独立的“视角”, 通过在不同视角上分别训练模型, 让两个模型互相为对方提供高置信度的伪标签样本来共同促进学习.  \
两者的主要联系在于*均旨在利用海量无标注数据来弥补标注数据的不足, 且在不同视角完全一样时, 协同训练将退化为自训练算法*. 两者的主要区别在于*对数据特征的要求不同*: 自训练通常只涉及单一模型在统一特征集上的迭代更新, 其最大的风险在于无法保证伪标签的准确性；而协同训练则强调视角的“充足性”与“条件独立性”, 通过不同视角间的信息互补来降低错误传播的概率, 提高学习的稳定性. 

== 计算题

1. 计算下列网络的输出
- *输入* $X$: $3 times 3 times 2$ (通道 $X_1, X_2$)
- *卷积核* $W$: $2 times 2 times 2$ (权重 $W_1, W_2$)
- *参数*: 步长 $S=1$, 填充 $P=0$, 偏置 $b=0.5$
- *流程*: 卷积 $arrow.r$ ReLU $arrow.r$ $2 times 2$ 最大池化

#grid(
  columns: (1fr, 1fr),
  inset: 5pt,
  align: center,
  [*输入通道 $X$*],
  [*卷积核权重 $W$*], 
  $ X_1 = mat(1, 0, 1; 2, 1, 0; 0, 0, 1) $, 
  $ W_1 = mat(1, -1; 0, 1) $, 
  $ X_2 = mat(0, 1, 2; 1, 0, 1; 2, 1, 0) $,
  $ W_2 = mat(0, 1; -1, 0) $

)

卷积输出尺寸为 $2 times 2$. 计算公式为 $Z = (X_1 * W_1) + (X_2 * W_2) + b$. 

- $Z_{1,1} = (1+0+0+1) + (0+1-1+0) + 0.5 = 2.5$
- $Z_{1,2} = (0-1+0+0) + (0+2+0+0) + 0.5 = 1.5$
- $Z_{2,1} = (2-1+0+0) + (0+0-2+0) + 0.5 = -0.5$
- $Z_{2,2} = (1+0+0+1) + (0+1-1+0) + 0.5 = 2.5$

得卷积结果: 
$ Z = mat(2.5, 1.5; -0.5, 2.5) $

应用 $f(x) = max(0, x)$: 
$ A = sigma(Z) = mat(2.5, 1.5; 0, 2.5) $

对 $2 times 2$ 矩阵执行全窗口最大池化: 
$ "Output" = max(2.5, 1.5, 0, 2.5) = bold(2.5) $

*最终结果: 2.5*

2. 给定初值: $a=2, b=3, c=4$, 根据计算图, 计算$(partial f_2) / (partial a), (partial f_2) / (partial b), (partial f_2) / (partial c)$

#figure(
  image("assets/practice/compute-graph.svg", width: 50%), 
  caption: [计算图]
)

给定初值: $a=2, b=3, c=4$. 

- $f_1 = a + b = 2 + 3 = 5$
- $f_2 = c dot f_1 = 4 dot 5 = 20$

根据链式法则, 我们从输出端 $f_2$ 向输入端推导. 

*对 $c$ 的偏导: *
  $ (partial f_2) / (partial c) = f_1 = bold(5) $

*对 $f_1$ 的中间偏导: *
  $ (partial f_2) / (partial f_1) = c = 4 $

*对 $a$ 的偏导: *
  $ (partial f_2) / (partial a) = (partial f_2) / (partial f_1) dot (partial f_1) / (partial a) = 4 dot 1 = bold(4) $

*对 $b$ 的偏导: *
  $ (partial f_2) / (partial b) = (partial f_2) / (partial f_1) dot (partial f_1) / (partial b) = 4 dot 1 = bold(4) $

$ "grad" = [ (partial f_2)/(partial a), (partial f_2)/(partial b), (partial f_2)/(partial c) ] = [4, 4, 5] $

3. 对于一个二维卷积, 输入为$3 times 3$, 卷积核大小为$2 times 2$, 将卷积重写成仿射变换的形式. 

将二维卷积重写为仿射变换 (Affine Transformation) 形式, 本质上是通过构造一个稀疏的卷积矩阵 $C$, 将卷积运算转化为矩阵与向量的乘法 $bold(z) = C bold(x) + bold(b)$. 

首先将输入 $3 times 3$ 的矩阵展平为向量 $ bold(x) = vec(x_11, x_12, x_13, x_21, x_22, x_23, x_31, x_32, x_33) $输出展平为向量 $ bold(z) = vec(z_11, z_12, z_21, z_22) $设卷积核为 $ W = mat(w_11, w_12; w_21, w_22) $
在互相关运算下, 矩阵 $C$ (尺寸 $4 times 9$) 的构造如下: 

$ bold(z) =  vec(z_11, z_12, z_21, z_22) = mat(
  w_11, w_12, 0, w_21, w_22, 0, 0, 0, 0;
  0, w_11, w_12, 0, w_21, w_22, 0, 0, 0;
  0, 0, 0, w_11, w_12, 0, w_21, w_22, 0;
  0, 0, 0, 0, w_11, w_12, 0, w_21, w_22
) vec(x_11, x_12, x_13, x_21, x_22, x_23, x_31, x_32, x_33) + bold(b) $

该矩阵 $C$ 具有明显的*稀疏性*, 其非零元素仅由卷积核元素组成；每一行仅 4 个非零元素体现了*局部连接*；不同行间重复使用相同权重则体现了*权重共享*. 通过此形式, 卷积层可视为一种特殊的、连接受限的全连接层. 