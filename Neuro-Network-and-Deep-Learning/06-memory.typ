= 注意力机制与外部记忆

== 引言: 为何需要注意力和记忆? 

- 注意力 (Attention) 机制允许模型像人一样, 通过一种信息选择机制, 从大量输入中筛选出关键、高价值的信息, 而忽略其余的无关部分. 
- 记忆 (Memory) 机制, 特别是引入外部记忆, 允许模型构建一个类似外部“硬盘”的结构, 用以存储、检索和利用长期信息, 从而突破自身固有的记忆容量限制. 

== 核心概念与术语解析

/ 注意力 (Attention):	一种人类不可或缺的复杂认知功能, 指人可以在关注一些信息的同时忽略另一些信息的选择能力. 其核心价值在于帮助大脑从大量输入信息中选择小部分的有用信息来重点处理. 
/ 聚焦式注意力 (Focus Attention):	也称为选择性注意力 (Selective Attention) . 这是一种自上而下、有预定目的、依赖于特定任务的有意识注意力. 例如, 在人群中寻找某个特定的人时, 我们会主动将注意力聚焦于人脸. 
/ 基于显著性的注意力 (Saliency-Based Attention):	一种自下而上、由外界刺激驱动的无意识注意力, 与具体任务无关. 当一个刺激信息与其周围环境显著不同时, 注意力会被动地转向这个对象. 
/ 鸡尾酒会效应 (Cocktail Party Effect):	一个经典的注意力示例. 指在嘈杂的派对上, 一个人可以专注于朋友的谈话 (聚焦式注意力), 同时也能立刻注意到背景声中突然出现的自己的名字 (基于显著性的注意力) . 
/ 联想记忆 (Associative Memory):	一种能够通过内容进行信息检索的记忆模型. 当输入一个部分或含噪声的信息时, 模型能够联想并恢复出存储在其中的完整模式. 
/ Hopfield网络 (Hopfield Network):	一种经典的联想记忆实现方式. 它是由二值神经元构成的循环神经网络, 网络状态的演化过程最终会收敛到某个稳定的“吸引点”, 从而实现模式恢复和联想记忆功能. 
/ 赫布规则 (Hebbian Rule):	Hopfield网络中使用的一种学习规则, 与人脑学习方式类似. 其核心思想是: 如果两个神经元经常同时激活, 它们之间的连接就会被加强；反之, 则连接会消失或减弱. 

== 注意力机制与外部记忆

在处理复杂任务时, 模型不需要关注输入的所有信息, 而应将计算资源分配给更重要的部分. 这种受人类视觉注意力启发的机制, 已成为深度学习处理序列数据的标准组件. 

=== 注意力机制的核心逻辑

注意力机制可以看作是一个“信息检索”过程. 给定一个查询向量 (Query), 通过计算其与每个键向量 (Key) 的相关性, 从值向量 (Value) 中提取相关信息. 

- *计算步骤*: 
  1. *评分函数*: 计算查询向量 $q$ 和每个输入向量 $x_n$ 的相关性得分 $s_n = s(q, x_n)$. 
  2. *概率分布*: 通过 $"softmax"$ 函数将得分转化为注意力分布 $alpha_n$. 
  3. *加权平均*: 计算输入向量的加权和 $"att"(q, X) = sum_{n=1}^N alpha_n x_n$. 

$
  alpha_n = "softmax"(s_n) = exp(s_n) / (sum_{i=1}^N exp(s_i))
$

=== 注意力得分函数

不同的注意力机制主要区别在于如何计算查询与键之间的相关性: 
- *加性模型*: $s(q, x) = v^T tanh(W q + U x)$. 
- *点积模型*: $s(q, x) = q^T x$. 
- #emoji.star *缩放点积模型*: $s(q, x) = (q^T x) / sqrt(d)$ (Transformer 采用此种方式, 防止在高维空间点积导致梯度消失) . 
- *双线性模型*: $s(q, x) = q^T W x$. 

=== 自注意力机制 (Self-Attention)

自注意力机制是注意力机制的一种变体, 其查询、键、值均来源于同一个输入序列. 它能够建立序列内部的远程依赖关系. 

- *基本原理*: 对于输入序列 $X = [x_1, dots, x_N]$, 通过线性变换得到三组向量: $Q=W_q X, K=W_k X, V=W_v X$. 
- *并行性*: 相比于 RNN, 自注意力机制可以并行计算序列中所有位置之间的相互关系, 计算路径长度为常数 $O(1)$, 极大地缓解了长距离信息衰减问题. 

=== 多头注意力机制 (Multi-Head Attention)

多头注意力通过多个独立的注意力“头”并行处理信息. 
- *核心逻辑*: 将 $Q, K, V$ 投影到不同的子空间, 分别计算注意力后再进行拼接. 
- *作用*: 允许模型从不同的表示空间同时捕捉序列的多种特征 (例如, 一个头关注语法结构, 另一个头关注语义信息) . 

=== 注意力机制的各种变体

- *硬注意力 (Hard Attention)*: 只选择概率最大的一个输入向量, 由于其选择过程*不可导*, 通常需要通过强化学习进行训练. 
- *软注意力 (Soft Attention)*: 对所有输入向量加权, 其过程*完全可导*. $ "att"(X, q) = sum_(n = 1)^N alpha_n x_n $
- *键值对注意力*: 更通用的形式, 输入由键和值组成 $(K, V)$. 根据查询 $Q$ 与 $K$ 的匹配度来提取 $V$. 
  $
    "att"((K, V), q) & = sum_(n = 1)^N alpha_n v_n \ 
    & = sum_(n = 1)^N "softmax"(s(k_n, q)) v_n \
    & = sum_(n = 1)^N exp(s(k_n, q)) / (sum_(i = 1)^N exp(s(k_i, q))) v_n
  $
- *结构化注意力*: 考虑输入信息之间的结构化关系 (如树形或图形结构) . 

== 外部记忆与神经网络

传统的循环神经网络 (RNN) 通过隐状态 $h_t$ 来存储历史信息, 但这种“内部记忆”面临容量限制和梯度消失问题. 为了处理更复杂的推理任务, 引入外部记忆模块成为必然选择. 

=== 记忆增强神经网络 (MANN)

记忆增强神经网络 (Memory Augmented Neural Networks) 通过引入外部记忆单元来扩展网络的存储容量. 

- *核心结构*: 由“控制器 (Controller) ”和“外部记忆 (Memory) ”组成. 控制器通常是一个神经网络 (如 CNN 或 RNN), 负责与环境交互并通过读写操作访问存储单元. 
- *读写机制*: 
  - *基于内容的寻址 (Content-based Addressing) *: 通过计算查询向量与记忆单元中存储向量的相似度 (如余弦相似度) 来提取信息. 
  - *基于位置的寻址 (Location-based Addressing) *: 通过偏移量或随机访问内存地址来存取信息. 
- *代表模型*: 神经图灵机 (NTM) 和记忆网络 (MemNN) . 

为了实现端到端的训练, 外部记忆的读写操作通常被设计成“软性”的可微操作. 假设外部记忆被组织为一个矩阵 $M in RR^(N times D)$, 其中 $N$ 是记忆单元的数量, $D$ 是每个单元的维度. 

==== 读取操作 (Reading)

控制器生成一个查询向量 $q$, 通过与记忆矩阵中的每一行进行匹配, 计算注意力分布 $alpha$ (读权重): 
- *计算权重*: $alpha_n = "softmax"(s(q, m_n))$, 其中 $s(dots)$ 通常是余弦相似度. 
- *提取信息*: 读取的内容 $r$ 是记忆单元的加权平均: $r = sum_(n=1)^N alpha_n m_n$. 

==== 写入操作 (Writing)

写入过程通常借鉴了 LSTM 的门控思想, 包含“擦除”和“添加”两个步骤: 
- *擦除 (Erase)*: 控制器生成一个擦除向量 $e$, 清除旧记忆: $M_t = M_(t-1) circle.tiny (1 - w_t e^T)$. 
- *添加 (Add)*: 控制器生成一个信息向量 $a$, 写入新内容: $M_t = M_t + w_t a^T$. 
其中 $w_t$ 是写入位置的权重. 

==== 端到端记忆网络 (MemNN)

端到端记忆网络 (End-to-End Memory Networks) 是一种可以多轮迭代从外部记忆中提取信息的架构. 

- *多层跳跃 (Multi-hop)*: 模型通常包含多个堆叠的层 (Hop) . 每一层的输出作为下一层的查询向量. 
- *工作流程*: 
  1. 将输入 (如故事背景) 映射为一组记忆向量 $m_i$. 
  2. 将问题映射为查询向量 $q$. 
  3. 计算 $q$ 与 $m_i$ 的注意力, 得到回复向量. 
  4. 经过 $K$ 轮“跳跃”推理后, 输出最终预测. 
- *优点*: 相比于简单的注意力机制, 多轮跳跃允许模型处理需要逻辑链推理的复杂问题. 

==== 神经图灵机 (NTM)

神经图灵机 (Neural Turing Machines) 由 DeepMind 提出, 它将神经网络的模式识别能力与传统计算机的符号处理能力结合起来. 

- *核心组成*: 
  - *控制器 (Controller)*: 通常是 LSTM, 负责处理输入、生成读写头所需的控制信号. 
  - *读写头 (Read/Write Heads)*: 在控制器指挥下对内存进行操作. 
  - *外部内存 (Memory Bank)*: 一个大的矩阵, 存储历史信息. 
- *混合寻址机制*: 
  - *内容寻址*: 根据内容的相似度找位置 (“找长得像的数据”) . 
  - *位置寻址*: 通过循环平移 (Shift) 实现顺序访问 (“找下一个位置的数据”) . 
- *能力*: NTM 展示了极强的算法学习能力, 能够学会简单的程序逻辑, 如复制序列 (Copy) 和排序 (Sort) . 

=== Hopfield 网络

*联想记忆分类*: 
- *自联想记忆 (Auto-associative)*: 根据部分内容找回完整内容 (Hopfield 属于此类) . 
- *异联想记忆 (Hetero-associative)*: 根据一个模式找回另一个关联模式 (如根据“人名”找回“电话”) . 

Hopfield 网络是一种经典的联想记忆模型 (Associative Memory), 由全连接的二值神经元组成, 其神经元状态取值为 ${+1, -1}$ 或 ${0, 1}$. 

- *能量函数 (Energy Function)*: 
  对于状态为 $s$ 的网络, 其能量定义为: 
  $ E = - 1/2 sum_(i,j) w_(i j) s_i s_j - sum_i b_i s_i $
  Hopfield 证明了在异步更新规则下, 网络的能量会单调递减, 最终收敛到一个局部最小点 (吸引子) . 

- *信息存储与学习*: 
  存储过程即通过 *赫布规则 (Hebbian Rule)* 确定权重矩阵 $W$. 若要存储一组模式 ${x^((1)), dots, x^((N))}$, 权重更新为: 
  $ w_(i j) = 1/M sum_(n=1)^N x_i^((n)) x_j^((n)) quad (i != j) $
  其直观理解是: 如果两个神经元在模式中经常同时激活, 它们之间的连接强度就会增加. 

- *信息检索 (联想过程) *: 
  当输入一个模糊或受损的模式时, 网络根据状态更新规则 $s_i = "sgn"(sum_j w_(i j) s_j + b_i)$ 循环迭代, 直到达到稳定状态. 这一过程体现了“基于内容的寻址”, 即通过部分特征恢复全体特征. 

== 备考习题预测

