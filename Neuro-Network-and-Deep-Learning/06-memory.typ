= 注意力机制与外部记忆

== 引言: 为何需要注意力和记忆? 

- 注意力 (Attention) 机制允许模型像人一样, 通过一种信息选择机制, 从大量输入中筛选出关键、高价值的信息, 而忽略其余的无关部分. 
- 记忆 (Memory) 机制, 特别是引入外部记忆, 允许模型构建一个类似外部“硬盘”的结构, 用以存储、检索和利用长期信息, 从而突破自身固有的记忆容量限制. 

== 核心概念与术语解析

/ 注意力 (Attention):	一种人类不可或缺的复杂认知功能, 指人可以在关注一些信息的同时忽略另一些信息的选择能力. 其核心价值在于帮助大脑从大量输入信息中选择小部分的有用信息来重点处理. 
/ 聚焦式注意力 (Focus Attention):	也称为选择性注意力 (Selective Attention). 这是一种自上而下、有预定目的、依赖于特定任务的有意识注意力. 例如, 在人群中寻找某个特定的人时, 我们会主动将注意力聚焦于人脸. 
/ 基于显著性的注意力 (Saliency-Based Attention):	一种自下而上、由外界刺激驱动的无意识注意力, 与具体任务无关. 当一个刺激信息与其周围环境显著不同时, 注意力会被动地转向这个对象. 
/ 鸡尾酒会效应 (Cocktail Party Effect):	一个经典的注意力示例. 指在嘈杂的派对上, 一个人可以专注于朋友的谈话 (聚焦式注意力), 同时也能立刻注意到背景声中突然出现的自己的名字 (基于显著性的注意力). 
/ 联想记忆 (Associative Memory):	一种能够通过内容进行信息检索的记忆模型. 当输入一个部分或含噪声的信息时, 模型能够联想并恢复出存储在其中的完整模式. 
/ Hopfield网络 (Hopfield Network):	一种经典的联想记忆实现方式. 它是由二值神经元构成的循环神经网络, 网络状态的演化过程最终会收敛到某个稳定的“吸引点”, 从而实现模式恢复和联想记忆功能. 
/ 赫布规则 (Hebbian Rule):	Hopfield网络中使用的一种学习规则, 与人脑学习方式类似. 其核心思想是: 如果两个神经元经常同时激活, 它们之间的连接就会被加强; 反之, 则连接会消失或减弱. 

== 注意力机制与外部记忆

在处理复杂任务时, 模型不需要关注输入的所有信息, 而应将计算资源分配给更重要的部分. 这种受人类视觉注意力启发的机制, 已成为深度学习处理序列数据的标准组件. 

=== 注意力机制的核心逻辑

注意力机制可以看作是一个“信息检索”过程. 给定一个查询向量 (Query), 通过计算其与每个键向量 (Key) 的相关性, 从值向量 (Value) 中提取相关信息. 

- *计算步骤*: 
  1. *评分函数*: 计算查询向量 $q$ 和每个输入向量 $x_n$ 的相关性得分 $s_n = s(q, x_n)$. 
  2. *概率分布*: 通过 $"softmax"$ 函数将得分转化为注意力分布 $alpha_n$. 
  3. *加权平均*: 计算输入向量的加权和 $"att"(q, X) = sum_{n=1}^N alpha_n x_n$. 

$
  alpha_n = "softmax"(s_n) = exp(s_n) / (sum_{i=1}^N exp(s_i))
$

=== 注意力得分函数

不同的注意力机制主要区别在于如何计算查询与键之间的相关性: 
- *加性模型*: $s(q, x) = v^T tanh(W q + U x)$. 
- *点积模型*: $s(q, x) = q^T x$. 
- *缩放点积模型*: $s(q, x) = (q^T x) / sqrt(d)$ (Transformer 采用此种方式, 防止在高维空间点积导致梯度消失). 
- *双线性模型*: $s(q, x) = q^T W x$. 

=== 自注意力机制 (Self-Attention)

自注意力机制是注意力机制的一种变体, 其查询、键、值均来源于同一个输入序列. 它能够建立序列内部的远程依赖关系. 

- *基本原理*: 对于输入序列 $X = [x_1, dots, x_N]$, 通过线性变换得到三组向量: $Q=W_q X, K=W_k X, V=W_v X$. 
- *并行性*: 相比于 RNN, 自注意力机制可以并行计算序列中所有位置之间的相互关系, 计算路径长度为常数 $O(1)$, 极大地缓解了长距离信息衰减问题. 

=== 多头注意力机制 (Multi-Head Attention)

多头注意力通过多个独立的注意力“头”并行处理信息. 
- *核心逻辑*: 将 $Q, K, V$ 投影到不同的子空间, 分别计算注意力后再进行拼接. 
- *作用*: 允许模型从不同的表示空间同时捕捉序列的多种特征 (例如, 一个头关注语法结构, 另一个头关注语义信息). 

=== 注意力机制的各种变体

- *硬注意力 (Hard Attention)*: 只选择概率最大的一个输入向量, 由于其选择过程*不可导*, 通常需要通过强化学习进行训练. 
- *软注意力 (Soft Attention)*: 对所有输入向量加权, 其过程*完全可导*. $ "att"(X, q) = sum_(n = 1)^N alpha_n x_n $
- *键值对注意力*: 更通用的形式, 输入由键和值组成 $(K, V)$. 根据查询 $Q$ 与 $K$ 的匹配度来提取 $V$. 
  $
    "att"((K, V), q) & = sum_(n = 1)^N alpha_n v_n \ 
    & = sum_(n = 1)^N "softmax"(s(k_n, q)) v_n \
    & = sum_(n = 1)^N exp(s(k_n, q)) / (sum_(i = 1)^N exp(s(k_i, q))) v_n
  $
- *结构化注意力*: 考虑输入信息之间的结构化关系 (如树形或图形结构). 

== 外部记忆与神经网络

传统的循环神经网络 (RNN) 通过隐状态 $h_t$ 来存储历史信息, 但这种“内部记忆”面临容量限制和梯度消失问题. 为了处理更复杂的推理任务, 引入外部记忆模块成为必然选择. 

=== 记忆增强神经网络 (MANN)

记忆增强神经网络 (Memory Augmented Neural Networks) 通过引入外部记忆单元来扩展网络的存储容量. 

- *核心结构*: 由“控制器 (Controller) ”和“外部记忆 (Memory) ”组成. 控制器通常是一个神经网络 (如 CNN 或 RNN), 负责与环境交互并通过读写操作访问存储单元. 
- *读写机制*: 
  - *基于内容的寻址 (Content-based Addressing) *: 通过计算查询向量与记忆单元中存储向量的相似度 (如余弦相似度) 来提取信息. 
  - *基于位置的寻址 (Location-based Addressing) *: 通过偏移量或随机访问内存地址来存取信息. 
- *代表模型*: 神经图灵机 (NTM) 和记忆网络 (MemNN). 

为了实现端到端的训练, 外部记忆的读写操作通常被设计成“软性”的可微操作. 假设外部记忆被组织为一个矩阵 $M in RR^(N times D)$, 其中 $N$ 是记忆单元的数量, $D$ 是每个单元的维度. 

==== 读取操作 (Reading)

控制器生成一个查询向量 $q$, 通过与记忆矩阵中的每一行进行匹配, 计算注意力分布 $alpha$ (读权重): 
- *计算权重*: $alpha_n = "softmax"(s(q, m_n))$, 其中 $s(dots)$ 通常是余弦相似度. 
- *提取信息*: 读取的内容 $r$ 是记忆单元的加权平均: $r = sum_(n=1)^N alpha_n m_n$. 

==== 写入操作 (Writing)

写入过程通常借鉴了 LSTM 的门控思想, 包含“擦除”和“添加”两个步骤: 
- *擦除 (Erase)*: 控制器生成一个擦除向量 $e$, 清除旧记忆: $M_t = M_(t-1) circle.tiny (1 - w_t e^T)$. 
- *添加 (Add)*: 控制器生成一个信息向量 $a$, 写入新内容: $M_t = M_t + w_t a^T$. 
其中 $w_t$ 是写入位置的权重. 

==== 端到端记忆网络 (MemNN)

端到端记忆网络 (End-to-End Memory Networks) 是一种可以多轮迭代从外部记忆中提取信息的架构. 

- *多层跳跃 (Multi-hop)*: 模型通常包含多个堆叠的层 (Hop). 每一层的输出作为下一层的查询向量. 
- *工作流程*: 
  1. 将输入 (如故事背景) 映射为一组记忆向量 $m_i$. 
  2. 将问题映射为查询向量 $q$. 
  3. 计算 $q$ 与 $m_i$ 的注意力, 得到回复向量. 
  4. 经过 $K$ 轮“跳跃”推理后, 输出最终预测. 
- *优点*: 相比于简单的注意力机制, 多轮跳跃允许模型处理需要逻辑链推理的复杂问题. 

==== 神经图灵机 (NTM)

神经图灵机 (Neural Turing Machines) 由 DeepMind 提出, 它将神经网络的模式识别能力与传统计算机的符号处理能力结合起来. 

- *核心组成*: 
  - *控制器 (Controller)*: 通常是 LSTM, 负责处理输入、生成读写头所需的控制信号. 
  - *读写头 (Read/Write Heads)*: 在控制器指挥下对内存进行操作. 
  - *外部内存 (Memory Bank)*: 一个大的矩阵, 存储历史信息. 
- *混合寻址机制*: 
  - *内容寻址*: 根据内容的相似度找位置 (“找长得像的数据”). 
  - *位置寻址*: 通过循环平移 (Shift) 实现顺序访问 (“找下一个位置的数据”). 
- *能力*: NTM 展示了极强的算法学习能力, 能够学会简单的程序逻辑, 如复制序列 (Copy) 和排序 (Sort). 

=== Hopfield 网络

*联想记忆分类*: 
- *自联想记忆 (Auto-associative)*: 根据部分内容找回完整内容 (Hopfield 属于此类). 
- *异联想记忆 (Hetero-associative)*: 根据一个模式找回另一个关联模式 (如根据“人名”找回“电话”). 

Hopfield 网络是一种经典的联想记忆模型 (Associative Memory), 由全连接的二值神经元组成, 其神经元状态取值为 ${+1, -1}$ 或 ${0, 1}$. 

- *能量函数 (Energy Function)*: 
  对于状态为 $s$ 的网络, 其能量定义为: 
  $ E = - 1/2 sum_(i,j) w_(i j) s_i s_j - sum_i b_i s_i $
  Hopfield 证明了在异步更新规则下, 网络的能量会单调递减, 最终收敛到一个局部最小点 (吸引子). 

- *信息存储与学习*: 
  存储过程即通过 *赫布规则 (Hebbian Rule)* 确定权重矩阵 $W$. 若要存储一组模式 ${x^((1)), dots, x^((N))}$, 权重更新为: 
  $ w_(i j) = 1/M sum_(n=1)^N x_i^((n)) x_j^((n)) quad (i != j) $
  其直观理解是: 如果两个神经元在模式中经常同时激活, 它们之间的连接强度就会增加. 

- *信息检索 (联想过程) *: 
  当输入一个模糊或受损的模式时, 网络根据状态更新规则 $s_i = "sgn"(sum_j w_(i j) s_j + b_i)$ 循环迭代, 直到达到稳定状态. 这一过程体现了“基于内容的寻址”, 即通过部分特征恢复全体特征. 

== 考题与考点预测

=== 核心考点预测

本章主要介绍了如何通过注意力机制和外部记忆来提升神经网络的处理能力, 特别是解决长程依赖和网络容量限制问题. 

1.  *注意力机制 (Attention Mechanism)*:
    -   *基本原理*: 资源分配方案, 解决信息超载. 
    -   *计算步骤*: 1. 计算注意力分布 (打分函数 $s(x, q)$ -> Softmax); 2. 计算加权平均. 
    -   *打分函数*: 加性模型、点积模型、缩放点积模型 ($"softmax"(Q K^T / sqrt(d_k))$)、双线性模型. 
    -   *变体*: 硬性注意力 (不可导, 需强化学习) vs 软性注意力 (可导, 端到端). 
    -   *自注意力 (Self-Attention)*: QKV模式, 建立长距离依赖, 并行计算能力优于RNN. 

2.  *记忆增强神经网络 (MANN)*:
    -   *核心思想*: 将计算 (控制器/主网络) 与存储 (外部记忆单元) 分离. 
    -   *寻址方式*: 基于内容的寻址 (Content-based Addressing), 利用注意力机制计算相似度. 
    -   *典型模型*:
        -   *端到端记忆网络 (MemN2N)*: 多跳 (Multi-Hop) 操作, 只读外部记忆. 
        -   *神经图灵机 (NTM)*: 读写头, 可读可写, 包含查询、删除、增加向量的操作. 

3.  *Hopfield 网络 (重点)*:
    -   *定义*: 循环神经网络, 神经元全连接但无自环, 权重对称 ($w_(i j) = w_(j i)$). 
    -   *状态*: 离散状态 $\{+1, -1\}$. 
    -   *学习规则*: 赫布规则 (Hebbian Rule), 经常同时激活的神经元连接加强. 
    -   *能量函数*: $E = -1/2 s^T W s - b^T s$. 
    -   *稳定性*: 能量函数随状态更新单调递减, 最终收敛于吸引点 (Attractor), 用于联想记忆和模式恢复. 

=== 考题预测

==== 选择题

1. 在自注意力模型 (Self-Attention) 中, 为了防止点积结果过大导致 Softmax 梯度过小, 通常采用缩放点积模型. 假设查询向量 $q$ 和键向量 $k$ 的维度为 $d_k$, 缩放因子通常取为：
  - A. $d_k$
  - B. $1 / d_k$
  - C. $sqrt(d_k)$
  - D. $1 / sqrt(d_k)$

  *答案：* D
  *解析：* 根据公式 8.22, 缩放点积注意力为 $"softmax"((K^T Q) / sqrt(D_k))$, 除以 $sqrt(D_k)$ 是为了让方差保持在 1 左右, 防止 Softmax 进入饱和区导致梯度消失. 

2. 关于 Hopfield 网络, 下列说法错误的是：
  - A. 这是一个全连接的循环神经网络, 每个神经元既是输入也是输出. 
  - B. 神经元之间的连接权重是对称的, 即 $w_(i j) = w_(j i)$. 
  - C. 神经元存在自反馈连接, 即 $w_(i i) != 0$. 
  - D. 网络的状态更新过程伴随着能量函数的单调递减. 

  *答案：* C
  *解析：* 根据 Hopfield 网络的定义 (公式 8.33), 连接权重满足 $w_(i i) = 0$, 即神经元和自身没有反馈相连. 

3. (多选) 下列关于记忆增强神经网络 (MANN) 的描述, 正确的是：
  - A. 引入外部记忆单元主要是为了解决标准 RNN 的长程依赖和容量限制问题. 
  - B. 神经图灵机 (NTM) 的外部记忆是不可写的, 只能读取. 
  - C. 端到端记忆网络 (MemN2N) 通常采用多跳 (Multi-Hop) 操作来增强推理能力. 
  - D. 外部记忆的读写通常使用注意力机制来实现基于内容的寻址. 

  *答案：* A, C, D
  *解析：* NTM 的外部记忆是可读可写的 (参见图 8.9), 所以 B 错误. 其他选项均符合教材描述. 

4. 在神经图灵机 (NTM) 的写操作中, 如果删除向量为 $e_t$, 增加向量为 $a_t$, 注意力分布为 $alpha_t$, 则记忆片段 $m_t$ 的更新公式涉及：
  - A. 仅相加：$m_(t+1) = m_t + alpha_t a_t$
  - B. 仅擦除：$m_(t+1) = m_t (1 - alpha_t e_t)$
  - C. 先擦除后相加：$m_(t+1) = m_t (1 - alpha_t e_t) + alpha_t a_t$
  - D. 卷积操作

  *答案：* C
  *解析：* 根据公式 8.31, 写操作分解为删除和增加两个步骤, 先按比例删除信息, 再按比例增加信息. 

==== 简答题

1. *简述注意力机制中“软性注意力”与“硬性注意力”的区别, 并说明为什么神经网络中更常使用软性注意力？*

  *参考答案：*
  - *软性注意力 (Soft Attention)*：
    1. 选择机制：计算所有输入向量在注意力分布下的加权平均 (期望). 
    2. 特点：涉及所有输入信息, 是确定性的. 
  - *硬性注意力 (Hard Attention)*：
    1. 选择机制：依据概率选择某一个输入向量 (随机采样或最大概率). 
    2. 特点：只关注某一个位置, 是随机的. 
  - *使用原因*：
    1. 硬性注意力通常不可导, 难以直接利用反向传播算法进行训练, 往往需要借助强化学习. 
    2. 软性注意力是可微的, 可以嵌入到整个神经网络中进行端到端的梯度下降训练. 

2. *请解释 Hopfield 网络的“能量函数”及其作用, 并说明为什么 Hopfield 网络具有联想记忆能力？*

  *参考答案：*
  - *能量函数*：定义为 $E = -1/2 s^T W s - b^T s$. 它是一个标量属性, 用于衡量网络状态的稳定性. 
  - *作用*：Hopfield 网络在神经元状态更新 (异步或同步) 的过程中, 能量函数是非增的 (单调递减或不变). 这意味着网络最终会收敛到一个能量的局部极小值, 称为“吸引点” (Attractor). 
  - *联想记忆能力*：
    1. 每一个吸引点对应一个存储的模式. 
    2. 当给定一个包含噪声或不完整的输入模式时, 该模式位于某个吸引点的“管辖区域”内. 
    3. 随着网络的动态演化, 状态会向能量更低的方向移动, 最终收敛到该吸引点, 从而恢复出完整的存储模式. 这就是基于内容寻址的联想记忆. 

3. *对比自注意力模型 (Self-Attention) 与 循环神经网络 (RNN) 在处理序列数据时的优缺点. *

  *参考答案：*
  - *计算并行性*：
    - RNN：依赖时间步的顺序计算, $h_t$ 依赖 $h_(t-1)$, 无法并行, 计算效率低. 
    - Self-Attention：可以并行计算所有位置的输出, 效率高. 
  - *长距离依赖*：
    - RNN：通过隐状态传递信息, 路径长度为 $O(N)$, 容易出现梯度消失, 难以捕捉长距离依赖. 
    - Self-Attention：任意两个位置之间的路径长度为 $O(1)$ (直接点积), 能有效捕捉长距离依赖. 
  - *位置信息*：
    - RNN：天然包含序列顺序信息. 
    - Self-Attention：本身不包含位置信息, 需要额外加入位置编码 (Positional Encoding). 

==== 计算题

1. *自注意力 (Self-Attention) 输出计算*

  假设输入序列由两个 2 维向量组成：$X = [x_1, x_2]$, 其中 $x_1 = [1, 0]^T, x_2 = [0, 1]^T$. 
  为简化计算, 假设查询矩阵 $W_q$、键矩阵 $W_k$、值矩阵 $W_v$ 均为单位矩阵 $I$. 
  使用点积注意力模型 (不缩放), 计算输出向量 $h_1$. 

  *解题步骤：*

  1. 计算 Query, Key, Value:
     由于 $W$ 都是单位矩阵, 所以 $Q = K = V = X$. 
     $q_1 = [1, 0]^T, k_1 = [1, 0]^T, k_2 = [0, 1]^T, v_1 = [1, 0]^T, v_2 = [0, 1]^T$. 

  2. 计算注意力打分 (未归一化):
     $ s(x_1, q_1) = k_1^T q_1 = [1, 0] dot [1, 0]^T = 1 $
     $ s(x_2, q_1) = k_2^T q_1 = [0, 1] dot [1, 0]^T = 0 $

  3. 计算注意力权重 (Softmax):
     $ alpha_(1,1) = exp(1) / (exp(1) + exp(0)) = e / (e + 1) approx 0.73 $
     $ alpha_(1,2) = exp(0) / (exp(1) + exp(0)) = 1 / (e + 1) approx 0.27 $

  4. 计算加权和得到输出 $h_1$:
     $ h_1 = alpha_(1,1) v_1 + alpha_(1,2) v_2 $
     $ h_1 = 0.73 dot [1, 0]^T + 0.27 dot [0, 1]^T $
     $ h_1 = [0.73, 0.27]^T $ (保留 $e$ 的形式作答通常更佳)
     即 $h_1 = [e/(e+1), 1/(e+1)]^T$. 