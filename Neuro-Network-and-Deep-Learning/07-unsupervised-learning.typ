= 无监督学习

==  核心概念与术语解析

/ 无监督特征学习:	从无标签数据中自动学习有效特征表示的方法。
/ 主成分分析 (PCA):	一种无监督的特征抽取方法，旨在通过线性投影减少数据冗余和噪声，并获得一组新的、相互不相关的特征。
/ 稀疏编码 (Sparse Coding):	一种特征学习方法，旨在用一个稀疏的特征向量来表示原始数据。
/ 自编码器 (Autoencoder):	一种通过神经网络进行无监督学习的模型，其目标是重构输入，使输出与输入尽可能相同。通过强制数据经过一个瓶颈（隐藏层），它能学习到一种有效的压缩表示。
/ 稀疏自编码器:	在自编码器的基础上，增加了对隐藏层神经元激活度的稀疏性约束，以学习到更有用的特征。
/ 降噪自编码器:	一种自编码器变体，通过将输入数据加入噪声，然后让模型学习恢复出原始的、无噪声的数据，从而学习到更鲁棒的特征。
/ 概率密度估计:	在无监督学习中，用于估计数据样本的概率密度函数的方法。

== 无监督特征学习

无监督特征学习的目标是通过某种变换，将原始高维数据映射到低维或更具代表性的空间中，以减少冗余并提取本质特征。

=== 主成分分析 (PCA)

PCA 是一种线性降维方法，其核心是寻找一组正交基。
- *目标*：将原始特征投影到相互不相关的方向上（主成分），使得投影后的方差最大化。
- *作用*：有效地减少数据中的冗余信息，去除噪声，并实现数据的低维压缩。

=== 稀疏编码 (Sparse Coding)

稀疏编码模拟了生物视觉系统的特性，通过一组基向量的线性组合来表示输入。
- *稀疏约束*：要求表示系数中只有极少数非零项。
- *意义*：迫使模型学习数据中具有解释性的、本质的“原子”结构。

=== 自编码器 (Autoencoder) 及其变体

自编码器（AE）通过“压缩-重构”的过程学习数据的有效表示。它包含编码器 $h = f(x)$ 和解码器 $z = g(h)$，目标是最小化重构误差 $||x - z||$。

- *稀疏自编码器 (Sparse Autoencoder)*：在损失函数中加入稀疏性惩罚（如 KL 散度约束），限制隐藏层神经元的激活。这使得神经元只对特定的输入模式产生响应。
- *堆叠自编码器 (Stacked Autoencoder)*：采用逐层贪心预训练的方式，将多个自编码器堆叠，每一层的输出作为下一层的输入。这能学习到从底层到高层的抽象特征层次。
- *降噪自编码器 (Denoising Autoencoder)*：训练模型从人为损坏（加噪）的输入 $tilde(x)$ 中恢复出原始数据 $x$。这迫使网络学习数据中稳定的、鲁棒的本质结构，防止模型简单地学到一个等恒映射。

== 概率密度估计

概率密度估计是学习数据分布 $p(x)$ 的过程。如果能够准确估计分布，我们就可以进行数据生成、异常检测或密度聚类。

=== 参数密度估计 (Parametric Density Estimation)
假设数据服从某种预定义的分布（如高斯分布、伯努利分布），核心任务是利用最大似然估计（MLE）等方法估计分布的参数。
- *局限性*：如果对分布形式的预设错误，估计结果会很不准确。

=== 非参数密度估计 (Non-parametric Density Estimation)
不预先假设数据的具体分布形式，而是直接根据样本数据点来估计概率密度。主要有核方法, 直方图方法以及最近邻方法等。
- *核密度估计 (KDE)*：在每个样本点处放置一个核函数（如高斯核），通过所有核函数的加权平均来构建整体概率密度。
- *优点*：对分布形式没有严格限制，灵活性极高。

== 考点预测与模拟习题

