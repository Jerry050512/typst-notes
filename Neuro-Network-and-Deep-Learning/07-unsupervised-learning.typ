= 无监督学习

==  核心概念与术语解析

/ 无监督特征学习:	从无标签数据中自动学习有效特征表示的方法。
/ 主成分分析 (PCA):	一种无监督的特征抽取方法，旨在通过线性投影减少数据冗余和噪声，并获得一组新的、相互不相关的特征。
/ 稀疏编码 (Sparse Coding):	一种特征学习方法，旨在用一个稀疏的特征向量来表示原始数据。
/ 自编码器 (Autoencoder):	一种通过神经网络进行无监督学习的模型，其目标是重构输入，使输出与输入尽可能相同。通过强制数据经过一个瓶颈（隐藏层），它能学习到一种有效的压缩表示。
/ 稀疏自编码器:	在自编码器的基础上，增加了对隐藏层神经元激活度的稀疏性约束，以学习到更有用的特征。
/ 降噪自编码器:	一种自编码器变体，通过将输入数据加入噪声，然后让模型学习恢复出原始的、无噪声的数据，从而学习到更鲁棒的特征。
/ 概率密度估计:	在无监督学习中，用于估计数据样本的概率密度函数的方法。

== 无监督特征学习

=== 主成分分析 (Principal Component Analysis, PCA)

主成分分析 (PCA) 是一种经典的无监督特征抽取技术。其核心目标是将原始特征通过线性变换投影到一个新的特征空间中，从而获得一组新的、相互不相关的特征。通过这种方式，PCA能够有效地减少数据中的冗余信息和噪声，是实现数据降维和特征提炼的常用方法。

=== 稀疏编码 (Sparse Coding)

稀疏编码提供了一种独特的特征学习视角。它的核心思想是寻找一组基向量，然后将每个输入数据点表示为这些基向量的稀疏线性组合。“稀疏”意味着在表示一个数据点时，只有少数几个基向量的系数是非零的。这种约束迫使模型学习到能够高效、紧凑地表达数据的底层结构和有意义的组成部分，从而提取出更具解释性的特征。

=== 自编码器 (Autoencoder) 及其关键变体

自编码器是一种利用神经网络进行无监督特征学习的强大模型。其基本结构由两部分组成：

1. 编码器 (Encoder): 将高维的输入数据 x 压缩成一个低维的潜在表示（或称为编码）h。
2. 解码器 (Decoder): 尝试从潜在表示 h 中重构出原始的输入数据，得到输出 x'。

自编码器的训练目标是最小化输入 x 与输出 x' 之间的重构误差 (Reconstruction Error)。通过这个过程，模型被迫在隐藏层 h 中学习到数据的最重要、最精华的特征表示。以下是几种关键的自编码器变体：

- 稀疏自编码器 (Sparse Autoencoder): 此模型在标准自编码器的损失函数中额外增加了一项稀疏性惩罚。该惩罚项旨在促使隐藏层神经元的激活值大部分时间为零。这种机制鼓励网络中的每个神经元成为一个“专家”，只对输入数据的特定模式或特征响应，从而学习到更加专业化和有用的特征。
- 堆叠自编码器 (Stacked Autoencoder): 该架构通过将多个自编码器层层堆叠而形成。具体而言，前一个自编码器隐藏层的输出，将作为下一个自编码器的输入。这种逐层训练的方式使得模型能够学习到数据的层次化特征表示，从底层的简单特征（如边缘）到高层的复杂特征（如物体的部件）。
- 降噪自编码器 (Denoising Autoencoder): 这种变体通过一种巧妙的方式提升了特征的鲁棒性。它首先向原始输入数据 x 中人为地加入一些噪声，得到一个损坏的版本 x'。然后，模型被训练用这个损坏的输入 x' 来重构出原始的、干净的输入 x。这个过程迫使模型不能简单地学习一个恒等映射，而是必须去捕捉数据中更稳定、更本质的结构，从而学习到对输入扰动不敏感的、更鲁棒的特征。

虽然以上讨论的方法侧重于学习数据的紧凑表示，但无监督学习还涵盖了另一项更根本的任务，即对数据自身的潜在概率分布进行建模，我们接下来将对此进行简要探讨。

== 概率密度估计

概率密度估计是无监督学习的另一个重要方向，其目标是根据观测到的数据样本，估计出这些数据背后所遵循的潜在概率分布。简单来说，就是为给定的数据集构建一个概率模型 p(x)。根据建模方法的不同，该领域主要分为两大类：参数密度估计 (Parametric Density Estimation)，它假设数据分布服从某个参数已知的函数形式（如高斯分布）；以及非参数密度估计 (Non-parametric Density Estimation)，它不预先假设数据分布的具体函数形式，而是直接根据观测数据来构建模型。

== 考点预测与模拟习题

=== 选择题

1. 降噪自编码器（Denoising Autoencoder）的核心思想是什么？ A. 最小化隐藏层的神经元数量 B. 从损坏的输入中重构出干净的原始输入 C. 强制隐藏层表示是稀疏的 D. 将数据投影到不相关的特征上 答案：B. 从损坏的输入中重构出干净的原始输入。 理由：根据9.1.6节，降噪自编码器通过学习恢复被噪声损坏的数据来学习更鲁棒的特征。
2. 下列哪种方法旨在通过线性变换将原始特征投影到新的特征空间，以获得一组相互不相关的特征并减少冗余？ A. 稀疏编码 (Sparse Coding) B. 自编码器 (Autoencoder) C. 主成分分析 (PCA) D. 堆叠自编码器 (Stacked Autoencoder) 答案：C. 主成分分析 (PCA)。 理由：根据9.1.1节，PCA的目标正是通过线性变换得到一组新的、相互不相关的特征，以减少冗余。
3. 关于自编码器（Autoencoder），以下描述错误的是？ A. 它是一种无监督学习模型。 B. 它的目标是让输出尽可能地复现输入。 C. 它由一个编码器和一个解码器组成。 D. 它必须使用有标签的数据进行训练。 答案：D. 它必须使用有标签的数据进行训练。 理由：根据9.1.3节，自编码器是一种无监督学习方法，不需要标签。

=== 简答题

1. 问题： 简述标准自编码器和降噪自编码器在训练目标上的主要区别及其对特征学习的影响。
  - 参考答案： 主要区别在于输入数据。标准自编码器学习重构其原始输入，即 x -> x。而降噪自编码器学习从一个被随机噪声损坏的输入 x' 中重构出原始的、干净的输入 x，即 x' -> x。这种差异使得降噪自编码器不能简单地学习一个恒等函数，而是被迫去学习数据中更稳定、更鲁棒的结构化特征，从而提高了模型的泛化能力。
2. 问题： 为什么无监督特征学习（如PCA和自编码器）在机器学习中具有重要意义？
  - 参考答案： 在许多实际应用中，获取大量有标签的数据成本高昂，而无标签数据则相对容易获得。无监督特征学习可以直接从这些无标签数据中自动学习到有效的数据表示（特征）。这些学习到的特征能够捕捉数据内在的结构和模式，可以替代耗时耗力的人工特征工程，并作为后续监督学习任务（如分类、回归）的输入，从而提升整体模型的性能。

=== 综合分析题

- 问题： 假设你正在处理一个高维图像数据集，其中许多像素特征之间存在高度相关性（冗余）。你希望在不损失过多信息的前提下降低数据维度，以便后续进行分类任务。请问你会选择本章介绍的哪种方法？请阐述你的选择理由，并简述该方法的基本步骤。
- 参考答案：
  - 选择的方法： 我会选择主成分分析 (PCA)。
  - 选择理由： PCA是一种经典且高效的无监督降维方法，其核心目标就是通过线性变换消除特征之间的相关性，从而获得一组新的、不相关的特征来表示数据。这与题目中“许多像素特征之间存在高度相关性”和“降低数据维度”的需求完全吻合。
  - 基本步骤：
    1. 数据中心化： 对原始数据集中的每个特征维度，减去该维度的均值，使得数据中心化。
    2. 计算协方差矩阵： 计算中心化后数据集的协方差矩阵，该矩阵描述了不同特征维度之间的相关性。
    3. 特征值分解： 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。每个特征向量代表一个主成分方向。
    4. 选择主成分： 将特征值从大到小排序，选择前K个最大的特征值对应的特征向量作为新的基。K的数量可以根据所需保留的信息百分比来确定。
    5. 数据投影： 将中心化后的原始数据投影到由这K个特征向量构成的新空间中，得到降维后的新特征表示。
