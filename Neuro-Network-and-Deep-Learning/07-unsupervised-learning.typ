= 无监督学习

==  核心概念与术语解析

/ 无监督特征学习:	从无标签数据中自动学习有效特征表示的方法. 
/ 主成分分析 (PCA):	一种无监督的特征抽取方法, 旨在通过线性投影减少数据冗余和噪声, 并获得一组新的、相互不相关的特征. 
/ 稀疏编码 (Sparse Coding):	一种特征学习方法, 旨在用一个稀疏的特征向量来表示原始数据. 
/ 自编码器 (Autoencoder):	一种通过神经网络进行无监督学习的模型, 其目标是重构输入, 使输出与输入尽可能相同. 通过强制数据经过一个瓶颈 (隐藏层), 它能学习到一种有效的压缩表示. 
/ 稀疏自编码器:	在自编码器的基础上, 增加了对隐藏层神经元激活度的稀疏性约束, 以学习到更有用的特征. 
/ 降噪自编码器:	一种自编码器变体, 通过将输入数据加入噪声, 然后让模型学习恢复出原始的、无噪声的数据, 从而学习到更鲁棒的特征. 
/ 概率密度估计:	在无监督学习中, 用于估计数据样本的概率密度函数的方法. 

== 无监督特征学习

无监督特征学习的目标是通过某种变换, 将原始高维数据映射到低维或更具代表性的空间中, 以减少冗余并提取本质特征. 

=== 主成分分析 (PCA)

PCA 是一种线性降维方法, 其核心是寻找一组正交基. 
- *目标*: 将原始特征投影到相互不相关的方向上 (主成分), 使得投影后的方差最大化. 
- *作用*: 有效地减少数据中的冗余信息, 去除噪声, 并实现数据的低维压缩. 

=== 稀疏编码 (Sparse Coding)

稀疏编码模拟了生物视觉系统的特性, 通过一组基向量的线性组合来表示输入. 
- *稀疏约束*: 要求表示系数中只有极少数非零项. 
- *意义*: 迫使模型学习数据中具有解释性的、本质的“原子”结构. 

=== 自编码器 (Autoencoder) 及其变体

自编码器 (AE) 通过“压缩-重构”的过程学习数据的有效表示. 它包含编码器 $h = f(x)$ 和解码器 $z = g(h)$, 目标是最小化重构误差 $||x - z||$. 

- *稀疏自编码器 (Sparse Autoencoder)*: 在损失函数中加入稀疏性惩罚 (如 KL 散度约束), 限制隐藏层神经元的激活. 这使得神经元只对特定的输入模式产生响应. 
- *堆叠自编码器 (Stacked Autoencoder)*: 采用逐层贪心预训练的方式, 将多个自编码器堆叠, 每一层的输出作为下一层的输入. 这能学习到从底层到高层的抽象特征层次. 
- *降噪自编码器 (Denoising Autoencoder)*: 训练模型从人为损坏 (加噪) 的输入 $tilde(x)$ 中恢复出原始数据 $x$. 这迫使网络学习数据中稳定的、鲁棒的本质结构, 防止模型简单地学到一个等恒映射. 

== 概率密度估计

概率密度估计是学习数据分布 $p(x)$ 的过程. 如果能够准确估计分布, 我们就可以进行数据生成、异常检测或密度聚类. 

=== 参数密度估计 (Parametric Density Estimation)
假设数据服从某种预定义的分布 (如高斯分布、伯努利分布), 核心任务是利用最大似然估计 (MLE) 等方法估计分布的参数. 
- *局限性*: 如果对分布形式的预设错误, 估计结果会很不准确. 

=== 非参数密度估计 (Non-parametric Density Estimation)
不预先假设数据的具体分布形式, 而是直接根据样本数据点来估计概率密度. 主要有核方法, 直方图方法以及最近邻方法等. 
- *核密度估计 (KDE)*: 在每个样本点处放置一个核函数 (如高斯核), 通过所有核函数的加权平均来构建整体概率密度. 
- *优点*: 对分布形式没有严格限制, 灵活性极高. 

== 考题与考点预测

=== 核心考点预测

本章 (第九章 无监督学习) 在考试中通常占比较小, 主要考察对基础概念的理解. 核心考点如下：

1.  *无监督特征学习*：
    -   *主成分分析 (PCA)*：核心思想是最大化投影方差 (或最小化重构误差), 通过协方差矩阵的特征值分解求解, 属于线性降维. 
    -   *稀疏编码 (Sparse Coding)*：基向量 (字典) 通常是过完备的 ($M > D$), 利用稀疏性约束 (如 $l_1$ 范数) 寻找数据的稀疏表示. 
    -   *自编码器 (AE)*：编码器与解码器结构, 最小化重构错误. 
    -   *降噪自编码器 (DAE)*：核心思想是在输入端加入噪声 (Mask或高斯噪声), 强迫网络从损坏数据中恢复原始数据, 学习鲁棒特征. 

2.  *概率密度估计*：
    -   *参数密度估计*：假设数据服从已知分布 (如高斯分布), 利用MLE估计参数. 
    -   *非参数密度估计*：不假设分布形式. 
        -   *直方图方法*：会导致维度灾难. 
        -   *核密度估计 (Parzen窗)*：固定区域大小 (窗宽 $H$), 统计样本数. 
        -   *K近邻方法 (KNN)*：固定样本数 $K$, 调整区域大小. 

=== 考题预测

==== 选择题

1. 主成分分析 (PCA) 是一种常用的无监督特征学习方法, 关于 PCA 的描述, 下列哪项是正确的？
  - A. PCA 是一种非线性降维方法
  - B. PCA 的目标是使得数据在转换后的空间中方差最小
  - C. PCA 的投影向量是原始样本协方差矩阵的特征向量
  - D. PCA 可以保证投影后数据的类别可分性最优

  *答案：C*
  *解析：* PCA 是线性降维, A 错；目标是最大化方差, B 错；PCA 是无监督的, 不利用标签, 无法保证类别可分性 (那是 LDA 的目标), D 错；PCA 通过计算协方差矩阵的特征向量来确定投影方向, C 对. 

2. 降噪自编码器 (Denoising Auto-Encoder, DAE) 相比于普通自编码器, 其主要的改进手段是？
  - A. 在隐藏层增加稀疏性约束
  - B. 强制解码器的权重等于编码器的转置 (捆绑权重) 
  - C. 将输入数据随机损坏 (置0或加噪) 后作为网络输入, 并以原始无损数据作为重构目标
  - D. 使用层与层之间逐层训练 (Layer-Wise Training) 的方式

  *答案：C*
  *解析：* DAE 的核心在于引入噪声, 通过重构原始输入来学习鲁棒特征. A 是稀疏自编码器, B 是 Tie Weight 技巧, D 是堆叠自编码器的训练策略. 

3. 在非参数密度估计中, 解决直方图方法中“维度灾难”以及不连续性问题的一种改进方法是核密度估计 (Kernel Density Estimation). 关于核密度估计, 下列说法正确的是？
  - A. 核密度估计假设数据服从高斯分布, 并估计其均值和方差
  - B. 核密度估计中, 核函数的宽度 (或方差) 通常是根据样本密度自适应变化的
  - C. 核密度估计本质上属于参数估计方法
  - D. 核密度估计可以看作是平滑的直方图方法, 它固定了核的宽度 (窗宽) 

  *答案：D*
  *解析：* A 和 C 描述的是参数估计；B 描述的是 K 近邻法的思想 (可变宽度) ；核密度估计 (Parzen窗) 通常固定窗宽 $H$, D 正确. 

4. 关于 K 近邻 (KNN) 方法在密度估计中的应用, 当样本总量 $N$ 趋于无穷大时, 为了保证估计的收敛性, 参数 $K$ 应该如何选择？
  - A. $K$ 保持为 1
  - B. $K$ 保持为一个固定的常数
  - C. $K$ 随 $N$ 增加而增加, 且 $K/N$ 趋向于 0
  - D. $K$ 随 $N$ 增加而增加, 且 $K approx N$

  *答案：C*
  *解析：* 根据统计学理论, 样本量增加时, 为了获得更准确的概率估计, $K$ 也需要增加以利用更多样本信息, 但为了保证区域足够小 (局部性), $K$ 的增长速度要慢于 $N$, 即 $K/N arrow.r 0$. 

==== 简答题

1. *简述降噪自编码器 (Denoising Auto-Encoder) 的工作原理及其相对于普通自编码器的优势. *
  *答案要点：*
  - *原理*：降噪自编码器在训练过程中, 首先根据一定的比例或规则 (如随机置零、加入高斯噪声) 损坏输入数据得到 $tilde(x)$；然后将 $tilde(x)$ 输入网络, 强迫网络输出重构出原始的、无损的数据 $x$. 
  - *优势*：相比于普通自编码器仅学习恒等映射 (Identity Function), DAE 能够学习到对噪声具有鲁棒性的特征表示, 不仅提取了数据的主要特征, 还捕捉了特征之间的依赖关系, 从而提高了模型的泛化能力. 

2. *请对比参数密度估计与非参数密度估计的区别, 并说明 K 近邻方法与核密度估计 (Parzen 窗) 在策略上的不同. *
  *答案要点：*
  - *参数 vs 非参数*：参数密度估计假设数据服从某个已知形式的分布 (如高斯分布), 只估计分布的参数 (如均值、方差) ；非参数密度估计不假设具体分布形式, 直接利用数据估计密度. 
  - *K 近邻 vs 核密度估计*：两者都是基于区域内的样本来估计密度 $p(x) approx K / (N V)$. 
    - 核密度估计 (Parzen 窗): 固定区域体积 (或窗宽) $V$, 统计落入该区域的样本数量 $K$ ($K$ 是变量). 
    - K 近邻方法：固定落入区域的样本数量 $K$, 调整区域的体积 $V$ 以刚好包含这 $K$ 个样本 ($V$ 是变量). 

