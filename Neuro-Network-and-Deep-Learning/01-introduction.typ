#import "../template/components.typ": card

= 绪论

== 人工智能基础

/ 贡献度分配问题: 每个组件都会对信息进行加工, 并进而影响后续的组件, 所以当我们最后得到输出结果时, 我们并不清楚其中每个组件的贡献是多少. 
/ 人工智能: 让机器具有人类的智能. "计算机控制" + "智能行为"
/ 图灵测试: 一个人在不接触对方的情况下, 通过一种特殊的方式和对方进行一系列的问答．如果在相当长时间内, 他无法根据这些问题判断对方是人还是计算机, 那么就可以认为这个计算机是智能的. 

1956年的*达特茅斯*会议上, “人工智能”被提出并作为本研究领域的名称．同时, 人工智能研究的使命也得以确定．JohnMcCarthy提出了人工智能的定义: *人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样．*

人工智能主流的方法大体上可以归结为以下两种: 
/ 符号主义: 又称逻辑主义、心理学派或计算机学派, 是指通过分析人类智能的功能, 然后用计算机来实现这些功能的一类方法．符号主义有两个基本假设: a) 信息可以用符号来表示; b) 符号可以通过显式的规则 (比如逻辑运算) 来操作．人类的认知过程可以看作符号操作过程．在人工智能的推理期和知识期, 符号主义的方法比较盛行, 并取得了大量的成果．
/ 连接主义: 又称仿生学派或生理学派, 是认知科学领域中的一类信息处理的方法和理论．在认知科学领域, 人类的认知过程可以看作一种信息处理过程．连接主义认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程, 而不是符号运算．因此, 连接主义模型的主要结构是由大量简单的信息处理单元组成的互联网络, 具有非线性、分布式、并行化、局部性计算以及自适应性等特性．

此外, 在很多文献中都有划分出*行为主义*, 主要从生物进化的角度考虑, 主张从和外界环境的互动中获取智能．

== 从传统机器学习到深度学习

/ 机器学习: 从有限的观测数据中学习 (或“猜测”) 出具有一般性的规律, 并利用这些规律对未知数据进行预测的方法.

传统的机器学习主要关注如何学习一个预测模型．一般需要首先将数据表示为一组特征, 特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型, 并输出预测结果．这类机器学习可以看作*浅层学习*．浅层学习的一个重要特点是*不涉及特征学习, 其特征主要靠人工经验或特征转换方法来抽取*．

#figure(
  image("assets/01/ml_process_flow.svg"), 
  caption: [传统机器学习的数据处理流程]
)

/ 表示学习: 可以自动地学习出有效的特征, 并提高最终机器学习模型的性能的学习算法
/ 语义鸿沟: 指输入数据的底层特征和高层语义信息之间的不一致性和差异性. 

在表示学习中, 有两个核心问题: 一是*“什么是一个好的表示”*; 二是*“如何学习到好的表示”*．

在机器学习中, 我们经常使用两种方式来表示特征: 局部表示 (LocalRepresentation) 和分布式表示 (Distributed Representation) ．

/ 深度学习: 是将原始的数据特征通过多步的特征转换得到一种特征表示, 并进一步输入到预测函数得到最终结果. 通过学习算法来让模型自动学习出好的特征表示. 

#card[深度学习是机器学习的一个子问题, 其主要目的是从数据中自动学习到有效的特征表示． 也就是说深度学习一定是机器学习, 机器学习不一定是深度学习. ]

#figure(
  image("assets/01/dl_process_flow.svg"),
  caption: [深度学习的数据处理流程]
)

深度学习需要解决的关键问题是贡献度分配问题, 即一个系统中不同的组件 (component) 或其参数对最终系统输出结果的贡献或影响．

/ 端到端学习: 在学习过程中不进行分模块或分阶段训练, 直接优化任务的总体目标．

== 神经网络

/ 赫布规则: 当神经元A的一个轴突和神经元B很近, 足以对它产生影响, 并且持续地、重复地参与了对神经元B的兴奋, 那么在这两个神经元或其中之一会发生某种生长过程或新陈代谢变化, 以致神经元A作为能使神经元B兴奋的细胞之一, 它的效能加强了．

早期的神经网络模型并不具备学习能力．首个可学习的人工神经网络是赫布网络, 采用一种基于赫布规则的无监督学习方法．感知器是最早的具有机器学习思想的神经网络, 但其学习方法无法扩展到多层的神经网络上．
直到1980年左右, *反向传播算法*才有效地解决了多层神经网络的学习问题, 并成为最为流行的神经网络学习算法．

/ 网络容量: 一个人工神经网络塑造复杂函数的能力. 

#figure(
  table(
    columns: (auto, 1.5fr, 3fr),
    inset: 10pt,
    align: horizon,
    fill: (x, y) => if y == 0 { gray.lighten(50%) },
    stroke: 0.5pt + gray,
    
    [*阶段*], [*核心特征*], [*关键事件与成果*],
    
    [第一阶段\ (1943-1969)], 
    [*模型提出期*], 
    [McCuloch-Pitts (MP) 模型开启序幕；Rosenblatt 提出*感知器 (Perceptron)* 及其学习算法，实现感知能力模拟的初步突破。],

    [第二阶段\ (1969-1983)], 
    [*冰河期*], 
    [Minsky 证明感知器无法处理 *XOR (异或)* 问题；受限于计算机算力不足，研究进入长达十余年的停滞。],

    [第三阶段\ (1983-1995)], 
    [*复兴期*], 
    [*反向传播 (BP)* 算法流行，解决多层网络学习难题；Hopfield 网络与卷积神经网络 (LeNet-5) 取得显著成功。],

    [第四阶段\ (1995-2006)], 
    [*流行度降低期*], 
    [*支持向量机 (SVM)* 等统计学习理论兴起；神经网络因优化困难、解释性差及理论基础模糊再次陷入低潮。],

    [第五阶段\ (2006-至今)], 
    [*深度学习崛起期*], 
    [Hinton 提出*逐层预训练*；伴随 *GPU* 并行计算与海量数据，深度学习在语音及图像领域取得巨大成功，迎来第三次高潮。],
  ),
  caption: [神经网络的发展历程], 
)

