#import "../template/components.typ": card

= 绪论

== 人工智能基础

/ 贡献度分配问题: 每个组件都会对信息进行加工, 并进而影响后续的组件, 所以当我们最后得到输出结果时, 我们并不清楚其中每个组件的贡献是多少. 
/ 人工智能: 让机器具有人类的智能. "计算机控制" + "智能行为"
/ 图灵测试: 一个人在不接触对方的情况下, 通过一种特殊的方式和对方进行一系列的问答．如果在相当长时间内, 他无法根据这些问题判断对方是人还是计算机, 那么就可以认为这个计算机是智能的. 

1956年的*达特茅斯*会议上, “人工智能”被提出并作为本研究领域的名称．同时, 人工智能研究的使命也得以确定．JohnMcCarthy提出了人工智能的定义: *人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样．*

人工智能主流的方法大体上可以归结为以下两种: 
/ 符号主义: 又称逻辑主义、心理学派或计算机学派, 是指通过分析人类智能的功能, 然后用计算机来实现这些功能的一类方法．符号主义有两个基本假设: a) 信息可以用符号来表示; b) 符号可以通过显式的规则 (比如逻辑运算) 来操作．人类的认知过程可以看作符号操作过程．在人工智能的推理期和知识期, 符号主义的方法比较盛行, 并取得了大量的成果．
/ 连接主义: 又称仿生学派或生理学派, 是认知科学领域中的一类信息处理的方法和理论．在认知科学领域, 人类的认知过程可以看作一种信息处理过程．连接主义认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程, 而不是符号运算．因此, 连接主义模型的主要结构是由大量简单的信息处理单元组成的互联网络, 具有非线性、分布式、并行化、局部性计算以及自适应性等特性．

此外, 在很多文献中都有划分出*行为主义*, 主要从生物进化的角度考虑, 主张从和外界环境的互动中获取智能．

== 从传统机器学习到深度学习

/ 机器学习: 从有限的观测数据中学习 (或“猜测”) 出具有一般性的规律, 并利用这些规律对未知数据进行预测的方法.

传统的机器学习主要关注如何学习一个预测模型．一般需要首先将数据表示为一组特征, 特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型, 并输出预测结果．这类机器学习可以看作*浅层学习*．浅层学习的一个重要特点是*不涉及特征学习, 其特征主要靠人工经验或特征转换方法来抽取*．

#figure(
  image("assets/01/ml_process_flow.svg"), 
  caption: [传统机器学习的数据处理流程]
)

/ 表示学习: 可以自动地学习出有效的特征, 并提高最终机器学习模型的性能的学习算法
/ 语义鸿沟: 指输入数据的底层特征和高层语义信息之间的不一致性和差异性. 

在表示学习中, 有两个核心问题: 一是*“什么是一个好的表示”*; 二是*“如何学习到好的表示”*．

在机器学习中, 我们经常使用两种方式来表示特征: 局部表示 (LocalRepresentation) 和分布式表示 (Distributed Representation) ．

/ 深度学习: 是将原始的数据特征通过多步的特征转换得到一种特征表示, 并进一步输入到预测函数得到最终结果. 通过学习算法来让模型自动学习出好的特征表示. 

#card[深度学习是机器学习的一个子问题, 其主要目的是从数据中自动学习到有效的特征表示． 也就是说深度学习一定是机器学习, 机器学习不一定是深度学习. ]

#figure(
  image("assets/01/dl_process_flow.svg"),
  caption: [深度学习的数据处理流程]
)

深度学习需要解决的关键问题是贡献度分配问题, 即一个系统中不同的组件 (component) 或其参数对最终系统输出结果的贡献或影响．

/ 端到端学习: 在学习过程中不进行分模块或分阶段训练, 直接优化任务的总体目标．

== 神经网络

/ 赫布规则: 当神经元A的一个轴突和神经元B很近, 足以对它产生影响, 并且持续地、重复地参与了对神经元B的兴奋, 那么在这两个神经元或其中之一会发生某种生长过程或新陈代谢变化, 以致神经元A作为能使神经元B兴奋的细胞之一, 它的效能加强了．

早期的神经网络模型并不具备学习能力．首个可学习的人工神经网络是赫布网络, 采用一种基于赫布规则的无监督学习方法．感知器是最早的具有机器学习思想的神经网络, 但其学习方法无法扩展到多层的神经网络上．
直到1980年左右, *反向传播算法*才有效地解决了多层神经网络的学习问题, 并成为最为流行的神经网络学习算法．

/ 网络容量: 一个人工神经网络塑造复杂函数的能力. 

#figure(
  table(
    columns: (auto, 1.5fr, 3fr),
    inset: 10pt,
    align: horizon,
    fill: (x, y) => if y == 0 { gray.lighten(50%) },
    stroke: 0.5pt + gray,
    
    [*阶段*], [*核心特征*], [*关键事件与成果*],
    
    [第一阶段\ (1943-1969)], 
    [*模型提出期*], 
    [McCuloch-Pitts (MP) 模型开启序幕；Rosenblatt 提出*感知器 (Perceptron)* 及其学习算法, 实现感知能力模拟的初步突破. ],

    [第二阶段\ (1969-1983)], 
    [*冰河期*], 
    [Minsky 证明感知器无法处理 *XOR (异或)* 问题；受限于计算机算力不足, 研究进入长达十余年的停滞. ],

    [第三阶段\ (1983-1995)], 
    [*复兴期*], 
    [*反向传播 (BP)* 算法流行, 解决多层网络学习难题；Hopfield 网络与卷积神经网络 (LeNet-5) 取得显著成功. ],

    [第四阶段\ (1995-2006)], 
    [*流行度降低期*], 
    [*支持向量机 (SVM)* 等统计学习理论兴起；神经网络因优化困难、解释性差及理论基础模糊再次陷入低潮. ],

    [第五阶段\ (2006-至今)], 
    [*深度学习崛起期*], 
    [Hinton 提出*逐层预训练*；伴随 *GPU* 并行计算与海量数据, 深度学习在语音及图像领域取得巨大成功, 迎来第三次高潮. ],
  ),
  caption: [神经网络的发展历程], 
)

== 考题与考点预测

=== 核心考点速记

#table(
  columns: (1fr, 3fr),
  inset: 5pt,
  align: horizon,
  [*考点模块*], [*核心内容与关键词*],
  
  [*AI基础与历史*],
  [- *图灵测试*: 判断机器是否智能的标准. 
  - *三大流派*: 
    1. *符号主义 (Symbolism)*: 逻辑演绎、显式规则, 可解释性强. 
    2. *连接主义 (Connectionism)*: 神经网络、仿生、分布式并行处理, 深度学习属于此流派. 
    3. *行为主义*: 进化与环境交互 (如强化学习). ],

  [*机器学习与深度学习*],
  [- *浅层学习 vs 深度学习*: 浅层学习依赖人工*特征工程*；深度学习通过多层结构自动进行*特征学习* (表示学习). 
  - *贡献度分配问题 (CAP)*: 深度学习的核心难点, 即如何确定系统中每个组件对最终结果的贡献. ],

  [*表示学习 (重难点)*],
  [- *语义鸿沟*: 底层特征 (如像素) 与高层语义之间的差异. 
  - *局部表示*: 如 One-hot 向量. 解释性好但维数高 (稀疏)、无法表示相似度. 
  - *分布式表示*: 如 词嵌入. 维数低 (稠密)、表示能力强、*可计算相似度*. ],

  [*神经网络发展史*],
  [- *五大阶段*: 模型提出 (MP模型, 感知器) $arrow.r$ 冰河期 (Minsky 指出 XOR 问题) $arrow.r$ 复兴 (反向传播 BP 算法) $arrow.r$ 低潮 $arrow.r$ 崛起 (深度信念网络). 
  - *赫布规则*: 两个神经元同时兴奋则连接增强. ]
)

=== 预测考题

==== 选择题

+ *(单选/多选) 关于人工智能流派, 下列说法正确的是：*
  - A. 深度学习属于符号主义流派
  - B. 符号主义主要通过逻辑运算操作符号, 具有较好的可解释性
  - C. 连接主义认为认知过程是神经元网络中的信息处理过程
  - D. 专家系统是连接主义的典型代表
  *解析：BC. A错误, 深度学习属于连接主义；D错误, 专家系统属于符号主义 (知识期). *

+ *(单选) 1969年, Marvin Minsky 出版《感知器》一书指出当时的神经网络 (感知器) 存在两个关键缺陷, 导致了神经网络研究进入“冰河期”. 这两个缺陷是：*
  - A. 无法解决异或 (XOR) 问题；计算机算力不足
  - B. 梯度消失问题；过拟合问题
  - C. 缺乏大量训练数据；缺乏有效的优化算法
  - D. 局部极小值问题；参数初始化困难
  *解析：A. *

+ *(单选) 在表示学习中, 关于“局部表示” (Local Representation) 和“分布式表示” (Distributed Representation) 的对比, 下列说法错误的是：*
  - A. One-hot 向量是典型的局部表示
  - B. 分布式表示通常对应低维稠密向量
  - C. 局部表示很容易计算两个概念之间的语义相似度
  - D. 分布式表示的表示能力通常强于局部表示
  *解析：C. 局部表示 (如One-hot) 中不同向量正交, 相似度为0, 无法体现语义相似性. *

+ *(多选) 深度学习区别于传统浅层机器学习的主要特征包括：*
  - A. 能够自动学习特征, 避免繁琐的人工特征工程
  - B. 通常采用“端到端”的学习方式
  - C. 模型结构通常具有多层非线性转换
  - D. 只能处理图像数据, 不能处理文本数据
  *解析：ABC. *

==== 简答题

+ *简述“语义鸿沟” (Semantic Gap) 的概念, 并说明深度学习如何解决这一问题. *
  - *参考答案*: 
    - *概念*: 语义鸿沟是指输入数据的底层特征 (如图像的像素值) 与高层语义信息 (如“这是一辆车”) 之间的不一致性和差异性. 
    - *解决*: 深度学习通过构建具有一定“深度”的多层神经网络, 将原始数据经过多步非线性特征转换, 从底层特征逐步抽象到中层、高层特征, 从而自动学习到能反映数据高层语义的表示. 

+ *什么是“贡献度分配问题” (Credit Assignment Problem, CAP)？为什么说它是深度学习的关键？*
  - *参考答案*: 
    - *定义*: 指在一个复杂的系统中 (如多层神经网络), 当我们得到最终的输出结果 (如分类正确或错误) 时, 并不清楚系统中每个内部组件 (或参数) 对这个结果的贡献或影响是多少. 
    - *关键性*: 深度学习模型通常包含大量参数和多层结构, 只有解决了CAP问题, 才能知道如何更新每个组件的参数以优化模型. 神经网络利用误差反向传播算法有效地解决了这一问题. 

+ *(高频考点) 简述神经网络发展的五个主要阶段及每个阶段的标志性事件. *
  - *参考答案*:
    1. *模型提出 (1943-1969)*: McCulloch & Pitts 提出 MP 模型；Rosenblatt 提出感知器. 
    2. *冰河期 (1969-1983)*: Minsky 出版《感知器》指出无法解决 XOR 问题. 
    3. *反向传播复兴 (1983-1995)*: Hinton 等人完善反向传播 (BP) 算法；LeCun 提出卷积神经网络. 
    4. *流行度降低 (1995-2006)*: 支持向量机 (SVM) 等统计学习方法兴起, 神经网络因难以训练和解释性差受冷落. 
    5. *深度学习崛起 (2006-至今)*: Hinton 提出深度信念网络 (预训练+精调)；GPU 算力提升与大数据爆发. 