= 卷积神经网络

在全连接网络FCN处理图像数据时, 主要面临*参数太大与局部不变特性*的问题．图像需要展平为一维向量, 然后输入到全连接层, 这样做会丢失图像的空间结构信息, 导致参数过多, 计算复杂度太高．同时图像的平移旋转往往不影响图像的语义, 但全连接层无法共享权重, 导致参数量过大．

/ 感受野: 主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支配的刺激区域内的信号．
/ 卷积神经网络: ConvolutionalNeuralNetwork，CNN, 是一种具有*局部连接、权重共享*等特性的深层前馈神经网络．卷积神经网络有三个结构上的特性：*局部连接、权重共享以及汇聚* (池化)．

== 卷积

/ 卷积: 卷积核在目标信号序列上移动, 并计算局部区域的乘积和. 
/ 步长: 卷积核在目标信号序列上移动的步长.
/ 零填充: 在目标信号序列的边缘添加零, 以确保卷积核在目标信号序列上移动时不会越界.

假设输入数量为$M$, 卷积核大小为$K$, 零填充大小为$P$, 步长为$S$, 则输出数量为$[(M+2P-K) / S] +1$

通常按照输出长度可以有三种变种: 
/ 窄卷积: 步长为1, 零填充为0, 输出长度为$M - K + 1$
/ 宽卷积: 步长为1, 零填充为$K-1$, 输出长度为$M + K - 1$
/ 等宽卷积: 步长为1, 零填充为$(K-1)/2$, 输出长度为$M$
一般而言, 现在的文献都默认为等宽卷积. 

/ 二维卷积: 给定图像$X in RR^(M times N)$, 卷积核$W in RR^(U times V)$, 对应卷积为$ y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i - u + 1, j - v + 1) $
注意有一个卷积核旋转180度的操作. 

/ 互相关: $y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i + u + 1, j + v + 1)$, 与卷积不同的是, 互相关没有旋转180度. 从而在深度学习中减少了运算量. 

在神经网络中使用卷积是为了进行特征抽取，*卷积核是否进行翻转和其特征抽取的能力无关*．特别是当卷积核是可学习的参数时，卷积和互相关在能力上是等价的．因此，为了实现上（或描述上）的方便起见，我们用互相关来代替卷积．

卷积的数学性质主要体现在交换性与导数性质上: 
$ x * y = y * x $
$ Y = W times.o X => (partial f(Y)) / (partial W) = X times.o (partial f(Y)) / (partial Y) $
注: $tilde(times.o)$表示宽卷积. 

== 卷积神经网络

用卷积层代替全连接层, 则公式变化为: $ z^((l)) = w^((l)) times.o a^((l - 1)) + b^((l)) $
其中$w^((l)) in RR^k, b^((l)) in RR$, 为可学习的权重与偏置. 

回顾一下卷积神经网络的两大特征: 
/ 局部连接: 每个神经元只与输入数据的一个局部区域连接, 而不是与所有的数据连接. (依据卷积核的权重相连)
/ 权重共享: 同一个卷积核在图像的不同位置共享参数. 同一层共享同样的卷积核. 

在处理图像时, 图像的输入可能有不同的通道数, 例如RGB图像有三个通道数, 我们将输入图像$X in RR^(M times N times D)$, 则这里的$D$表示通道数. 此外, 一组卷积核共享权重, 只能提取到一种特征, 为了提取到多种不同层次的特征, 我们需要使用多组卷积核. 从而卷积核的尺寸 $W in RR^(U times V times P times D)$. 其中$U times V$为二维卷积的尺寸, $P$为卷积核的通道数(也就是有几组卷积, 也就是提取几组特征), $D$为输入图像的通道数. \
这样一层卷积层的参数量为$U times V times P times D + P$, 其中$P$为偏置的个数.

/ 汇聚层: 也叫池化层, 子采样层，其作用是进行特征选择，降低特征数量，从而减少参数数量．

常见的池化方式有最大池化 (选取区域内的最大值), 平均池化 (计算区域的平均值). \
目前主流的卷积网络中，汇聚层仅包含*下采样*操作．但在早期的一些卷积网络（比如LeNet-5）中，有时也会在汇聚层使用非线性激活函数. 

一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成．目前常用的卷积网络整体结构如@common-conv-architecture 一个卷积块为连续𝑀个卷积层和𝑏个汇聚层（𝑀通常设置为2∼5，𝑏为0或1）．一个卷积网络中可以堆叠𝑁个连续的卷积块，然后在后面接着𝐾个全连接层（𝑁的取值区间比较大，比如1∼100或者更大；𝐾一般为0∼2）

#figure(
  image("assets/03/common-conv-architecture.svg"), 
  caption: [常用的卷积神经网络的整体结构]
)<common-conv-architecture>

== CNN 的反向传播算法

=== 池化层

由于池化层做的是*下采样*操作, 因此反向传播时对误差项进行上采样再传递即可.
$ delta^((l, p)) = f_l '(Z^((l, p))) times.o "up"(delta^((l + 1, p))) $
其中$ "up"(delta^((l + 1, p)))$为上采样操作.

对于最大值池化, 误差项每个值直接传递给前面一层每个区域的最大值位置即可; 对于平均池化, 误差项每个值平均传递给前面一层每个区域的所有位置即可. 

=== 卷积层

$
  delta^((l, d)) & = (partial cal(L)) / (partial Z^((l, d))) = (partial X^((l, d))) / (partial Z^((l, d))) dot (partial cal(L)) / (partial X^((l, d))) \
  & = f_l '(Z^((l, d))) dot.o sum_(p = 1)^P ("rot180"(W^((l + 1, p, d))) tilde(times.o) delta^((l + 1, p))) 
$

== 其他类型的卷积

