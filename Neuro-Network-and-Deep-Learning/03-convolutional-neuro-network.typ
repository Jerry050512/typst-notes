= 卷积神经网络

在全连接网络FCN处理图像数据时, 主要面临*参数太大与局部不变特性*的问题．图像需要展平为一维向量, 然后输入到全连接层, 这样做会丢失图像的空间结构信息, 导致参数过多, 计算复杂度太高．同时图像的平移旋转往往不影响图像的语义, 但全连接层无法共享权重, 导致参数量过大．

/ 感受野: 主要是指听觉、视觉等神经系统中一些神经元的特性, 即神经元只接受其所支配的刺激区域内的信号．
/ 卷积神经网络: ConvolutionalNeuralNetwork, CNN, 是一种具有*局部连接、权重共享*等特性的深层前馈神经网络．卷积神经网络有三个结构上的特性: *局部连接、权重共享以及汇聚* (池化)．

== 卷积

/ 卷积: 卷积核在目标信号序列上移动, 并计算局部区域的乘积和. 
/ 步长: 卷积核在目标信号序列上移动的步长.
/ 零填充: 在目标信号序列的边缘添加零, 以确保卷积核在目标信号序列上移动时不会越界.

假设输入数量为$M$, 卷积核大小为$K$, 零填充大小为$P$, 步长为$S$, 则输出数量为$[(M+2P-K) / S] +1$

通常按照输出长度可以有三种变种: 
/ 窄卷积: 步长为1, 零填充为0, 输出长度为$M - K + 1$
/ 宽卷积: 步长为1, 零填充为$K-1$, 输出长度为$M + K - 1$
/ 等宽卷积: 步长为1, 零填充为$(K-1)/2$, 输出长度为$M$
一般而言, 现在的文献都默认为等宽卷积. 

/ 二维卷积: 给定图像$X in RR^(M times N)$, 卷积核$W in RR^(U times V)$, 对应卷积为$ y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i - u + 1, j - v + 1) $
注意有一个卷积核旋转180度的操作. 

/ 互相关: $y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i + u + 1, j + v + 1)$, 与卷积不同的是, 互相关没有旋转180度. 从而在深度学习中减少了运算量. 

在神经网络中使用卷积是为了进行特征抽取, *卷积核是否进行翻转和其特征抽取的能力无关*．特别是当卷积核是可学习的参数时, 卷积和互相关在能力上是等价的．因此, 为了实现上 (或描述上) 的方便起见, 我们用互相关来代替卷积．

卷积的数学性质主要体现在交换性与导数性质上: 
$ x * y = y * x $
$ Y = W times.o X => (partial f(Y)) / (partial W) = X times.o (partial f(Y)) / (partial Y) $
注: $tilde(times.o)$表示宽卷积. 

== 卷积神经网络

用卷积层代替全连接层, 则公式变化为: $ z^((l)) = w^((l)) times.o a^((l - 1)) + b^((l)) $
其中$w^((l)) in RR^k, b^((l)) in RR$, 为可学习的权重与偏置. 

回顾一下卷积神经网络的两大特征: 
/ 局部连接: 每个神经元只与输入数据的一个局部区域连接, 而不是与所有的数据连接. (依据卷积核的权重相连)
/ 权重共享: 同一个卷积核在图像的不同位置共享参数. 同一层共享同样的卷积核. 

在处理图像时, 图像的输入可能有不同的通道数, 例如RGB图像有三个通道数, 我们将输入图像$X in RR^(M times N times D)$, 则这里的$D$表示通道数. 此外, 一组卷积核共享权重, 只能提取到一种特征, 为了提取到多种不同层次的特征, 我们需要使用多组卷积核. 从而卷积核的尺寸 $W in RR^(U times V times P times D)$. 其中$U times V$为二维卷积的尺寸, $P$为卷积核的通道数(也就是有几组卷积, 也就是提取几组特征), $D$为输入图像的通道数. \
这样一层卷积层的参数量为$U times V times P times D + P$, 其中$P$为偏置的个数.

/ 汇聚层: 也叫池化层, 子采样层, 其作用是进行特征选择, 降低特征数量, 从而减少参数数量．

常见的池化方式有最大池化 (选取区域内的最大值), 平均池化 (计算区域的平均值). \
目前主流的卷积网络中, 汇聚层仅包含*下采样*操作．但在早期的一些卷积网络 (比如LeNet-5) 中, 有时也会在汇聚层使用非线性激活函数. 

一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成．目前常用的卷积网络整体结构如图 一个卷积块为连续𝑀个卷积层和𝑏个汇聚层 (𝑀通常设置为2∼5, 𝑏为0或1) ．一个卷积网络中可以堆叠𝑁个连续的卷积块, 然后在后面接着𝐾个全连接层 (𝑁的取值区间比较大, 比如1∼100或者更大; 𝐾一般为0∼2) 

#figure(
  image("assets/03/common-conv-architecture.svg"), 
  caption: [常用的卷积神经网络的整体结构]
)

== CNN 的反向传播算法

=== 池化层

由于池化层做的是*下采样*操作, 因此反向传播时对误差项进行上采样再传递即可.
$ delta^((l, p)) = f_l '(Z^((l, p))) times.o "up"(delta^((l + 1, p))) $
其中$ "up"(delta^((l + 1, p)))$为上采样操作.

对于最大值池化, 误差项每个值直接传递给前面一层每个区域的最大值位置即可; 对于平均池化, 误差项每个值平均传递给前面一层每个区域的所有位置即可. 

=== 卷积层

$
  delta^((l, d)) & = (partial cal(L)) / (partial Z^((l, d))) = (partial X^((l, d))) / (partial Z^((l, d))) dot (partial cal(L)) / (partial X^((l, d))) \
  & = f_l '(Z^((l, d))) dot.o sum_(p = 1)^P ("rot180"(W^((l + 1, p, d))) tilde(times.o) delta^((l + 1, p))) 
$

== 其他类型的卷积

/ 转置卷积: 将低维特征映射到高维特征的卷积操作. 也称反卷积. 
/ 微步卷积: 步长小于1的转置卷积. 
/ 空洞卷积: 再卷积核之间插入空洞(0), 从而扩大感受野.例如3x3的卷积两个元素之间插入一个空洞就可以变成5x5的卷积.

卷积操作也可以写成仿射变换的形式, 例如大小为三的卷积核, 对5维向量卷积: 

$
  z & = w times.o x \
  & = mat(
    w_1, w_2, w_3, 0, 0;
    0, w_1, w_2, w_3, 0;
    0, 0, w_1, w_2, w_3;
    delim: "["
  ) dot x
$

== 典型的卷积神经网络

=== LeNet-5
LeNet-5 是由 Yann LeCun 等人在 1998 年提出的, 主要用于手写体数字识别, 是早期卷积神经网络的代表. 
- *结构特点*: 由卷积层、汇聚层 (池化层) 和全连接层交替构成. 
- *意义*: 确立了卷积神经网络的基本架构, 即“卷积 + 汇聚 + 全连接”的模式. 

=== AlexNet
AlexNet 在 2012 年的 ImageNet 挑战赛中夺冠, 标志着深度学习热潮的开始. 
- *主要改进*: 
  - 使用了 *ReLU* 激活函数, 解决了深层网络中的梯度消失问题. 
  - 引入了 *Dropout* 方法来减轻过拟合. 
  - 采用了 *数据增强* 技术来扩大训练集. 
  - 使用了 *GPU* 进行加速计算. 

=== VGGNet
VGGNet (如 VGG-16) 由牛津大学视觉几何组提出, 其核心思想是使用重复的小卷积核结构. 
- *设计原则*: 通过堆叠多个 $3 times 3$ 的小型卷积核来代替大型卷积核. 
- *优势*: 在保持感受野相同的情况下, 增加了网络的深度和非线性表达能力. 

=== Inception 网络 (GoogLeNet)
Inception 网络通过增加网络的“宽度”来提升性能, 而不是单纯增加深度. 
- *Inception 模块*: 在一个层级中并行使用不同取样窗口大小的卷积 (如 $1 times 1, 3 times 3, 5 times 5$) 和汇聚操作. 
- *1x1 卷积*: 大量使用 $1 times 1$ 卷积来进行降维, 显著减少了参数量和计算开销. 

=== 残差网络 (ResNet)
ResNet 是深度学习领域的一个重要里程碑, 解决了深层网络难以训练的问题. 
- *残差学习*: 引入了“捷径连接” (Shortcut Connection), 使网络学习的是残差映射 $f(x) = h(x) - x$, 而非原始映射 $h(x)$. 
- *突破*: 这种设计极大地缓解了深层网络的梯度消失问题, 使得训练上百甚至上千层的网络成为可能. 

== 考题与考点预测

=== 核心考点预测

本章重点在于卷积的基本运算规则、参数计算以及卷积神经网络的基本结构特性. 

#table(
  columns: (1fr, 3fr),
  align: (center, left),
  [*考点模块*], [*核心内容与公式*],
  [*卷积运算基础*], [
    - *卷积 vs 互相关*: 深度学习中的“卷积”通常指互相关(不翻转卷积核). 真正的卷积需要旋转卷积核180度 (rot180). 
    - *输出尺寸计算*: 输入大小 $M$, 卷积核 $K$, 填充 $P$, 步长 $S$. 
      $ M' = floor((M - K + 2 P) / S) + 1 $
    - *三种卷积类型*:
      - 窄卷积 ($S=1, P=0$): 输出变小 ($M - K + 1$)
      - 宽卷积 ($S=1, P=K-1$): 输出变大 ($M + K - 1$)
      - 等宽卷积 ($S=1, P=(K-1)/2$): 输出不变 ($M$)
  ],
  [*CNN结构特性*], [
    - *三大特性*: 局部连接、权重共享、汇聚 (Pooling). 
    - *参数量计算*: 假设输入通道 $D$, 输出通道 $P$, 卷积核 $U times V$. 
      $ "Params" = (U times V times D) times P + P ("bias") $
    - *连接数计算*: 取决于输出特征图的大小. 
      $ "Connections" = M' times N' times P times (U times V times D + 1) $
  ],
  [*汇聚层*], [
    - *作用*: 特征选择, 降低特征数量, 减少参数, 防止过拟合. 
    - *类型*: 最大汇聚 (Max Pooling) 与 平均汇聚 (Mean Pooling). 
    - *性质*: 增强对微小变换(平移、旋转)的不变性. 
  ]
)

=== 考题预测

==== 选择题

1. 在卷积神经网络中, 假设输入特征图大小为 $32 times 32$, 使用 $5 times 5$ 的卷积核, 步长(Stride)设为 1, 零填充(Padding)设为 2, 则输出特征图的大小为：
  - A. $28 times 28$
  - B. $30 times 30$
  - C. $32 times 32$
  - D. $34 times 34$

2. 相比于全连接前馈神经网络, 卷积神经网络通过以下哪些特性有效减少了参数数量？(多选)
  - A. 局部连接 (Local Connectivity)
  - B. 权重共享 (Weight Sharing)
  - C. 零填充 (Zero Padding)
  - D. 随机失活 (Dropout)

3. 关于卷积(Convolution)与互相关(Cross-Correlation)的关系, 下列说法正确的是：
  - A. 深度学习框架中实现的“卷积”操作在数学定义上通常实际上是互相关
  - B. 互相关运算需要对卷积核进行 180 度旋转
  - C. 卷积运算具有交换性, 而互相关绝对不具有交换性
  - D. 在图像处理中, 如果不进行翻转, 无法提取边缘特征

4. 假设某卷积层的输入通道数为 3 (RGB图像), 要求输出通道数为 64, 卷积核大小为 $3 times 3$. 若包含偏置项, 该层需要学习的参数总量是多少？
  - A. $3 times 3 times 64 + 64 = 640$
  - B. $3 times 3 times 3 times 64 = 1728$
  - C. $3 times 3 times 3 times 64 + 64 = 1792$
  - D. $3 times 3 times 64 = 576$

5. 下列关于池化层(Pooling Layer)的描述, 错误的是：
  - A. 池化层可以减少特征图的维度, 降低计算量
  - B. 最大池化(Max Pooling)选取区域内的最大值作为输出, 能提取显著特征
  - C. 池化层通常包含大量的可训练参数(权重)
  - D. 池化层有助于使网络对输入图像的微小位移保持不变性

==== 简答题

1. *(核心概念)* 请简述卷积神经网络中“局部连接”和“权重共享”的含义, 并说明它们如何帮助处理图像数据. 
  *参考答案*:
  - *局部连接*: 神经元不再与上一层的所有神经元相连, 而只与一个局部区域(感受野)内的神经元相连. 这符合图像的空间局部相关性(相邻像素联系紧密). 
  - *权重共享*: 同一个卷积核(滤波器)在图像的所有位置上滑动时, 其权重参数是固定的. 这意味着网络在图像不同位置使用相同的特征提取器. 
  - *作用*: 这两个机制大大减少了网络的参数数量, 提高了训练效率, 并赋予了网络平移不变性的基础. 

2. *(数学性质)* 在教材 5.1 节中提到了“宽卷积”、“窄卷积”和“等宽卷积”. 请分别解释这三种卷积在步长 $S=1$ 时的定义(基于填充 $P$ 和核大小 $K$ 的关系)及输出尺寸的变化趋势. 
  *参考答案*:
  - *窄卷积*: $P=0$, 输出尺寸为 $M - K + 1$, 特征图逐渐变小. 
  - *宽卷积*: $P=K-1$, 输出尺寸为 $M + K - 1$, 特征图变大. 
  - *等宽卷积*: $P=(K-1)/2$, 输出尺寸为 $M$, 特征图大小保持不变(常用于深层网络以维持空间维度). 

3. *(结构分析)* 为什么现在的深层卷积网络(如 ResNet, Inception) 倾向于使用较小的卷积核(如 $1 times 1$ 和 $3 times 3$), 而不是大的卷积核(如 $5 times 5$ 或 $7 times 7$)？
  *参考答案*:
  - *减少参数量*: 堆叠两个 $3 times 3$ 卷积层的感受野与一个 $5 times 5$ 相同, 但参数量更少 ($2 times 3^2 = 18$ vs $5^2 = 25$). 
  - *增加非线性*: 多层小卷积核之间会加入激活函数(如 ReLU), 增加了网络的非线性表达能力, 使网络能学习更复杂的特征. 

==== 计算题

1. *(卷积数值计算)*
  给定输入图像 $X$ 和卷积核 $W$, 步长 $S=1$, 无零填充 ($P=0$). 请计算互相关(Cross-correlation)操作后的输出特征图 $Y$. 
  $ X = mat(
    1, 1, 1, 0, 0;
    0, 1, 1, 1, 0;
    0, 0, 1, 1, 1;
    0, 0, 1, 1, 0;
    0, 1, 1, 0, 0
  ), quad W = mat(
    1, 0, 1;
    0, 1, 0;
    1, 0, 1
  ) $

  *解题思路*:
  1.  *确定输出尺寸*: 输入 $M=5$, 核 $K=3$, 步长 $S=1$, 填充 $P=0$. 
      $ M' = (5 - 3 + 0)/1 + 1 = 3 $. 输出为 $3 times 3$ 矩阵. 
  2.  *滑动窗口计算*:
      - 左上角 $(0,0)$ 区域:
        $ 1(1) + 1(0) + 1(1) + 0(0) + 1(1) + 1(0) + 0(1) + 0(0) + 1(1) = 1+0+1+0+1+0+0+0+1 = 4 $
      - 依此类推计算其余 8 个位置. 
      - 例如中心位置 $X_(2,2)$ 对应的窗口是:
        $ mat(1,1,1; 1,1,1; 1,1,0) dot W = 1+1+1+1+1 = 5 $ (注：此处需严格对应位置相乘求和). 
  *(注：考试时请写出至少两个关键位置的详细计算过程)*

2. *(参数量与连接数)*
  考虑一个卷积层, 输入特征图大小为 $28 times 28 times 6$ (宽$times$高$times$深度). 
  该层使用 10 个 $5 times 5$ 的卷积核, 步长 $S=1$, 无填充 $P=0$. 
  请计算：
  1.  输出特征图的大小 (Width $times$ Height $times$ Depth). 
  2.  该层的可训练参数总数 (Parameters). 
  3.  该层的连接总数 (Connections). 

  *解题步骤*:
  1.  *输出大小*:
      - 宽/高: $(28 - 5 + 0)/1 + 1 = 24$. 
      - 深度: 等于卷积核个数, 即 10. 
      - 结果: $24 times 24 times 10$. 
  2.  *参数总数*:
      - 每个卷积核参数: $5 times 5 times 6 ("输入深度") = 150$. 
      - 加上偏置: $150 + 1 = 151$. 
      - 总参数: $151 times 10 ("核个数") = 1510$. 
  3.  *连接总数*:
      - 每一个输出神经元都与输入的一个 $5 times 5 times 6$ 区域相连, 并加上一个偏置连接. 
      - 输出神经元总数: $24 times 24 times 10 = 5760$. 
      - 每个神经元的连接数: $5 times 5 times 6 + 1 = 151$. 
      - 总连接数: $5760 times 151 = 869760$. 