= 卷积神经网络

在全连接网络FCN处理图像数据时, 主要面临*参数太大与局部不变特性*的问题．图像需要展平为一维向量, 然后输入到全连接层, 这样做会丢失图像的空间结构信息, 导致参数过多, 计算复杂度太高．同时图像的平移旋转往往不影响图像的语义, 但全连接层无法共享权重, 导致参数量过大．

/ 感受野: 主要是指听觉、视觉等神经系统中一些神经元的特性, 即神经元只接受其所支配的刺激区域内的信号．
/ 卷积神经网络: ConvolutionalNeuralNetwork, CNN, 是一种具有*局部连接、权重共享*等特性的深层前馈神经网络．卷积神经网络有三个结构上的特性: *局部连接、权重共享以及汇聚* (池化)．

== 卷积

/ 卷积: 卷积核在目标信号序列上移动, 并计算局部区域的乘积和. 
/ 步长: 卷积核在目标信号序列上移动的步长.
/ 零填充: 在目标信号序列的边缘添加零, 以确保卷积核在目标信号序列上移动时不会越界.

假设输入数量为$M$, 卷积核大小为$K$, 零填充大小为$P$, 步长为$S$, 则输出数量为$[(M+2P-K) / S] +1$

通常按照输出长度可以有三种变种: 
/ 窄卷积: 步长为1, 零填充为0, 输出长度为$M - K + 1$
/ 宽卷积: 步长为1, 零填充为$K-1$, 输出长度为$M + K - 1$
/ 等宽卷积: 步长为1, 零填充为$(K-1)/2$, 输出长度为$M$
一般而言, 现在的文献都默认为等宽卷积. 

/ 二维卷积: 给定图像$X in RR^(M times N)$, 卷积核$W in RR^(U times V)$, 对应卷积为$ y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i - u + 1, j - v + 1) $
注意有一个卷积核旋转180度的操作. 

/ 互相关: $y_(i j) = sum_(u = 1)^U sum_(v = 1)^V W_(u v) X_(i + u + 1, j + v + 1)$, 与卷积不同的是, 互相关没有旋转180度. 从而在深度学习中减少了运算量. 

在神经网络中使用卷积是为了进行特征抽取, *卷积核是否进行翻转和其特征抽取的能力无关*．特别是当卷积核是可学习的参数时, 卷积和互相关在能力上是等价的．因此, 为了实现上 (或描述上) 的方便起见, 我们用互相关来代替卷积．

卷积的数学性质主要体现在交换性与导数性质上: 
$ x * y = y * x $
$ Y = W times.o X => (partial f(Y)) / (partial W) = X times.o (partial f(Y)) / (partial Y) $
注: $tilde(times.o)$表示宽卷积. 

== 卷积神经网络

用卷积层代替全连接层, 则公式变化为: $ z^((l)) = w^((l)) times.o a^((l - 1)) + b^((l)) $
其中$w^((l)) in RR^k, b^((l)) in RR$, 为可学习的权重与偏置. 

回顾一下卷积神经网络的两大特征: 
/ 局部连接: 每个神经元只与输入数据的一个局部区域连接, 而不是与所有的数据连接. (依据卷积核的权重相连)
/ 权重共享: 同一个卷积核在图像的不同位置共享参数. 同一层共享同样的卷积核. 

在处理图像时, 图像的输入可能有不同的通道数, 例如RGB图像有三个通道数, 我们将输入图像$X in RR^(M times N times D)$, 则这里的$D$表示通道数. 此外, 一组卷积核共享权重, 只能提取到一种特征, 为了提取到多种不同层次的特征, 我们需要使用多组卷积核. 从而卷积核的尺寸 $W in RR^(U times V times P times D)$. 其中$U times V$为二维卷积的尺寸, $P$为卷积核的通道数(也就是有几组卷积, 也就是提取几组特征), $D$为输入图像的通道数. \
这样一层卷积层的参数量为$U times V times P times D + P$, 其中$P$为偏置的个数.

/ 汇聚层: 也叫池化层, 子采样层, 其作用是进行特征选择, 降低特征数量, 从而减少参数数量．

常见的池化方式有最大池化 (选取区域内的最大值), 平均池化 (计算区域的平均值). \
目前主流的卷积网络中, 汇聚层仅包含*下采样*操作．但在早期的一些卷积网络 (比如LeNet-5) 中, 有时也会在汇聚层使用非线性激活函数. 

一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成．目前常用的卷积网络整体结构如图 一个卷积块为连续𝑀个卷积层和𝑏个汇聚层 (𝑀通常设置为2∼5, 𝑏为0或1) ．一个卷积网络中可以堆叠𝑁个连续的卷积块, 然后在后面接着𝐾个全连接层 (𝑁的取值区间比较大, 比如1∼100或者更大; 𝐾一般为0∼2) 

#figure(
  image("assets/03/common-conv-architecture.svg"), 
  caption: [常用的卷积神经网络的整体结构]
)

== CNN 的反向传播算法

=== 池化层

由于池化层做的是*下采样*操作, 因此反向传播时对误差项进行上采样再传递即可.
$ delta^((l, p)) = f_l '(Z^((l, p))) times.o "up"(delta^((l + 1, p))) $
其中$ "up"(delta^((l + 1, p)))$为上采样操作.

对于最大值池化, 误差项每个值直接传递给前面一层每个区域的最大值位置即可; 对于平均池化, 误差项每个值平均传递给前面一层每个区域的所有位置即可. 

=== 卷积层

$
  delta^((l, d)) & = (partial cal(L)) / (partial Z^((l, d))) = (partial X^((l, d))) / (partial Z^((l, d))) dot (partial cal(L)) / (partial X^((l, d))) \
  & = f_l '(Z^((l, d))) dot.o sum_(p = 1)^P ("rot180"(W^((l + 1, p, d))) tilde(times.o) delta^((l + 1, p))) 
$

== 其他类型的卷积

/ 转置卷积: 将低维特征映射到高维特征的卷积操作. 也称反卷积. 
/ 微步卷积: 步长小于1的转置卷积. 
/ 空洞卷积: 再卷积核之间插入空洞(0), 从而扩大感受野.例如3x3的卷积两个元素之间插入一个空洞就可以变成5x5的卷积.

卷积操作也可以写成仿射变换的形式, 例如大小为三的卷积核, 对5维向量卷积: 

$
  z & = w times.o x \
  & = mat(
    w_1, w_2, w_3, 0, 0;
    0, w_1, w_2, w_3, 0;
    0, 0, w_1, w_2, w_3;
    delim: "["
  ) dot x
$

== 典型的卷积神经网络

=== LeNet-5
LeNet-5 是由 Yann LeCun 等人在 1998 年提出的, 主要用于手写体数字识别, 是早期卷积神经网络的代表. 
- *结构特点*: 由卷积层、汇聚层 (池化层) 和全连接层交替构成. 
- *意义*: 确立了卷积神经网络的基本架构, 即“卷积 + 汇聚 + 全连接”的模式. 

=== AlexNet
AlexNet 在 2012 年的 ImageNet 挑战赛中夺冠, 标志着深度学习热潮的开始. 
- *主要改进*: 
  - 使用了 *ReLU* 激活函数, 解决了深层网络中的梯度消失问题. 
  - 引入了 *Dropout* 方法来减轻过拟合. 
  - 采用了 *数据增强* 技术来扩大训练集. 
  - 使用了 *GPU* 进行加速计算. 

=== VGGNet
VGGNet (如 VGG-16) 由牛津大学视觉几何组提出, 其核心思想是使用重复的小卷积核结构. 
- *设计原则*: 通过堆叠多个 $3 times 3$ 的小型卷积核来代替大型卷积核. 
- *优势*: 在保持感受野相同的情况下, 增加了网络的深度和非线性表达能力. 

=== Inception 网络 (GoogLeNet)
Inception 网络通过增加网络的“宽度”来提升性能, 而不是单纯增加深度. 
- *Inception 模块*: 在一个层级中并行使用不同取样窗口大小的卷积 (如 $1 times 1, 3 times 3, 5 times 5$) 和汇聚操作. 
- *1x1 卷积*: 大量使用 $1 times 1$ 卷积来进行降维, 显著减少了参数量和计算开销. 

=== 残差网络 (ResNet)
ResNet 是深度学习领域的一个重要里程碑, 解决了深层网络难以训练的问题. 
- *残差学习*: 引入了“捷径连接” (Shortcut Connection), 使网络学习的是残差映射 $f(x) = h(x) - x$, 而非原始映射 $h(x)$. 
- *突破*: 这种设计极大地缓解了深层网络的梯度消失问题, 使得训练上百甚至上千层的网络成为可能. 