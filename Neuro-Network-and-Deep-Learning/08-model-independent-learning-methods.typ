= 模型独立的学习方式

== 核心概念

/ 集成学习 (Ensemble Learning): 了解如何通过“集体智慧”组合多个模型, 以获得超越任何单一模型的性能. 
/ 半监督学习 (Semi-Supervised Learning): 探索在仅有少量标注数据和海量未标注数据的情况下, 如何有效训练模型. 
/ 迁移学习 (Transfer Learning): 学习如何将一个任务上获得的知识“迁移”并应用于另一个相关任务, 特别是在目标任务数据不足时. 
/ 终身学习 (Lifelong Learning): 探讨如何让模型像人一样持续学习新技能, 同时不遗忘已掌握的旧知识. 
/ 元学习 (Meta-Learning): 深入理解“学习如何学习”的前沿理念, 使模型能够快速适应全新的、未曾见过的任务. 

模型独立的学习方式 (Model-Independent Learning) 是指不依赖于具体模型结构的通用学习范式, 重点在于如何组织数据、任务以及多个模型之间的关系. 

=== 集成学习 (Ensemble Learning)

集成学习通过构建并结合多个个体学习器 (基学习器) 来完成学习任务. 

- *核心思想*: 通过组合多个“弱学习器”来构造成一个“强学习器”. 其泛化能力往往显著优于单个学习器. 
- *分类*: 
  - *Bagging*: 并行式. 通过自助采样 (Bootstrap Sampling) 训练多个独立模型, 最后投票或平均. 代表模型: 随机森林. 
  - *Boosting*: 串行式. 每个新模型都尝试纠正前一个模型的错误. 
- *AdaBoost 算法*: 
  - 核心逻辑: 每一轮迭代中提高被前一轮弱分类器错误分类样本的权重. 
  - 最终分类器是弱分类器的加权线性组合, 权重取决于该分类器的分类准确率. 

=== 自训练与协同训练 (Self-training & Co-training)

这两种方法主要用于*半监督学习*, 利用少量有标签数据和大量无标签数据. 

- *自训练 (Self-training)*: 模型先在有标签数据上训练, 然后对无标签数据进行预测, 将预测置信度高的样本及其伪标签 (Pseudo Label) 加入训练集, 循环往复. 
- *协同训练 (Co-training)*: 假设数据有两个充分且条件的独立视图. 在每个视图上训练一个模型, 互相为对方提供高置信度的伪标签样本. 

=== 多任务学习 (Multi-task Learning)

多任务学习利用多个相关任务之间的领域特定信息来提升每个任务的泛化能力. 

- *核心概念*: 多个任务并行学习, 并共享一部分网络表示 (如共享前几层特征提取层) . 
- *作用*: 通过底层特征共享, 起到隐式的正则化作用, 减少过拟合风险. 

=== 迁移学习 (Transfer Learning)

迁移学习研究如何将从一个源领域 (Source Domain) 学到的知识应用到目标领域 (Target Domain) . 

- *归纳迁移学习*: 源领域和目标领域任务不同 (如先学识别猫, 再迁移到识别虎) . 
- *转导迁移学习*: 任务相同但数据分布不同 (如用摄影图片模型去识别素描图) . 
- *常见手段*: 微调 (Fine-tuning), 即利用在大规模数据集上预训练好的参数作为初始值. 

=== 终身学习 (Lifelong Learning)

又称增量学习或持续学习, 模拟人类不断学习新知识而不遗忘旧知识的能力. 

- *核心挑战*: *灾难性遗忘 (Catastrophic Forgetting)*. 即学习新任务时, 原本在旧任务上学到的权重被覆盖, 导致旧任务性能剧降. 

=== 元学习 (Meta-Learning)

元学习即“学会学习” (Learning to Learn), 目标是让模型具备从少量经验中快速适应新任务的能力. 

- *基于优化器的元学习*: 学习一个通用的优化规则 (如替代 SGD 的更新公式) . 
- *模型无关元学习 (MAML)*: 寻找一组对各种任务都高度敏感的初始化参数. 通过极少量的梯度更新, 模型就能在新的小样本任务上达到最佳性能. 

== 核心术语与考点预测

