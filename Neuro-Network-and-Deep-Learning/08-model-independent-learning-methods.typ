= 模型独立的学习方式

== 核心概念

/ 集成学习 (Ensemble Learning): 了解如何通过“集体智慧”组合多个模型, 以获得超越任何单一模型的性能. 
/ 半监督学习 (Semi-Supervised Learning): 探索在仅有少量标注数据和海量未标注数据的情况下, 如何有效训练模型. 
/ 迁移学习 (Transfer Learning): 学习如何将一个任务上获得的知识“迁移”并应用于另一个相关任务, 特别是在目标任务数据不足时. 
/ 终身学习 (Lifelong Learning): 探讨如何让模型像人一样持续学习新技能, 同时不遗忘已掌握的旧知识. 
/ 元学习 (Meta-Learning): 深入理解“学习如何学习”的前沿理念, 使模型能够快速适应全新的、未曾见过的任务. 

模型独立的学习方式 (Model-Independent Learning) 是指不依赖于具体模型结构的通用学习范式, 重点在于如何组织数据、任务以及多个模型之间的关系. 

=== 集成学习 (Ensemble Learning)

集成学习通过构建并结合多个个体学习器 (基学习器) 来完成学习任务. 

- *核心思想*: 通过组合多个“弱学习器”来构造成一个“强学习器”. 其泛化能力往往显著优于单个学习器. 
- *分类*: 
  - *Bagging*: 并行式. 通过自助采样 (Bootstrap Sampling) 训练多个独立模型, 最后投票或平均. 代表模型: 随机森林. 
  - *Boosting*: 串行式. 每个新模型都尝试纠正前一个模型的错误. 
- *AdaBoost 算法*: 
  - 核心逻辑: 每一轮迭代中提高被前一轮弱分类器错误分类样本的权重. 
  - 最终分类器是弱分类器的加权线性组合, 权重取决于该分类器的分类准确率. 

=== 自训练与协同训练 (Self-training & Co-training)

这两种方法主要用于*半监督学习*, 利用少量有标签数据和大量无标签数据. 

- *自训练 (Self-training)*: 模型先在有标签数据上训练, 然后对无标签数据进行预测, 将预测置信度高的样本及其伪标签 (Pseudo Label) 加入训练集, 循环往复. 
- *协同训练 (Co-training)*: 假设数据有两个充分且条件的独立视图. 在每个视图上训练一个模型, 互相为对方提供高置信度的伪标签样本. 

=== 多任务学习 (Multi-task Learning)

多任务学习利用多个相关任务之间的领域特定信息来提升每个任务的泛化能力. 

- *核心概念*: 多个任务并行学习, 并共享一部分网络表示 (如共享前几层特征提取层). 
- *作用*: 通过底层特征共享, 起到隐式的正则化作用, 减少过拟合风险. 

=== 迁移学习 (Transfer Learning)

迁移学习研究如何将从一个源领域 (Source Domain) 学到的知识应用到目标领域 (Target Domain). 

- *归纳迁移学习*: 源领域和目标领域任务不同 (如先学识别猫, 再迁移到识别虎). 
- *转导迁移学习*: 任务相同但数据分布不同 (如用摄影图片模型去识别素描图). 
- *常见手段*: 微调 (Fine-tuning), 即利用在大规模数据集上预训练好的参数作为初始值. 

=== 终身学习 (Lifelong Learning)

又称增量学习或持续学习, 模拟人类不断学习新知识而不遗忘旧知识的能力. 

- *核心挑战*: *灾难性遗忘 (Catastrophic Forgetting)*. 即学习新任务时, 原本在旧任务上学到的权重被覆盖, 导致旧任务性能剧降. 

=== 元学习 (Meta-Learning)

元学习即“学会学习” (Learning to Learn), 目标是让模型具备从少量经验中快速适应新任务的能力. 

- *基于优化器的元学习*: 学习一个通用的优化规则 (如替代 SGD 的更新公式). 
- *模型无关元学习 (MAML)*: 寻找一组对各种任务都高度敏感的初始化参数. 通过极少量的梯度更新, 模型就能在新的小样本任务上达到最佳性能. 

== 考题与考点预测

=== 核心考点预测

本章 (教材第10章) 主要介绍独立于具体网络结构的通用学习范式. 考试重点在于理解各种学习方式的*动机*、*适用场景*以及*核心思想*, 计算题可能涉及集成学习的误差分析或 AdaBoost 推导. 

1.  *集成学习 (Ensemble Learning)*
    -   *核心原理*：通过组合多个弱模型来提升性能. $cal(R)(F) <= cal(R)(f)$. 
    -   *Bagging (如随机森林)*：并行训练, 有放回采样 (Bootstrap), 主要降低*方差*, 增强独立性. 
    -   *Boosting (如 AdaBoost)*：串行训练, 关注被前序模型分错的样本, 主要降低*偏差*. 
    -   *AdaBoost 算法*：样本权重更新公式, 加性模型与指数损失函数的统计学解释. 

2.  *半监督学习 (Self-Training & Co-Training)*
    -   *自训练*：利用模型自身的高置信度预测作为伪标签. 
    -   *协同训练*：利用多视图 (Multi-view) 数据, 条件独立性假设, 互相打标签. 

3.  *多任务学习 (Multi-Task Learning)*
    -   *共享机制*：硬共享 (Hard Parameter Sharing) vs 软共享 (Soft Parameter Sharing). 
    -   *动机*：利用任务相关性作为归纳偏置, 隐式数据增强, 防止过拟合. 

4.  *迁移学习 (Transfer Learning)*
    -   *基本概念*：源领域 $cal(D)_S$ / 任务 $cal(T)_S$ vs 目标领域 $cal(D)_T$ / 任务 $cal(T)_T$. 
    -   *分类*：
        -   *归纳迁移 (Inductive)*：目标域有标签 (如预训练+微调). 
        -   *转导迁移 (Transductive)*：目标域无标签 (如领域适应 Domain Adaptation). 
    -   *领域适应 (DA)*：解决数据分布不一致问题. 
        -   *协变量偏移 (Covariate Shift)*：输入分布 $p(x)$ 不同, 后验 $p(y|x)$ 相同. 
        -   *概念偏移 (Concept Shift)*：后验 $p(y|x)$ 不同. 

5.  *终身学习 (Lifelong Learning)*
    -   *灾难性遗忘 (Catastrophic Forgetting)*：学习新任务导致旧任务性能急剧下降. 
    -   *弹性权重巩固 (EWC)*：利用 Fisher 信息矩阵约束重要参数的变化. 

6.  *元学习 (Meta-Learning)*
    -   *MAML (Model-Agnostic Meta-Learning)*：学习一个好的初始化参数, 使其能通过少量梯度下降快速适应新任务. 
    -   *基于优化器*：用 LSTM 学习梯度更新规则. 

=== 考题预测

==== 选择题

1. 关于集成学习中的 Bagging 和 Boosting 方法, 下列说法错误的是：
  - A. Bagging 方法中, 基模型之间不存在强依赖关系, 可以并行训练. 
  - B. 随机森林 (Random Forest) 是 Bagging 方法的一种典型代表, 它引入了随机特征选择. 
  - C. Boosting 方法通过关注被前一轮基模型误分类的样本来提升性能. 
  - D. Bagging 主要用于降低模型的偏差 (Bias), 而 Boosting 主要用于降低模型的方差 (Variance). 
  
  *答案：D*
  *解析：* 一般认为 Bagging 通过平均多个模型主要降低方差, 适合高方差模型 (如决策树) ；Boosting 通过拟合残差主要降低偏差. 

2. 在迁移学习中, 若源领域和目标领域的输入空间相同 $cal(X)_S = cal(X)_T$, 但输入数据的边际分布不同 $p_S(bold(x)) != p_T(bold(x))$, 且条件概率分布相同 $p_S(y|bold(x)) = p_T(y|bold(x))$, 这种情况被称为：
  - A. 概念偏移 (Concept Shift)
  - B. 协变量偏移 (Covariate Shift)
  - C. 先验偏移 (Prior Shift)
  - D. 标签偏移 (Label Shift)

  *答案：B*
  *解析：* 这是协变量偏移的定义, 即输入分布变化, 但映射关系不变. 

3. 关于多任务学习 (Multi-task Learning) 的共享模式, 下列描述正确的是：
  - A. 硬共享模式通常在网络的高层进行参数共享, 在底层使用私有模块. 
  - B. 软共享模式显式地强制不同任务使用完全相同的参数矩阵. 
  - C. 硬共享模式通常让不同任务的神经网络共同使用底层的共享模块, 高层使用任务特定模块. 
  - D. 多任务学习会导致模型在所有任务上的性能都下降, 因为任务之间存在干扰. 

  *答案：C*
  *解析：* 硬共享通常共享底层特征提取器；多任务学习利用相关性通常能提升泛化能力, 虽可能存在负迁移, 但 D 选项过于绝对. 

4. 终身学习 (Lifelong Learning) 中主要面临的挑战是“灾难性遗忘”, 弹性权重巩固 (EWC) 算法通过引入哪种矩阵来衡量参数对旧任务的重要性？
  - A. 混淆矩阵 (Confusion Matrix)
  - B. 协方差矩阵 (Covariance Matrix)
  - C. Fisher 信息矩阵 (Fisher Information Matrix)
  - D. 雅可比矩阵 (Jacobian Matrix)

  *答案：C*
  *解析：* EWC 利用 Fisher 信息矩阵近似后验分布的二阶导数, 作为正则化项的权重. 

5. 模型无关的元学习 (MAML) 的主要目标是：
  - A. 学习一个通用的优化器 (Optimizer) 来替代 SGD. 
  - B. 学习一组模型参数的初始值, 使得模型在面对新任务时能经过少量梯度更新达到较好效果. 
  - C. 将所有任务的数据混合在一起训练一个巨大的通用模型. 
  - D. 通过对抗训练对齐源域和目标域的特征分布. 

  *答案：B*
  *解析：* MAML 的核心是寻找一个对多任务敏感的初始化点 (Initialization). 

==== 简答题

1. *简述 AdaBoost 算法的核心思想及其样本权重调整策略. *
  *参考答案：*
  - *核心思想*：AdaBoost 是一种迭代的 Boosting 算法, 它通过串行训练一系列弱分类器来构建强分类器. 每个新的弱分类器都重点关注被前一个弱分类器错分的样本. 
  - *权重调整策略*：
    - 初始化时, 所有样本权重相等. 
    - 在每一轮迭代中, 增加被当前弱分类器误分类样本的权重, 降低分类正确样本的权重. 
    - 最终的强分类器是所有弱分类器的加权组合, 分类误差率低的弱分类器拥有更高的投票权重. 

2. *什么是协同训练 (Co-Training)？它需要满足哪两个假设？*
  *参考答案：*
  - *定义*：协同训练是一种半监督学习方法, 它利用数据的两个不同视角 (Views) 分别训练两个分类器, 并让它们互相为无标签数据打上伪标签 (互为老师) 来扩充训练集. 
  - *假设*：
    1.  *条件独立性*：给定类别标签时, 两个视角下的特征是条件独立的. 
    2.  *充足与冗余性*：仅使用任何一个视角的数据特征, 都足以训练出一个性能较好的分类器. 

3. *解释什么是“灾难性遗忘” (Catastrophic Forgetting), 并简述一种缓解该问题的方法思路. *
  *参考答案：*
  - *定义*：在终身学习或连续学习中, 神经网络在学习新任务时, 为了适应新任务的数据分布, 其参数发生大幅变化, 导致在之前已经学会的旧任务上的性能急剧下降甚至完全丧失的现象. 
  - *缓解思路 (如 EWC)*：
    - 在学习新任务时, 对模型参数的更新施加约束. 
    - 识别出对旧任务重要的参数 (例如通过 Fisher 信息矩阵衡量). 
    - 在损失函数中加入正则化项, 惩罚这些重要参数的改变, 允许不重要的参数大幅更新以适应新任务. 

