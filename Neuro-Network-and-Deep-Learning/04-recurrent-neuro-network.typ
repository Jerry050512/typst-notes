= 循环神经网络

== 核心概念与关键术语解析

在深入探讨复杂的循环网络模型之前，我们必须首先掌握一些基础的核心概念和术语。这些是构建后续知识体系的基石，准确理解它们对于掌握RNN的工作原理、解决相关考试问题至关重要。

/ 序列数据 (Sequential Data): 由一系列按特定顺序排列的数据点组成的数据类型。与普通数据不同，序列数据具有两个关键特性：
  - 顺序依赖性 (Sequence Dependency)：序列中元素的顺序至关重要。音频波形或股票价格的时间顺序也蕴含着关键信息。
  - 可变长度 (Variable Length)：序列的长度通常是不固定的。例如，句子可以由几个词组成，也可以由几十个词组成。模型必须能够灵活处理这种长度上的变化。
/ 循环 (Recurrence): 在RNN中，每个时间步的计算不仅依赖于当前的输入，还依赖于前一时间步的隐藏状态。具体来说，网络在时间步 t 的输出会作为一部分输入，参与到时间步 t+1 的计算中。这种信息回送的机制就是“循环”。
/ 记忆 (Memory): 通过这种循环结构，RNN维持了一个随时间动态更新的隐藏状态 (Hidden State)。这个隐藏状态可以被视为网络的“记忆”，它编码了到当前时间步为止的序列历史信息。正是这个记忆单元，使得RNN能够捕捉到序列中的时间动态和上下文关联。
/ 梯度消失 (Vanishing Gradients): 如果权重矩阵的范数小于1，经过多次连乘后，梯度会以指数级速度缩小，最终变得微乎其微。这导致网络无法学习到序列中距离较远（即“长期”）的依赖关系，因为来自遥远过去的梯度信号无法有效传播回前端。
/ 梯度爆炸 (Exploding Gradients): 相反，如果权重矩阵的范数大于1，梯度则会指数级增大，导致参数更新过大，使训练过程变得不稳定甚至崩溃。

这两个问题，尤其是梯度消失，极大地限制了简单RNN捕捉长期依赖的能力。这也催生了后续更高级的RNN变体，如我们稍后将要讨论的长短期记忆网络（LSTM），它们的设计初衷就是为了解决这一核心难题。

== #emoji.star 核心模型架构与参数学习

本节是整个章节的核心，我们将深入剖析简单RNN的内部工作机制，并重点介绍其关键的训练算法——随时间反向传播（BPTT）。这部分内容是理解所有循环网络变体的基础，也是考试的重中之重。

=== 简单循环神经网络 (Simple RNNs)

简单RNN是循环网络最基础的形式。其核心在于一个循环单元，该单元在每个时间步 t 执行相同的计算任务。

信息流动过程 如下：

1. 输入：在时间步 $t$，网络接收两个输入：
  - 当前时刻的输入向量 $x_t$。
  - 前一时刻的隐藏状态 $h_(t-1)$。
2. 计算：网络通过一个激活函数（通常是 tanh）来更新其隐藏状态：
  - $W_(x h)$ 是输入到隐藏层的权重矩阵。
  - $W_(h h)$ 是从前一隐藏状态到当前隐藏状态的权重矩阵，这是实现“循环”的关键。
  - $b_h$ 是隐藏层的偏置向量。
  - 关键点：$W_(x h)$, $W_(h h)$, 和 $b_h$ 这三组参数在所有时间步中是共享的，这大大减少了模型的参数量，并使得模型能够处理变长序列。
3. 输出：网络根据新的隐藏状态 $h_t$ 计算当前时间步的输出 $y_t$

=== #emoji.star 随时间反向传播算法 (BPTT)

随时间反向传播（Backpropagation Through Time, BPTT）是训练RNN的标准算法。我们可以将其理解为标准反向传播算法（第4章, 4.5节）在时间维度上的扩展。

核心原理：BPTT的本质是在一个按时间步“展开”（Unrolled）的RNN计算图上应用链式法则来计算梯度。

核心流程可以分解为以下四个步骤：

1. 前向传播：给定一个输入序列，按照时间步从 $t=1$ 到 $T$ 依次计算每个时间步的隐藏状态 $h_t$ 和输出 $y_t$。最后，根据所有时间步的输出和真实标签计算总损失（Loss）。
2. 沿时间展开网络：将RNN的循环结构在时间维度上“展开”，转换成一个等效的深度前馈网络。这个展开后的网络有两个关键特性：第一，网络的每一层对应序列中的一个时间步；第二，所有这些层级（时间步）共享同一套权重参数。
3. 反向传播计算梯度：总损失对于每个时间步的参数的梯度被分别计算出来。由于参数是共享的，损失在 t 时刻的梯度不仅依赖于当前时刻的输出，还通过 $h_t$ 影响到 $t+1$ 时刻，因此梯度需要从最后一个时间步 T 一直反向传播到第一个时间步 $t=1$。
4. 汇总梯度并更新参数：将所有时间步计算出的梯度进行累加，得到共享参数 $W_(x h)$, $W_(h h)$, $b_h$ 等的最终梯度。最后，使用梯度下降等优化算法来更新这些共享参数。

BPTT的实现使得RNN能够从序列数据中学习，但正如前面所讨论的，在处理长序列时，它也正是导致梯度消失或爆炸问题的根源。

== 高级循环网络变体

为了克服简单RNN在捕捉长期依赖方面的局限性，研究人员开发了多种更为复杂的变体。这些模型通过引入精巧的内部机制来控制信息流，从而有效地缓解了梯度消失问题。本节将重点介绍其中最著名、应用最广泛的模型——长短期记忆网络（LSTM）。

=== 长短期记忆网络 (LSTM)

长短期记忆网络（Long Short-Term Memory, LSTM）是解决梯度消失问题的关键方案。它通过引入一个细胞状态（Cell State）门控机制（Gating Mechanisms），使得网络能够有选择地记忆、遗忘和输出信息。细胞状态像一条传送带，让信息在序列中几乎无衰减地传递，而门控单元则像阀门一样，精确地控制着信息的流入和流出。

以下是LSTM三大核心门控单元的功能解析：

#table(
  columns: (auto, auto, auto),
  [*门控单元*], [*功能描述*], [*对信息流的影响*],
  [遗忘门 (Forget Gate)], [决定从细胞状态中丢弃哪些旧信息。], [基于当前输入和前一时刻的隐藏状态，控制历史信息的保留程度。],
  [输入门 (Input Gate)], [决定哪些新信息将被存入细胞状态。], [过滤并选择有价值的当前输入信息，以更新长期记忆。],
  [输出门 (Output Gate)], [决定细胞状态的哪些部分将作为当前隐藏状态输出。], [从长期记忆中提取与当前任务相关的信息，用于预测或传递给下一个时间步。],
)


在训练LSTM网络时，为了防止网络在训练初期过快地遗忘历史信息，导致难以捕捉长距离依赖，遗忘门的偏置项通常被初始化为一个较大的正值（如1或2）。这使得遗忘门在训练开始时倾向于保持开放（即不遗忘），从而保证了梯度的有效传播。

=== LSTM网络变体

在核心LSTM架构的基础上，还衍生出了一些变体，它们对门控机制进行了调整和优化。

- 无遗忘门的LSTM网络：由[Hochreiter et al., 1997]最早提出的原始LSTM网络实际上没有遗忘门。这种设计的缺陷在于，记忆单元 c 的内容会随着时间不断累积和增大，当输入序列非常长时，记忆单元的容量可能会饱和，从而影响模型性能。
- Peephole连接：这是一种允许门控单元直接“窥视”细胞状态的变体。具体来说，三个门（遗忘、输入、输出）的计算不仅依赖于当前输入和前一隐藏状态，还直接依赖于前一时刻的细胞状态 $c_(t-1)$。这为门控决策提供了更丰富的信息。

=== 扩展模型

/ 深度循环神经网络 (Deep RNNs): 深度RNN通过在每个时间步上堆叠多个循环层来增加模型的深度。底层的循环层负责捕捉序列的低阶特征，而其输出则作为上一层循环层的输入，使得更高层的网络能够学习到数据中更抽象、更复杂的层次化特征表示。
/ 拓展到图结构 (Extension to Graph Structures): 传统的RNN主要处理线性的序列数据。然而，通过将其核心思想进行扩展，模型可以被应用于处理更普遍的图结构数据（如社交网络、分子结构等）。这种扩展使得模型能够捕捉非线性、非顺序的复杂依赖关系，极大地拓宽了循环网络的应用范围。

== 备考习题预测

=== 选择题

1. 长短期记忆网络（LSTM）主要解决了简单RNN中的什么问题？
  - A. 计算复杂度过高
  - B. 梯度消失问题
  - C. 模型参数过多
  - D. 只能处理定长序列
2. 在随时间反向传播（BPTT）算法中，网络共享参数的最终梯度是如何计算的？
  - A. 只使用最后一个时间步的梯度
  - B. 将所有时间步计算出的梯度进行累加
  - C. 取所有时间步梯度的平均值
  - D. 使用梯度最大的那个时间步的梯度
3. 在LSTM的三个门中，哪个门负责决定从细胞状态中丢弃多少历史信息？
  - A. 输入门
  - B. 输出门
  - C. 遗忘门
  - D. 状态门

=== 简答题

1. 问题： 请简要解释“随时间反向传播（BPTT）”的基本原理，并说明它与标准反向传播算法有何异同。
  - 参考答案：
    - 基本原理： BPTT是训练RNN的标准算法。其核心思想是将RNN沿时间维度展开，形成一个参数共享的深度前馈网络。然后，在这个展开的网络上应用标准的反向传播算法，计算损失函数对共享参数的梯度。
    - 相同点： 两者都基于链式法则来计算梯度，以实现端到端的模型训练。
    - 不同点： 标准反向传播应用于层与层之间，而BPTT则应用于时间步与时间步之间。此外，BPTT计算出的梯度需要在所有时间步上进行累加，以更新在时间维度上共享的参数。
2. 问题： 为什么说简单RNN在处理长序列时会遇到“长期依赖”问题？LSTM是如何通过其门控机制来缓解这个问题的？
  - 参考答案：
    - 长期依赖问题原因： 简单RNN在处理长序列时，梯度需要通过BPTT算法在时间步上进行反向传播。这个过程中梯度会反复乘以相关的权重矩阵，导致梯度呈指数级消失或爆炸。梯度消失使得模型无法学习到序列中相距较远的元素之间的依赖关系，这就是长期依赖问题。
    - LSTM的解决方案： LSTM引入了细胞状态和门控机制。细胞状态像一条“传送带”，允许信息在时间步之间直接传递，避免了梯度连乘导致的衰减。遗忘门、输入门和输出门这三个门控单元则像智能阀门，有选择地决定哪些信息应该被遗忘，哪些新信息应该被加入，以及哪些信息应该被输出。这种精细的控制机制确保了有用的长期信息可以被保留，从而有效缓解了梯度消失问题，使网络能够学习长期依赖。

=== 综合计算题

问题： 假设一个简单的RNN单元，其隐藏状态的更新公式为 $h_t = tanh(W_(h h) - h_(t-1) + W_(x h) - x_t + b_h)$。请描述当给定初始隐藏状态 $h_0$ 和一个长度为2的输入序列 $[x_1, x_2]$ 时，计算 $h_2$ 的详细步骤。请写出计算 $h_1$ 和 $h_2$ 的概念性公式，并指明其中涉及的所有权重矩阵和偏置向量。
  - 参考答案： 计算 $h_2$ 的过程需要分两步进行，严格遵循时间顺序。其中涉及的共享参数包括：输入-隐藏层权重 $W_(x h)$，隐藏-隐藏层权重 $W_(h h)$，以及隐藏层偏置 $b_h$。
    - 第一步：计算 $h_1$ 使用初始隐藏状态 $h_0$ 和第一个输入 $x_1$ 来计算第一个时间步的隐藏状态 $h_1$。
    - 第二步：计算 $h_2$ 使用上一步计算出的隐藏状态 $h_1$ 和第二个输入 $x_2$ 来计算第二个时间步的隐藏状态 $h_2$。
  - 通过这两个步骤，网络将序列 $[x_1, x_2]$ 的信息逐步编码，最终得到的 $h_2$ 包含了整个序列的上下文信息。
