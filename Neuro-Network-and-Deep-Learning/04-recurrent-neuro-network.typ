= 循环神经网络

== 核心概念与关键术语解析

在深入探讨复杂的循环网络模型之前, 我们必须首先掌握一些基础的核心概念和术语. 这些是构建后续知识体系的基石, 准确理解它们对于掌握RNN的工作原理、解决相关考试问题至关重要. 

/ 序列数据 (Sequential Data): 由一系列按特定顺序排列的数据点组成的数据类型. 与普通数据不同, 序列数据具有两个关键特性: 
  - 顺序依赖性 (Sequence Dependency): 序列中元素的顺序至关重要. 音频波形或股票价格的时间顺序也蕴含着关键信息. 
  - 可变长度 (Variable Length): 序列的长度通常是不固定的. 例如, 句子可以由几个词组成, 也可以由几十个词组成. 模型必须能够灵活处理这种长度上的变化. 
/ 循环 (Recurrence): 在RNN中, 每个时间步的计算不仅依赖于当前的输入, 还依赖于前一时间步的隐藏状态. 具体来说, 网络在时间步 t 的输出会作为一部分输入, 参与到时间步 t+1 的计算中. 这种信息回送的机制就是“循环”. 
/ 记忆 (Memory): 通过这种循环结构, RNN维持了一个随时间动态更新的隐藏状态 (Hidden State). 这个隐藏状态可以被视为网络的“记忆”, 它编码了到当前时间步为止的序列历史信息. 正是这个记忆单元, 使得RNN能够捕捉到序列中的时间动态和上下文关联. 
/ 梯度消失 (Vanishing Gradients): 如果权重矩阵的范数小于1, 经过多次连乘后, 梯度会以指数级速度缩小, 最终变得微乎其微. 这导致网络无法学习到序列中距离较远 (即“长期”) 的依赖关系, 因为来自遥远过去的梯度信号无法有效传播回前端. 
/ 梯度爆炸 (Exploding Gradients): 相反, 如果权重矩阵的范数大于1, 梯度则会指数级增大, 导致参数更新过大, 使训练过程变得不稳定甚至崩溃. 

这两个问题, 尤其是梯度消失, 极大地限制了简单RNN捕捉长期依赖的能力. 这也催生了后续更高级的RNN变体, 如我们稍后将要讨论的长短期记忆网络 (LSTM), 它们的设计初衷就是为了解决这一核心难题. 

== 核心模型架构与参数学习

=== 增加记忆能力

/ 延时神经网络 (Time Delay Neural Network): TDNN 通过建立一个额外的延时单元, 在前馈神经网络的*非输出层*都增加一个*延时器*, 记录最近的几次活性值. 在$t$时刻, 第$l + 1$层的活性值依赖于第$l$层最近$p$时刻的活性值. $ h_t^((l + 1)) = f(h_t^((l)), h_(t - 1)^((l)), dots, h_(t - p)^((l))) $
/ 有外部输入的非线性自回归模型 (Nonlinear Autoregressive with Exogenous Inputs Model): NARX 通过将外部输入 $x_t$ 加入到自回归模型中, 增加了记忆能力. 每个时刻$t$, 都有一个外部输入$x_t$产生一个输出$y_t$; 通过延时器记录最近$K x$和$K y$的输入输出 $ y_t = f(x_t, x_(t-1), dots, x_(t-K x), y_(t-1), dots, y_(t-K y)) $
/ 循环神经网络 (Recurrent Neural Network): RNN 通过引入一个隐藏状态$h_t$, 将当前时刻的输入$x_t$和前一时刻的隐藏状态$h_(t-1)$作为输入, 计算当前时刻的隐藏状态$h_t$和输出$y_t$. $ h_t = f(x_t, h_(t-1)) $


=== 简单循环神经网络 (Simple RNNs)

简单 RNN 是循环网络最基础的形式. 其核心在于一个循环单元, 该单元在每个时间步 $t$ 执行相同的计算任务. 

1. *输入*: 在时间步 $t$, 网络接收两个输入: 
  - 当前时刻的输入向量 $x_t$; 
  - 前一时刻的隐藏状态 $h_(t-1)$. 

2. *计算 (净输入与状态更新) *: 
  网络首先计算当前时刻的净输入 $z_t$, 然后通过激活函数更新隐藏状态: 
  $ z_t = U h_(t-1) + W x_t + b $
  $ h_t = f(z_t) $
  其中: 
  - $W$ 是状态-输入权重矩阵 (对应 $x_t$); 
  - $U$ 是状态-状态转移权重矩阵 (对应 $h_(t-1)$), 这是实现“循环”的关键; 
  - $b$ 是偏置向量; 
  - $f(dot)$ 通常为 $tanh$ 激活函数. 

  *关键点*: 参数 $W, U, b$ 在所有时间步中是*共享*的, 这显著减少了参数量并赋予模型处理变长序列的能力. 

3. *输出*: 网络根据新的隐藏状态 $h_t$ 计算当前时间步的输出 $y_t$: 
  $ y_t = V h_t + b_y $

SRN 可以按时间展开, 将每个时刻的状态都看作前馈神经网络的每一层, 那么就看作*在时间维度上权重共享*的神经网络. 

应用到机器学习主要有三种模式: *序列到类别* (文本情感分类), *同步的序列到序列* (中文分词), *异步的序列到序列* (机器翻译).

=== 随时间反向传播算法 (BPTT)

随时间反向传播 (Backpropagation Through Time, BPTT) 是训练 RNN 的标准算法. 其本质是将标准的反向传播算法应用在按时间步“展开”后的计算图上. 

BPTT 将循环结构转化为一个等效的*深度前馈网络*, 通过链式法则在时间轴上逆向传递误差信号. 其展开后的网络具有两个关键特性: 
- *深度等效*: 每一层对应序列中的一个时间步; 
- *权值共享*: 所有层级 (时间步) 共享同一套参数 $W, U, b$. 

BPTT 执行流程如下: 

+ *前向传播 (Forward Pass)*: 
  给定序列 $x_1, dots, x_T$, 按时间步计算隐藏状态和输出: 
  $ z_t = U h_(t-1) + W x_t + b $
  $ h_t = f(z_t) $
  最后计算总损失 $L$, 通常为各时刻损失之和: $L = sum_(t=1)^T L_t$. 

+ *网络展开 (Unrolling)*: 
  将循环连接转化为层级连接, 形成 $t=1 arrow.r t=2 arrow.r dots arrow.r T$ 的长链. 

+ *反向传播计算梯度 (Backward Pass)*: 
  梯度不仅在层间传播, 还沿时间轴 *由后向前* 传播. 
  对于参数 $U$ (状态转移矩阵), 其在时刻 $t$ 的偏导数取决于: 
  - 当前时刻的损失梯度; 
  - 以后时刻通过 $h_(t+1)$ 传回的梯度. 
  $ (partial L)/(partial U) = sum_(t=1)^T (partial L_t)/(partial U) $

+ *汇总与更新 (Accumulation & Update)*: 
  由于参数共享, 最终梯度是所有时刻梯度的 *累加值*: 
  $ Delta W = sum_(t=1)^T (partial L)/(partial W_t), quad Delta U = sum_(t=1)^T (partial L)/(partial U_t) $
  最后使用优化器 (如 Adam 或 SGD) 更新参数. 

BPTT 在处理长序列时, 由于梯度在时间轴上的连乘效应 (特别是 $U$ 的连乘), 会导致: 
- *梯度消失*: 梯度趋近于 0, 模型无法学习远距离依赖. 
- *梯度爆炸*: 梯度呈指数级增长, 导致训练不稳定. 

== 高级循环网络变体

为了克服简单RNN在捕捉长期依赖方面的局限性, 研究人员开发了多种更为复杂的变体. 这些模型通过引入精巧的内部机制来控制信息流, 从而有效地缓解了梯度消失问题. 本节将重点介绍其中最著名、应用最广泛的模型——长短期记忆网络 (LSTM). 

=== 长短期记忆网络 (LSTM)

长短期记忆网络 (Long Short-Term Memory, LSTM) 是解决 RNN 梯度消失问题的核心方案. 它通过引入*细胞状态 (Cell State) *和*门控机制 (Gating Mechanisms) *, 实现了信息的长期保存. LSTM 的精妙之处在于 $c_t = f_t dot C_(t-1) + i_t dot tilde(c)_t$. 这种*线性加法*结构使得误差在反向传播时, 即使经过很多个时间步, 只要遗忘门开启, 梯度就不会像传统 RNN 那样因为连乘而迅速消失. 

在每个时刻 $t$, LSTM 的计算逻辑由以下公式构成: 

1. *计算门控状态* (均使用 $sigma$ 激活函数, 输出在 $[0, 1]$ 之间): 
  - 遗忘门: $f_t = sigma(W_f x_t + U_f h_(t-1) + b_f)$
  - 输入门: $i_t = sigma(W_i x_t + U_i h_(t-1) + b_i)$
  - 输出门: $o_t = sigma(W_o x_t + U_o h_(t-1) + b_o)$

2. *更新细胞状态*: 
  - 候选状态: $tilde(c)_t = tanh(W_c x_t + U_c h_(t-1) + b_c)$
  - 当前状态: $c_t = f_t dot C_(t-1) + i_t dot tilde(c)_t$  (*关键: 加法运算避免了梯度消失*) 

3. *计算隐藏输出*: 
  - $h_t = o_t dot tanh(c_t)$

#figure(
  table(
    columns: 3,
    [*门控单元*], [*功能描述*], [*对信息流的影响*],
    [遗忘门 $f_t$], [决定从细胞状态中丢弃哪些旧信息. ], [若 $f_t approx 0$, 历史信息被清空; 若 $f_t approx 1$, 则无损保留. ],
    [输入门 $i_t$], [决定哪些新信息将被存入细胞状态. ], [配合 $tilde(c)_t$ 筛选有价值的信息存入长期记忆. ],
    [输出门 $o_t$], [决定细胞状态的哪些部分将输出. ], [从长期记忆中提取与当前预测相关的信息. ],
  ),
  caption: "LSTM 的核心门控机制及其作用"
)

在训练 LSTM 时, 一个常用的策略是将遗忘门的偏置 $b_f$ 初始化为一个较大的正值 (如 $1.0$ 或 $2.0$). 
- *目的*: 确保在训练初期 $f_t$ 接近 1. 
- *效果*: 让网络在学习之初倾向于“记住所有信息”, 使梯度能够通过 $c_t$ 路径长距离回传, 有效缓解冷启动时的模型训练困难. 

=== LSTM网络变体

在核心LSTM架构的基础上, 还衍生出了一些变体, 它们对门控机制进行了调整和优化. 

- 无遗忘门的LSTM网络: 由[Hochreiter et al., 1997]最早提出的原始LSTM网络实际上没有遗忘门. 这种设计的缺陷在于, 记忆单元 c 的内容会随着时间不断累积和增大, 当输入序列非常长时, 记忆单元的容量可能会饱和, 从而影响模型性能. 
- Peephole连接: 这是一种允许门控单元直接“窥视”细胞状态的变体. 具体来说, 三个门 (遗忘、输入、输出) 的计算不仅依赖于当前输入和前一隐藏状态, 还直接依赖于前一时刻的细胞状态 $c_(t-1)$. 这为门控决策提供了更丰富的信息. 

=== 门控循环单元 (GRU)

门控循环单元 (Gated Recurrent Unit, GRU) 由 Cho 等人在 2014 年提出. 相比 LSTM, GRU 的结构更加精简, 它将 LSTM 的“细胞状态”与“隐藏状态”合并, 并减少了门控的数量, 从而降低了计算复杂度. 

在每个时刻 $t$, GRU 的内部状态更新遵循以下逻辑: 

1. *门控计算*: 
  - *重置门 (Reset Gate)*: $r_t = sigma(W_r x_t + U_r h_(t-1) + b_r)$
    决定了前一时刻状态 $h_(t-1)$ 对当前候选状态的贡献程度. 
  - *更新门 (Update Gate)*: $z_t = sigma(W_z x_t + U_z h_(t-1) + b_z)$
    控制前一时刻状态与当前候选状态的融合比例, 相当于 LSTM 中遗忘门和输入门的结合. 

2. *候选隐藏状态*: 
  - $tilde(h)_t = tanh(W_h x_t + U_h (r_t dot h_(t-1)) + b_h)$
    这里的 $r_t$ 控制了忽略多少过去的信息. 

3. *最终隐藏状态更新*: 
  - $h_t = (1 - z_t) dot h_(t-1) + z_t dot tilde(h)_t$
    这是一个线性插值操作, 决定了保留多少旧状态以及加入多少新候选状态. 

#figure(
  table(
    columns: 3,
    [*特性*], [*LSTM*], [*GRU*],
    [门控数量], [3个 (遗忘门、输入门、输出门) ], [2个 (重置门、更新门) ],
    [内部状态], [隐藏状态 $h_t$ + 细胞状态 $c_t$], [仅隐藏状态 $h_t$],
    [计算开销], [参数较多, 计算量相对较大], [参数较少, 计算速度更快, 易于收敛],
    [记忆机制], [通过 $c_t$ 的累加链条防止梯度消失], [通过更新门 $z_t$ 的线性内插防止梯度消失],
  ), 
  caption: "LSTM 和 GRU 的主要区别"
)

=== 扩展模型

/ 深度循环神经网络 (Deep RNNs): 深度RNN通过在每个时间步上堆叠多个循环层来增加模型的深度. 底层的循环层负责捕捉序列的低阶特征, 而其输出则作为上一层循环层的输入, 使得更高层的网络能够学习到数据中更抽象、更复杂的层次化特征表示. 
/ 拓展到图结构 (Extension to Graph Structures): 传统的RNN主要处理线性的序列数据. 然而, 通过将其核心思想进行扩展, 模型可以被应用于处理更普遍的图结构数据 (如社交网络、分子结构等). 这种扩展使得模型能够捕捉非线性、非顺序的复杂依赖关系, 极大地拓宽了循环网络的应用范围. 

== 考题与考点预测

=== 核心考点预测

本章在期末考试中通常占据较大比重, 考察形式涵盖选择、简答及计算推导. 

+ *循环神经网络 (RNN) 基础*：
  - 了解RNN与前馈神经网络 (FNN) 的区别 (记忆能力、参数共享). 
  - 掌握简单循环网络的状态更新公式 $h_t = f(U h_(t-1) + W x_t + b)$. 
  - 理解Seq2Seq模式 (同步/异步). 

+ *随时间反向传播算法 (BPTT) *：
  - *重点*：理解BPTT是将RNN按时间展开后的BP算法. 
  - 掌握梯度计算中“按时间累积”的概念. 
  - 能够写出损失函数关于参数 $U$ (状态-状态权重) 的梯度通式. 

+ *长程依赖问题*：
  - 理解梯度消失和梯度爆炸的数学本质 (权重矩阵的连乘). 
  - 掌握解决梯度爆炸的方法 (梯度截断) 和梯度消失的方法 (门控机制、残差连接). 

+ *门控循环神经网络 (LSTM & GRU) *：
  - *LSTM*：掌握三个门 (遗忘门、输入门、输出门) 的功能, 特别是遗忘门的作用；理解记忆单元 $c_t$ 与 隐状态 $h_t$ 的区别. 
  - *GRU*：掌握两个门 (重置门、更新门) 的功能；对比GRU与LSTM的参数量与结构差异. 

=== 考题预测

==== 选择题

1. 在循环神经网络 (RNN) 中, 关于随时间反向传播算法 (BPTT), 下列说法错误的是【 】
  - A. BPTT算法主要用于计算RNN中的参数梯度
  - B. 随着序列长度增加, BPTT计算梯度的路径会变长, 容易导致梯度消失或爆炸
  - C. 为了降低计算复杂度, 通常可以使用截断的BPTT (Truncated BPTT) 
  - D. BPTT中, 参数 $U$ 的梯度只取决于当前时刻 $t$ 的误差, 与历史时刻无关

2. 导致简单循环神经网络 (Simple RNN) 难以训练“长程依赖”问题的主要原因是【 】
  - A. 激活函数导数过大
  - B. 随着时间步增加, 梯度在反向传播时经过连乘产生指数级衰减或增长
  - C. 网络参数数量过多导致过拟合
  - D. 损失函数定义不合理

3. 长短期记忆网络 (LSTM) 通过引入门控机制解决了梯度消失问题. 其中, 控制上一时刻的内部状态 $c_(t-1)$ 有多少信息需要保留到当前时刻 $c_t$ 的是【 】
  - A. 输入门 (Input Gate)
  - B. 遗忘门 (Forget Gate)
  - C. 输出门 (Output Gate)
  - D. 重置门 (Reset Gate)

4. 相比于LSTM, 门控循环单元 (GRU) 的结构更为简单. 下列关于GRU的描述正确的是【 】
  - A. GRU 包含三个门：输入门、遗忘门和输出门
  - B. GRU 引入了独立的记忆单元 (Cell State) $c_t$ 和隐状态 $h_t$
  - C. GRU 将遗忘门和输入门合并为一个更新门 (Update Gate)
  - D. GRU 的参数量通常多于同等隐层大小的 LSTM

5. (多选) 针对循环神经网络中的梯度爆炸问题, 有效的解决方案包括【 】
  - A. 梯度截断 (Gradient Clipping)
  - B. 权重衰减 (Weight Decay / L2 正则化)
  - C. 将激活函数更换为 Sigmoid
  - D. 使用 ReLU 激活函数并配合特定的参数初始化

==== 简答题

1. *简述随时间反向传播 (BPTT) 算法的基本思想, 并说明它与标准反向传播 (BP) 算法的区别. *

  *参考答案：*
  - *基本思想*：BPTT 是基于误差反向传播算法训练循环神经网络的方法. 其核心思想是将循环神经网络在时间维度上进行展开, 将其看作一个参数共享的深层前馈神经网络. 
  - *区别*：
    1. *结构展开*：标准 BP 用于层数固定的前馈网络；BPTT 需将网络按时间步 $t$ 展开, 层数等于序列长度. 
    2. *参数共享*：BPTT 中, 展开后的每一层 (每个时间步) 共享同一组权重参数 ($U, W, b$). 
    3. *梯度累加*：计算参数梯度时, BPTT 需要将所有时间步计算得到的梯度进行累加 (或平均), 而不仅仅是当前层的梯度. 

2. *分析循环神经网络中梯度消失产生的原因, 并从数学角度解释为什么LSTM能缓解这一问题. *

  *参考答案：*
  - *梯度消失原因*：在简单RNN中, 反向传播时梯度包含形如 $product_(k=t)^(T) "diag"(f') W$ 的连乘项. 如果权重矩阵的模或激活函数的导数小于1, 随着时间步 $t$ 的增加, 连乘项会趋近于0, 导致长距离的梯度无法传导到初始时刻. 
  - *LSTM的缓解机制*：
    1. LSTM引入了内部状态 $c_t$, 其更新公式近似为 $c_t = f_t dot c_(t-1) + i_t dot tilde(c)_t$. 
    2. 在反向传播求导 $(partial c_t) / (partial c_(t-1))$ 时, 梯度主要受遗忘门 $f_t$ 控制. 
    3. 如果遗忘门 $f_t approx 1$, 梯度可以无损地 (线性地) 在时间维度上传播, 形成“常数误差传送带” (Constant Error Carousel), 从而避免了连乘导致的梯度快速衰减. 

3. *对比LSTM与GRU的区别. *

  *参考答案：*
  - *结构差异*：LSTM 有三个门 (输入、遗忘、输出) 和一个独立的记忆单元 $c_t$；GRU 只有两个门 (更新门、重置门), 直接对隐状态 $h_t$ 进行操作, 无独立记忆单元. 
  - *功能合并*：GRU 将 LSTM 的输入门和遗忘门的功能融合到了更新门中. 
  - *计算效率*：由于门控数量减少, GRU 的参数量比 LSTM 少, 计算与训练速度通常更快, 但表达能力在某些复杂任务上可能略弱于 LSTM. 

==== 计算题

*题目：简单循环神经网络的梯度推导*

考虑一个简单的循环神经网络, 在时刻 $t$ 的状态更新公式为：
$ h_t = f(z_t) = f(U h_(t-1) + W x_t + b) $
其中 $f(dot)$ 为激活函数. 假设 $t$ 时刻的损失函数为 $L_t$. 

1. 写出 $t$ 时刻的净输入 $z_t$ 关于上一时刻状态 $h_(t-1)$ 的直接导数. 
2. 定义误差项 $delta_(t, k) = (partial L_t) / (partial z_k)$ (其中 $k < t$). 请利用链式法则, 推导 $delta_(t, k)$ 与 $delta_(t, k+1)$ 之间的递推关系式. 
3. 说明当时间间隔 $t-k$ 很大时, 上述递推关系如何导致梯度问题. 

*参考解答：*

*1. 净输入关于上一时刻状态的导数：*
由 $z_t = U h_(t-1) + W x_t + b$, 可知：
$ (partial z_t) / (partial h_(t-1)) = U $
注意：若考虑 $h_(t-1) = f(z_(t-1))$, 则 $(partial h_(t-1)) / (partial z_(t-1)) = "diag"(f'(z_(t-1)))$. 

*2. 误差项递推关系推导：*
根据链式法则 (BPTT的核心): 
$ delta_(t, k) = (partial L_t) / (partial z_k) = (partial L_t) / (partial z_(k+1)) dot (partial z_(k+1)) / (partial h_k) dot (partial h_k) / (partial z_k) $

代入已知项：
- $(partial L_t) / (partial z_(k+1)) = delta_(t, k+1)$
- $(partial z_(k+1)) / (partial h_k) = U$
- $(partial h_k) / (partial z_k) = "diag"(f'(z_k))$

因此递推关系为：
$ delta_(t, k) = delta_(t, k+1) dot U dot "diag"(f'(z_k)) $
或者写成转置形式 (取决于向量定义习惯, 通常在反向传播中): 
$ delta_(t, k) = "diag"(f'(z_k)) U^T delta_(t, k+1) $

*3. 梯度问题分析：*
将上述递推公式展开, 从 $t$ 时刻传回到 $k$ 时刻：
$ delta_(t, k) = (product_(tau=k)^(t-1) "diag"(f'(z_tau)) U^T) delta_(t, t) $
当 $t-k$ 很大时, 梯度包含 $U^T$ 的 $t-k$ 次连乘. 
- 若 $||U||$ 的特征值 $< 1$ 且激活函数导数 $< 1$ (如Tanh/Sigmoid), 连乘结果趋向于0, 导致*梯度消失*. 
- 若 $||U||$ 的特征值 $> 1$ (且未被激活函数导数抵消), 连乘结果趋向于无穷, 导致*梯度爆炸*. 