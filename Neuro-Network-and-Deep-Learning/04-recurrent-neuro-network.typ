= 循环神经网络

== 核心概念与关键术语解析

在深入探讨复杂的循环网络模型之前，我们必须首先掌握一些基础的核心概念和术语。这些是构建后续知识体系的基石，准确理解它们对于掌握RNN的工作原理、解决相关考试问题至关重要。

/ 序列数据 (Sequential Data): 由一系列按特定顺序排列的数据点组成的数据类型。与普通数据不同，序列数据具有两个关键特性：
  - 顺序依赖性 (Sequence Dependency)：序列中元素的顺序至关重要。音频波形或股票价格的时间顺序也蕴含着关键信息。
  - 可变长度 (Variable Length)：序列的长度通常是不固定的。例如，句子可以由几个词组成，也可以由几十个词组成。模型必须能够灵活处理这种长度上的变化。
/ 循环 (Recurrence): 在RNN中，每个时间步的计算不仅依赖于当前的输入，还依赖于前一时间步的隐藏状态。具体来说，网络在时间步 t 的输出会作为一部分输入，参与到时间步 t+1 的计算中。这种信息回送的机制就是“循环”。
/ 记忆 (Memory): 通过这种循环结构，RNN维持了一个随时间动态更新的隐藏状态 (Hidden State)。这个隐藏状态可以被视为网络的“记忆”，它编码了到当前时间步为止的序列历史信息。正是这个记忆单元，使得RNN能够捕捉到序列中的时间动态和上下文关联。
/ 梯度消失 (Vanishing Gradients): 如果权重矩阵的范数小于1，经过多次连乘后，梯度会以指数级速度缩小，最终变得微乎其微。这导致网络无法学习到序列中距离较远（即“长期”）的依赖关系，因为来自遥远过去的梯度信号无法有效传播回前端。
/ 梯度爆炸 (Exploding Gradients): 相反，如果权重矩阵的范数大于1，梯度则会指数级增大，导致参数更新过大，使训练过程变得不稳定甚至崩溃。

这两个问题，尤其是梯度消失，极大地限制了简单RNN捕捉长期依赖的能力。这也催生了后续更高级的RNN变体，如我们稍后将要讨论的长短期记忆网络（LSTM），它们的设计初衷就是为了解决这一核心难题。

== #emoji.star 核心模型架构与参数学习

=== 增加记忆能力

/ 延时神经网络 (Time Delay Neural Network): TDNN 通过建立一个额外的延时单元, 在前馈神经网络的*非输出层*都增加一个*延时器*, 记录最近的几次活性值. 在$t$时刻, 第$l + 1$层的活性值依赖于第$l$层最近$p$时刻的活性值. $ h_t^((l + 1)) = f(h_t^((l)), h_(t - 1)^((l)), dots, h_(t - p)^((l))) $
/ 有外部输入的非线性自回归模型 (Nonlinear Autoregressive with Exogenous Inputs Model): NARX 通过将外部输入 $x_t$ 加入到自回归模型中, 增加了记忆能力. 每个时刻$t$, 都有一个外部输入$x_t$产生一个输出$y_t$; 通过延时器记录最近$K x$和$K y$的输入输出 $ y_t = f(x_t, x_(t-1), dots, x_(t-K x), y_(t-1), dots, y_(t-K y)) $
/ 循环神经网络 (Recurrent Neural Network): RNN 通过引入一个隐藏状态$h_t$, 将当前时刻的输入$x_t$和前一时刻的隐藏状态$h_(t-1)$作为输入, 计算当前时刻的隐藏状态$h_t$和输出$y_t$. $ h_t = f(x_t, h_(t-1)) $


=== 简单循环神经网络 (Simple RNNs)

简单 RNN 是循环网络最基础的形式。其核心在于一个循环单元，该单元在每个时间步 $t$ 执行相同的计算任务。

1. *输入*：在时间步 $t$，网络接收两个输入：
  - 当前时刻的输入向量 $x_t$；
  - 前一时刻的隐藏状态 $h_(t-1)$。

2. *计算（净输入与状态更新）*：
  网络首先计算当前时刻的净输入 $z_t$，然后通过激活函数更新隐藏状态：
  $ z_t = U h_(t-1) + W x_t + b $
  $ h_t = f(z_t) $
  其中：
  - $W$ 是状态-输入权重矩阵（对应 $x_t$）；
  - $U$ 是状态-状态转移权重矩阵（对应 $h_(t-1)$），这是实现“循环”的关键；
  - $b$ 是偏置向量；
  - $f(dot)$ 通常为 $tanh$ 激活函数。

  *关键点*：参数 $W, U, b$ 在所有时间步中是*共享*的，这显著减少了参数量并赋予模型处理变长序列的能力。

3. *输出*：网络根据新的隐藏状态 $h_t$ 计算当前时间步的输出 $y_t$：
  $ y_t = V h_t + b_y $

SRN 可以按时间展开, 将每个时刻的状态都看作前馈神经网络的每一层, 那么就看作*在时间维度上权重共享*的神经网络. 

应用到机器学习主要有三种模式: *序列到类别* (文本情感分类), *同步的序列到序列* (中文分词), *异步的序列到序列* (机器翻译).

=== #emoji.star 随时间反向传播算法 (BPTT)

随时间反向传播（Backpropagation Through Time, BPTT）是训练 RNN 的标准算法。其本质是将标准的反向传播算法应用在按时间步“展开”后的计算图上。

BPTT 将循环结构转化为一个等效的*深度前馈网络*，通过链式法则在时间轴上逆向传递误差信号。其展开后的网络具有两个关键特性：
- *深度等效*：每一层对应序列中的一个时间步；
- *权值共享*：所有层级（时间步）共享同一套参数 $W, U, b$。

BPTT 执行流程如下: 

+ *前向传播 (Forward Pass)*：
  给定序列 $x_1, dots, x_T$，按时间步计算隐藏状态和输出：
  $ z_t = U h_(t-1) + W x_t + b $
  $ h_t = f(z_t) $
  最后计算总损失 $L$，通常为各时刻损失之和：$L = sum_(t=1)^T L_t$。

+ *网络展开 (Unrolling)*：
  将循环连接转化为层级连接，形成 $t=1 arrow.r t=2 arrow.r dots arrow.r T$ 的长链。

+ *反向传播计算梯度 (Backward Pass)*：
  梯度不仅在层间传播，还沿时间轴 *由后向前* 传播。
  对于参数 $U$（状态转移矩阵），其在时刻 $t$ 的偏导数取决于：
  - 当前时刻的损失梯度；
  - 以后时刻通过 $h_(t+1)$ 传回的梯度。
  $ (partial L)/(partial U) = sum_(t=1)^T (partial L_t)/(partial U) $

+ *汇总与更新 (Accumulation & Update)*：
  由于参数共享，最终梯度是所有时刻梯度的 *累加值*：
  $ Delta W = sum_(t=1)^T (partial L)/(partial W_t), quad Delta U = sum_(t=1)^T (partial L)/(partial U_t) $
  最后使用优化器（如 Adam 或 SGD）更新参数。

BPTT 在处理长序列时，由于梯度在时间轴上的连乘效应（特别是 $U$ 的连乘），会导致：
- *梯度消失*：梯度趋近于 0，模型无法学习远距离依赖。
- *梯度爆炸*：梯度呈指数级增长，导致训练不稳定。

== 高级循环网络变体

为了克服简单RNN在捕捉长期依赖方面的局限性，研究人员开发了多种更为复杂的变体。这些模型通过引入精巧的内部机制来控制信息流，从而有效地缓解了梯度消失问题。本节将重点介绍其中最著名、应用最广泛的模型——长短期记忆网络（LSTM）。

=== 长短期记忆网络 (LSTM)

长短期记忆网络（Long Short-Term Memory, LSTM）是解决 RNN 梯度消失问题的核心方案。它通过引入*细胞状态（Cell State）*和*门控机制（Gating Mechanisms）*，实现了信息的长期保存。LSTM 的精妙之处在于 $C_t = f_t dot C_(t-1) + i_t dot tilde(C)_t$。这种*线性加法*结构使得误差在反向传播时，即使经过很多个时间步，只要遗忘门开启，梯度就不会像传统 RNN 那样因为连乘而迅速消失。

在每个时刻 $t$，LSTM 的计算逻辑由以下公式构成：

1. *计算门控状态*（均使用 $sigma$ 激活函数，输出在 $[0, 1]$ 之间）：
  - 遗忘门：$f_t = sigma(W_f x_t + U_f h_(t-1) + b_f)$
  - 输入门：$i_t = sigma(W_i x_t + U_i h_(t-1) + b_i)$
  - 输出门：$o_t = sigma(W_o x_t + U_o h_(t-1) + b_o)$

2. *更新细胞状态*：
  - 候选状态：$tilde(C)_t = tanh(W_c x_t + U_c h_(t-1) + b_c)$
  - 当前状态：$C_t = f_t dot C_(t-1) + i_t dot tilde(C)_t$ （*关键：加法运算避免了梯度消失*）

3. *计算隐藏输出*：
  - $h_t = o_t dot tanh(C_t)$

#figure(
  table(
    columns: 3,
    [*门控单元*], [*功能描述*], [*对信息流的影响*],
    [遗忘门 $f_t$], [决定从细胞状态中丢弃哪些旧信息。], [若 $f_t approx 0$，历史信息被清空；若 $f_t approx 1$，则无损保留。],
    [输入门 $i_t$], [决定哪些新信息将被存入细胞状态。], [配合 $tilde(C)_t$ 筛选有价值的信息存入长期记忆。],
    [输出门 $o_t$], [决定细胞状态的哪些部分将输出。], [从长期记忆中提取与当前预测相关的信息。],
  ),
  caption: "LSTM 的核心门控机制及其作用"
)

在训练 LSTM 时，一个常用的策略是将遗忘门的偏置 $b_f$ 初始化为一个较大的正值（如 $1.0$ 或 $2.0$）。
- *目的*：确保在训练初期 $f_t$ 接近 1。
- *效果*：让网络在学习之初倾向于“记住所有信息”，使梯度能够通过 $C_t$ 路径长距离回传，有效缓解冷启动时的模型训练困难。

=== LSTM网络变体

在核心LSTM架构的基础上，还衍生出了一些变体，它们对门控机制进行了调整和优化。

- 无遗忘门的LSTM网络：由[Hochreiter et al., 1997]最早提出的原始LSTM网络实际上没有遗忘门。这种设计的缺陷在于，记忆单元 c 的内容会随着时间不断累积和增大，当输入序列非常长时，记忆单元的容量可能会饱和，从而影响模型性能。
- Peephole连接：这是一种允许门控单元直接“窥视”细胞状态的变体。具体来说，三个门（遗忘、输入、输出）的计算不仅依赖于当前输入和前一隐藏状态，还直接依赖于前一时刻的细胞状态 $c_(t-1)$。这为门控决策提供了更丰富的信息。

=== 门控循环单元 (GRU)

门控循环单元（Gated Recurrent Unit, GRU）由 Cho 等人在 2014 年提出。相比 LSTM，GRU 的结构更加精简，它将 LSTM 的“细胞状态”与“隐藏状态”合并，并减少了门控的数量，从而降低了计算复杂度。

在每个时刻 $t$，GRU 的内部状态更新遵循以下逻辑：

1. *门控计算*：
  - *重置门 (Reset Gate)*：$r_t = sigma(W_r x_t + U_r h_(t-1) + b_r)$
    决定了前一时刻状态 $h_(t-1)$ 对当前候选状态的贡献程度。
  - *更新门 (Update Gate)*：$z_t = sigma(W_z x_t + U_z h_(t-1) + b_z)$
    控制前一时刻状态与当前候选状态的融合比例，相当于 LSTM 中遗忘门和输入门的结合。

2. *候选隐藏状态*：
  - $tilde(h)_t = tanh(W_h x_t + U_h (r_t dot h_(t-1)) + b_h)$
    这里的 $r_t$ 控制了忽略多少过去的信息。

3. *最终隐藏状态更新*：
  - $h_t = (1 - z_t) dot h_(t-1) + z_t dot tilde(h)_t$
    这是一个线性插值操作，决定了保留多少旧状态以及加入多少新候选状态。

#figure(
  table(
    columns: 3,
    [*特性*], [*LSTM*], [*GRU*],
    [门控数量], [3个（遗忘门、输入门、输出门）], [2个（重置门、更新门）],
    [内部状态], [隐藏状态 $h_t$ + 细胞状态 $C_t$], [仅隐藏状态 $h_t$],
    [计算开销], [参数较多，计算量相对较大], [参数较少，计算速度更快，易于收敛],
    [记忆机制], [通过 $C_t$ 的累加链条防止梯度消失], [通过更新门 $z_t$ 的线性内插防止梯度消失],
  ), 
  caption: "LSTM 和 GRU 的主要区别"
)

=== 扩展模型

/ 深度循环神经网络 (Deep RNNs): 深度RNN通过在每个时间步上堆叠多个循环层来增加模型的深度。底层的循环层负责捕捉序列的低阶特征，而其输出则作为上一层循环层的输入，使得更高层的网络能够学习到数据中更抽象、更复杂的层次化特征表示。
/ 拓展到图结构 (Extension to Graph Structures): 传统的RNN主要处理线性的序列数据。然而，通过将其核心思想进行扩展，模型可以被应用于处理更普遍的图结构数据（如社交网络、分子结构等）。这种扩展使得模型能够捕捉非线性、非顺序的复杂依赖关系，极大地拓宽了循环网络的应用范围。

== 备考习题预测

=== 选择题

1. 长短期记忆网络（LSTM）主要解决了简单RNN中的什么问题？
  - A. 计算复杂度过高
  - B. 梯度消失问题
  - C. 模型参数过多
  - D. 只能处理定长序列
2. 在随时间反向传播（BPTT）算法中，网络共享参数的最终梯度是如何计算的？
  - A. 只使用最后一个时间步的梯度
  - B. 将所有时间步计算出的梯度进行累加
  - C. 取所有时间步梯度的平均值
  - D. 使用梯度最大的那个时间步的梯度
3. 在LSTM的三个门中，哪个门负责决定从细胞状态中丢弃多少历史信息？
  - A. 输入门
  - B. 输出门
  - C. 遗忘门
  - D. 状态门

=== 简答题

1. 问题： 请简要解释“随时间反向传播（BPTT）”的基本原理，并说明它与标准反向传播算法有何异同。
  - 参考答案：
    - 基本原理： BPTT是训练RNN的标准算法。其核心思想是将RNN沿时间维度展开，形成一个参数共享的深度前馈网络。然后，在这个展开的网络上应用标准的反向传播算法，计算损失函数对共享参数的梯度。
    - 相同点： 两者都基于链式法则来计算梯度，以实现端到端的模型训练。
    - 不同点： 标准反向传播应用于层与层之间，而BPTT则应用于时间步与时间步之间。此外，BPTT计算出的梯度需要在所有时间步上进行累加，以更新在时间维度上共享的参数。
2. 问题： 为什么说简单RNN在处理长序列时会遇到“长期依赖”问题？LSTM是如何通过其门控机制来缓解这个问题的？
  - 参考答案：
    - 长期依赖问题原因： 简单RNN在处理长序列时，梯度需要通过BPTT算法在时间步上进行反向传播。这个过程中梯度会反复乘以相关的权重矩阵，导致梯度呈指数级消失或爆炸。梯度消失使得模型无法学习到序列中相距较远的元素之间的依赖关系，这就是长期依赖问题。
    - LSTM的解决方案： LSTM引入了细胞状态和门控机制。细胞状态像一条“传送带”，允许信息在时间步之间直接传递，避免了梯度连乘导致的衰减。遗忘门、输入门和输出门这三个门控单元则像智能阀门，有选择地决定哪些信息应该被遗忘，哪些新信息应该被加入，以及哪些信息应该被输出。这种精细的控制机制确保了有用的长期信息可以被保留，从而有效缓解了梯度消失问题，使网络能够学习长期依赖。

=== 综合计算题

问题： 假设一个简单的RNN单元，其隐藏状态的更新公式为 $h_t = tanh(W_(h h) - h_(t-1) + W_(x h) - x_t + b_h)$。请描述当给定初始隐藏状态 $h_0$ 和一个长度为2的输入序列 $[x_1, x_2]$ 时，计算 $h_2$ 的详细步骤。请写出计算 $h_1$ 和 $h_2$ 的概念性公式，并指明其中涉及的所有权重矩阵和偏置向量。
  - 参考答案： 计算 $h_2$ 的过程需要分两步进行，严格遵循时间顺序。其中涉及的共享参数包括：输入-隐藏层权重 $W_(x h)$，隐藏-隐藏层权重 $W_(h h)$，以及隐藏层偏置 $b_h$。
    - 第一步：计算 $h_1$ 使用初始隐藏状态 $h_0$ 和第一个输入 $x_1$ 来计算第一个时间步的隐藏状态 $h_1$。
    - 第二步：计算 $h_2$ 使用上一步计算出的隐藏状态 $h_1$ 和第二个输入 $x_2$ 来计算第二个时间步的隐藏状态 $h_2$。
  - 通过这两个步骤，网络将序列 $[x_1, x_2]$ 的信息逐步编码，最终得到的 $h_2$ 包含了整个序列的上下文信息。
